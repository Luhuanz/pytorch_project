{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:16:18.347236Z",
     "start_time": "2022-04-18T00:16:14.619950Z"
    },
    "id": "g831xANXh2HY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch1.0_gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612, modify by wmathor\n",
    "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "              https://github.com/JayParks/transformer\n",
    "'''\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "sentences = [\n",
    "        # enc_input           dec_input         dec_output\n",
    "        ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
    "        ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
    "]\n",
    "\n",
    "\n",
    "## 构建词表\n",
    "    # 编码端的词表\n",
    "# Padding Should be Zero\n",
    "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4, 'cola' : 5} \n",
    "src_vocab_size = len(src_vocab) #实际情况下，它的长度应该是所有德语单词的个数 # 输入6\n",
    " # 解码端的词表\n",
    "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'coke' : 5, 'S' : 6, 'E' : 7, '.' : 8}\n",
    "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab) # 实际情况下，它应该是所有英语单词个数 8\n",
    "\n",
    "src_len = 5 # enc_input max sequence length  编码端的输入长度  \n",
    "tgt_len = 6 # dec_input(=dec_output) max sequence length\n",
    "\n",
    "# Transformer Parameters\n",
    "d_model = 512  # Embedding Size\n",
    "d_ff = 2048 # FeedForward dimension   前馈神经网络映射到多少维度\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_layers = 6  # number of Encoder of Decoder Layer\n",
    "n_heads = 8  # number of heads in Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:16:31.289348Z",
     "start_time": "2022-04-18T00:16:31.273357Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbWwisDpPWbd",
    "outputId": "8800f5ff-6a4d-4bee-c876-2bbb74d4d805"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
       " ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfA3k4QrPakw",
    "outputId": "22c85731-65c6-49f0-884d-45483cf17936"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0, 'bier': 4, 'cola': 5, 'ein': 3, 'ich': 1, 'mochte': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jpp-hnkEPa7n",
    "outputId": "4bd64d0f-8c64-49c4-dca8-fe118cb5ccd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybQcrituRP42",
    "outputId": "75a3a551-1dd0-462c-d16d-abdf3446482d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 8,\n",
       " 'E': 7,\n",
       " 'P': 0,\n",
       " 'S': 6,\n",
       " 'a': 3,\n",
       " 'beer': 4,\n",
       " 'coke': 5,\n",
       " 'i': 1,\n",
       " 'want': 2}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1i7lB_nrPbe5",
    "outputId": "9014ff7e-b4b9-4178-b453-fa4f41cb5841"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'P',\n",
       " 1: 'i',\n",
       " 2: 'want',\n",
       " 3: 'a',\n",
       " 4: 'beer',\n",
       " 5: 'coke',\n",
       " 6: 'S',\n",
       " 7: 'E',\n",
       " 8: '.'}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:16:37.465278Z",
     "start_time": "2022-04-18T00:16:37.447326Z"
    },
    "id": "m6PIfZqsBnMK"
   },
   "outputs": [],
   "source": [
    "def make_data(sentences):   # make  X \n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    for i in range(len(sentences)):\n",
    "       #把单词序列转换为数字序列  矩阵X\n",
    "      enc_input = [[src_vocab[n] for n in sentences[i][0].split()]] # [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\n",
    "      dec_input = [[tgt_vocab[n] for n in sentences[i][1].split()]] # [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\n",
    "      dec_output = [[tgt_vocab[n] for n in sentences[i][2].split()]] # [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\n",
    "\n",
    "      enc_inputs.extend(enc_input)\n",
    "      dec_inputs.extend(dec_input)\n",
    "      dec_outputs.extend(dec_output)\n",
    "\n",
    "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
    "\n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
    "\"\"\"自定义DataLoader\"\"\"\n",
    "class MyDataSet(Data.Dataset):\n",
    "   \n",
    "  def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
    "    super(MyDataSet, self).__init__()\n",
    "    self.enc_inputs = enc_inputs\n",
    "    self.dec_inputs = dec_inputs\n",
    "    self.dec_outputs = dec_outputs\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.enc_inputs.shape[0]\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "\n",
    "loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)  # dataset=MyDataset, batch=2, shuffe=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nsQWQ7YaSNn0",
    "outputId": "0217538c-a8c4-41f0-f8b5-1e307a5a340e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 0],\n",
       "        [1, 2, 3, 5, 0]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E29xifLlXlTc",
    "outputId": "8428f335-20c0-43db-af71-5d6cb96e9e6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zkugeDx1gWD",
    "outputId": "c4e6e7ab-21cb-4b23-c923-e512b68f181e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 1, 2, 3, 4, 8],\n",
       "        [6, 1, 2, 3, 5, 8]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dUp5Vrca1h28",
    "outputId": "3556a215-6d9f-4fc3-e9e7-729877128042"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 8, 7],\n",
       "        [1, 2, 3, 5, 8, 7]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyXOT35_1RmE",
    "outputId": "fd7115c0-fc42-45ef-f2a5-859666fcbe02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f073769d1d0>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rF0eBrnP1U-R",
    "outputId": "ab2db125-1ac5-443c-8a57-176bd5693fdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[1, 2, 3, 4, 0],\n",
       "          [1, 2, 3, 5, 0]]), tensor([[6, 1, 2, 3, 4, 8],\n",
       "          [6, 1, 2, 3, 5, 8]]), tensor([[1, 2, 3, 4, 8, 7],\n",
       "          [1, 2, 3, 5, 8, 7]])]]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*loader] #一个batch 一个两个样本（句子） 表示K Q不一定要维度一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:16:44.024057Z",
     "start_time": "2022-04-18T00:16:44.009059Z"
    },
    "id": "Hpc3Iiti1pPd"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "       #max_len: 一个标量。文本序列的最大长度   一种可能是一个句子的长度   ->  应该是一种是词库的长度 forward \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 从理解来讲，需要注意的就是偶数和奇数在公式上有一个共同部分，我们使用log函数把次方拿下来，方便计算；\n",
    "        # pos代表的是单词在句子中的索引，这点需要注意；比如max_len是128个，那么索引就是从0，1，2，...,127\n",
    "        # 假设我的d_model是512，2i那个符号中i从0取到了255，那么2i对应取值就是0,2,4...510\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)#[5000,512]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)# [5000,1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) #[256]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) #从0开始到最后面，步长为2，其实代表的就是偶数位置 512   div_term为256\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 从1开始到最后面，步长为2，其实代表的就是奇数位置\n",
    "     # 下面这个代码之后，我们得到的pe形状是：[max_len*1*d_model]\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) #[5000,1,512]\n",
    "         # 定一个缓冲区，其实简单理解为这个参数不更新就可以\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "# 不同句子词的相同位置加的是同一种位置编码 ，因为max_len代表的是词表长度。 \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4IMK8Hpv-oEQ",
    "outputId": "1cfbc1b7-36b3-4535-d2e7-5c8a0f01e2c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 512])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5000, d_model).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKGOrSreBI5d",
    "outputId": "31033050-9553-4edb-bab7-7a5822aad140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 5000, dtype=torch.float).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uEDZmoOGES2a",
    "outputId": "057d5c13-0850-458b-a0ec-9df2408ce1fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pcje6dWElX5"
   },
   "outputs": [],
   "source": [
    "pe = torch.zeros(5000, d_model)\n",
    "position = torch.arange(0, 5000, dtype=torch.float).unsqueeze(1) \n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "pe[:, 0::2] = torch.sin(position * div_term) #从0开始到最后面，步长为2，其实代表的就是偶数位置 512   div_term为256\n",
    "pe[:, 1::2] = torch.cos(position * div_term) # 从1开始到最后面，步长为2，其实代表的就是奇数位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YAM1S1pGHNs",
    "outputId": "8429e54e-c75e-4329-c4ae-2099be493cd4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 512])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntzMcB73GMAD",
    "outputId": "00b81ebd-af3a-4388-f937-24dbbd9018b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5000, 512])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7lktQed2T90P",
    "outputId": "56199e14-3341-4614-a07e-871092538af7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "        [[ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "           1.0366e-04,  1.0000e+00]],\n",
       "\n",
       "        [[ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "           2.0733e-04,  1.0000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 9.5625e-01, -2.9254e-01,  9.3594e-01,  ...,  8.5926e-01,\n",
       "           4.9515e-01,  8.6881e-01]],\n",
       "\n",
       "        [[ 2.7050e-01, -9.6272e-01,  8.2251e-01,  ...,  8.5920e-01,\n",
       "           4.9524e-01,  8.6876e-01]],\n",
       "\n",
       "        [[-6.6395e-01, -7.4778e-01,  1.4615e-03,  ...,  8.5915e-01,\n",
       "           4.9533e-01,  8.6871e-01]]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.unsqueeze(0).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7D7GDVjoHc8E",
    "outputId": "647ad8ba-6735-4ad5-f6fd-1dccc66111d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1, 512])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.unsqueeze(0).transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4HzBV1MkD8Y"
   },
   "outputs": [],
   "source": [
    " pe=pe.unsqueeze(0).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWBndryHkG5_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:16:50.181307Z",
     "start_time": "2022-04-18T00:16:50.161360Z"
    },
    "id": "YN_OYtMjPnRg"
   },
   "outputs": [],
   "source": [
    "## 4. get_attn_pad_mask\n",
    "\n",
    "## 比如说，我现在的句子长度是5，在后面注意力机制的部分，我们在计算出来QK转置除以根号之后，softmax之前，我们得到的形状\n",
    "## len_input * len_input  代表每个单词对其余包含自己的单词的影响力\n",
    "## 所以这里我需要有一个同等大小形状的矩阵，告诉我哪个位置是PAD部分，之后在计算softmax之前会把这里置为无穷大；\n",
    "## 一定需要注意的是这里得到的矩阵形状是batch_size x len_q x len_k，我们是对k中的pad符号进行标识，并没有对k中的做标识，因为没必要\n",
    "## seq_q 和 seq_k 不一定一致(我自己的理解是原文是德文，翻译成英文，而原文的德语的单词个数和英语的单词个数不一样多，所以这儿可能不一致)，\n",
    "#在交互注意力，q来自解码端，k来自编码端，所以告诉模型编码这边pad符号信息就可以，解码端的pad信息在交互注意力层是没有用到的；\n",
    "\n",
    " # pad mask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    '''\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    seq_len could be src_len or it could be tgt_len\n",
    "    '''\n",
    "    \n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], False is masked\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k][2,5,5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_I91oQYqVdE",
    "outputId": "ae573fd6-a652-4b47-e752-05f43217421a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfMIje_W_SVS",
    "outputId": "2cd3ad5d-bb71-4b8f-f9d0-d4922157ef80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wx_n_f2H_fA7",
    "outputId": "aa45a477-df18-45bd-857c-222a6487266d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 0],\n",
       "        [1, 2, 3, 5, 0]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e4meYfq_lJH",
    "outputId": "afc92488-4efd-4ee3-ce52-7148d17311db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False,  True],\n",
       "        [False, False, False, False,  True]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.data.eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "voJF_PZF_qpm",
    "outputId": "9454dc6d-b829-45ca-fee2-1e85c753b875"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.data.eq(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PI4y-iEu_upR",
    "outputId": "c4cd5129-11fd-4a38-c2f2-2d052b1e7080"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False,  True]],\n",
       "\n",
       "        [[False, False, False, False,  True]]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.data.eq(0).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvIbpwjf_13C",
    "outputId": "8b79455a-9ac5-4ebf-917c-b6ea131cc9bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 5])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.data.eq(0).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRd3B4yBAGqH",
    "outputId": "7b7fc08f-cb37-4ced-9635-9492fe9462e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True]],\n",
       "\n",
       "        [[False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True]]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.data.eq(0).unsqueeze(1).expand(2, 5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUaa-50bAVuy"
   },
   "outputs": [],
   "source": [
    "yy=enc_inputs.data.eq(0).unsqueeze(1).expand(2, 5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxs7S5YgIxgT",
    "outputId": "a65f91a0-fb75-4a7f-9169-662d3faf0753"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.data.eq(0).unsqueeze(1).expand(2, 5,5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:16:58.047701Z",
     "start_time": "2022-04-18T00:16:58.040718Z"
    },
    "id": "ChFWmKxmIp8T"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_attn_subsequence_mask(seq):\n",
    "    '''\n",
    "    seq: [batch_size, tgt_len]\n",
    "    '''\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1) # Upper triangular matrix  https://blog.csdn.net/hufei_neo/article/details/100773462\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
    "    return subsequence_mask # [batch_size, tgt_len, tgt_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:39:35.487832Z",
     "start_time": "2022-04-18T00:39:34.440899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 1, 2, 3, 5, 8],\n",
       "        [6, 1, 2, 3, 4, 8]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seq=dec_inputs\n",
    "dec_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:39:53.916845Z",
     "start_time": "2022-04-18T00:39:53.910835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:41:53.671691Z",
     "start_time": "2022-04-18T00:41:53.661718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_shape = [dec_inputs.size(0),dec_inputs.size(1), dec_inputs.size(1)]\n",
    "np.triu(np.ones(attn_shape), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:50:32.022674Z",
     "start_time": "2022-04-18T00:50:32.015718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu(np.ones(attn_shape), k=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:51:57.822635Z",
     "start_time": "2022-04-18T00:51:57.807642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.triu(np.ones(attn_shape), k=1)).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:16:59.980547Z",
     "start_time": "2022-04-18T00:16:59.973570Z"
    },
    "id": "39Ko2lftNM7L"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        '''\n",
    "        Q: [batch_size, n_heads, len_q, d_k] [2,8,5,64]encoder\n",
    "        K: [batch_size, n_heads, len_k, d_k]\n",
    "        V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        '''\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\n",
    "        \n",
    "        # 下面这个就是用到了我们之前的attn_mask，把被mask的地方置为无限小，softmax之后基本就是0，对q的单词不起作用\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.  \n",
    "        \n",
    "        attn = nn.Softmax(dim=-1)(scores) #attention矩阵\n",
    "        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v] context相当于Z\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIuCjcVoKdJH",
    "outputId": "ecf32262-8041-4acf-dc45-0372a9eb4e61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 5, 64]), torch.Size([2, 8, 5, 64]))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yy, Q,K,V\n",
    "Q.shape, K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALZF-u9WPJSg",
    "outputId": "225acb0f-557c-4985-edf8-fbe805a30433"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_q1JbAnQR91",
    "outputId": "12222963-06b8-4746-f77c-a807e7d2315c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 5, 64]), torch.Size([2, 8, 64, 5]))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " K.shape,K.transpose(-1, -2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qik9y9r3Qqbu"
   },
   "outputs": [],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wKMWV-c8QsTY",
    "outputId": "66111f95-d72a-4ee8-f46a-c1582f7c3bb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 5, 5])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5JJzdA-RSSQ",
    "outputId": "a67ed9aa-0fd6-44f1-b11a-5dd44b52168f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6boTHbzqRamV",
    "outputId": "19c0832f-2a10-4979-b272-128a5e896723"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 5, 5])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.masked_fill_(yy, -1e9).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gua7Mi1BSmgG",
    "outputId": "f9b4253e-fd05-4cbc-d0d8-e3f799e64a86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 5, 64])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4Cos52GSpN4"
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:17:06.235708Z",
     "start_time": "2022-04-18T00:17:06.218754Z"
    },
    "id": "Znu2uPqpNTz9"
   },
   "outputs": [],
   "source": [
    "  \"\"\"这个Attention类可以实现:\n",
    "    Encoder的Self-Attention\n",
    "    Decoder的Masked Self-Attention\n",
    "    Encoder-Decoder的Attention\n",
    "    \"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "         # 输入进来的QKV是相等的，我们会使用映射linear做一个映射得到参数矩阵Wq, Wk,Wv\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False) # q,k必须维度相同，不然无法做点积 64*8\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "       # 这个多头分为这几个步骤，首先映射分头，然后计算atten_scores，然后计算atten_value;\n",
    "        # 输入进来的数据形状：\n",
    "        '''\n",
    "        input_Q: [batch_size, len_q, d_model] [2,5,512]\n",
    "        input_K: [batch_size, len_k, d_model]  \n",
    "        input_V: [batch_size, len_v(=len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        ''' \n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]  写-1不写定的原因是因为一句话不同batch可能长度不一样 八组不同的Q\n",
    "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\n",
    "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        # context: [batch_size, n_heads, len_q, d_v] [2,8,5,64]\n",
    "        #attn: [batch_size, n_heads, len_q, len_k]\n",
    "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
    "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v) # context: [batch_size, len_q, n_heads * d_v][2.5.512]\n",
    "\n",
    "        output = self.fc(context) # [batch_size, len_q, d_model]\n",
    "        return self.layer_norm.cuda()(output + residual), attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPOimEQSG3oz",
    "outputId": "c505b3a1-9471-44b2-9a3a-6d235d187c50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 512])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJVeYRgAHUwm"
   },
   "outputs": [],
   "source": [
    "W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S82a59AMNU8F"
   },
   "outputs": [],
   "source": [
    "W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "W_V = nn.Linear(d_model, d_v * n_heads, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgOTPUDHNaWu",
    "outputId": "4bbbca6a-2507-47e6-9ec3-53d4809862e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 8, 64])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_K(enc_outputs).view(2, -1, n_heads, d_k).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jec11QX-HFXg",
    "outputId": "a30570d2-475e-41ce-fe60-0303835d1d9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 8, 64])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q(enc_outputs).view(2, -1, n_heads, d_k).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3U53oB8NfTe",
    "outputId": "3f6076be-759e-4976-cf9a-f79e427240ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q(enc_outputs)==W_K(enc_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDF769BZHsdK",
    "outputId": "2c22bc0a-9aab-4da1-d478-c589b0129a80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 8, 64])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q(enc_outputs).view(2, 5, n_heads, d_k).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1qkxhWNH1lH",
    "outputId": "db13ad3b-0300-4cfe-ad9a-b82277a8fe35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 5, 64])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q(enc_outputs).view(2, -1, n_heads, d_k).transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4VPp1-QJSxn",
    "outputId": "985fac0b-ab60-4ba4-9ed1-71c065b735c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True]],\n",
       "\n",
       "        [[False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True]]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy #padmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErW2Wj8hI6tL",
    "outputId": "bbb322ee-5e41-446d-8260-1e9203bb1a12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]],\n",
       "\n",
       "         [[False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True],\n",
       "          [False, False, False, False,  True]]]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.unsqueeze(1).repeat(1, n_heads, 1, 1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Z9XBMJlJiVv",
    "outputId": "f584b9a4-6e1a-412f-dc95-1d4914a74491"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 5, 5])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.unsqueeze(1).shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iv8grh0PJIFs",
    "outputId": "c30caf29-dbc2-4c37-aa9c-db199ecf3e7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 5, 5])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.unsqueeze(1).repeat(1, n_heads, 1, 1).shape # 复制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vODzLxZ8O3fv"
   },
   "outputs": [],
   "source": [
    "yy=yy.unsqueeze(1).repeat(1, n_heads, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-gJK0qgOU1M"
   },
   "outputs": [],
   "source": [
    "Q =W_Q(enc_outputs).view(2, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]  写-1不写定的原因是因为一句话不同batch可能长度不一样 八组不同的Q\n",
    "K =W_K(enc_outputs).view(2, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\n",
    "V =W_V(enc_outputs).view(2, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vZW27wWOqHR",
    "outputId": "67b69911-48c7-4e73-d966-86c135b9bbe5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 5, 64])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:17:15.580549Z",
     "start_time": "2022-04-18T00:17:15.574565Z"
    },
    "id": "ZmE4buSCNWvE"
   },
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model, bias=False)\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        inputs: [batch_size, seq_len, d_model]\n",
    "        '''\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        return nn.LayerNorm(d_model).cuda()(output + residual) # [batch_size, seq_len, d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:17:17.854354Z",
     "start_time": "2022-04-18T00:17:17.848370Z"
    },
    "id": "P-JIvRw8NYv_"
   },
   "outputs": [],
   "source": [
    "# EncoderLayer ：包含两个部分，多头注意力机制和前馈神经网络\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len, d_model] [2,5,512]  位置编码过后\n",
    "        enc_self_attn_mask: [batch_size, src_len, src_len][2,5,5] padmask矩阵\n",
    "        enc_outputs: [batch_size, src_len, d_model] [2,5,512]\n",
    "        attn: [batch_size, n_heads, src_len, src_len] \n",
    "        '''\n",
    "      # 下面这个就是做自注意力层，输入是enc_inputs，形状是[batch_size x seq_len_q x d_model]\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_outputs, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:17:19.593412Z",
     "start_time": "2022-04-18T00:17:19.582469Z"
    },
    "id": "bVTz8vd8Ndff"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "        dec_enc_attn_mask: [batch_size, tgt_len, src_len]\n",
    "        '''\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask) # 这里的Q,K,V全是Decoder自己的输入\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "        \n",
    "        # Attention层的Q(来自decoder) 和 K,V(来自encoder)\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs) # [batch_size, tgt_len, d_model]\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn  # dec_self_attn, dec_enc_attn这两个是为了可视化的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:17:21.442168Z",
     "start_time": "2022-04-18T00:17:21.419230Z"
    },
    "id": "CGCs3RTKNjr8"
   },
   "outputs": [],
   "source": [
    "#Encoder 部分包含三个部分：词向量embedding，位置编码部分，注意力层及后续的前馈神经网络 \n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        #  Embedding构建一张词表 https://blog.csdn.net/qq_41477675/article/details/114645012\n",
    "        # 10，3词表   (2,4)输入  （2,4,3）https://blog.csdn.net/qq_39540454/article/details/115215056     \n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)# 定义生成一个矩阵，大小是 src_vocab_size * d_model   #embed = torch.nn.Embedding(n_vocabulary,embedding_size)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len] 2个句子 和每个句子长度 [2,5]\n",
    "        '''\n",
    "        enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len, d_model][2,5,512]\n",
    "        #enc_outputs.transpose(0, 1) 为什么要改维度 https://www.jianshu.com/p/63e7acc5e890  为了方便加posti_encoding\n",
    "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, d_model] [2,5,512]\n",
    "        # get_attn_pad_mask是为了得到句子中pad的位置信息，给到模型后面，在计算自注意力和交互注意力的时候去掉pad符号的影响，去看一下这个函数\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) # [batch_size, src_len, src_len] [2,5,5]\n",
    "        enc_self_attns = [] # 在计算中不需要用到，它主要用来保存你接下来返回的attention的值（这个主要是为了你画热力图等，用来看各个词之间的关系\n",
    "        # for循环访问nn.ModuleList对象\n",
    "            # 上一个block的输出enc_outputs作为当前block的输入\n",
    "        for layer in self.layers:\n",
    "            # enc_outputs: [batch_size, src_len, d_model][2,5,512] 经过位置编码的out\n",
    "            #enc_self_attn: [batch_size, n_heads, src_len, src_len] [2,8,5,5]\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)# 传入的enc_outputs其实是input，传入mask矩阵是因为你要做self attention\n",
    "            enc_self_attns.append(enc_self_attn) #只是为了画图这个不是重点\n",
    "        return enc_outputs, enc_self_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhASuk-9VDtv",
    "outputId": "89f17b3f-37cf-46a8-c3c5-b07151329a86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 0],\n",
       "        [1, 2, 3, 5, 0]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piem5YKbVGo2"
   },
   "outputs": [],
   "source": [
    "src_emb = nn.Embedding(src_vocab_size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRwVxirkVcoa",
    "outputId": "58e23ca7-5665-4ec7-ca42-c4872fdaecaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(6, 512)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_emb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9qA4QT2Vfz2",
    "outputId": "737c82cd-f87e-4005-da88-d00187dcc047"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 512])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_emb(enc_inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZ3s7evWV135"
   },
   "outputs": [],
   "source": [
    "enc_outputs=src_emb(enc_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjMUwO0JV_tl",
    "outputId": "fb14f994-4be1-49b0-a027-5d48d3bd8b2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 512])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outputs.transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb2eMj1cj-ww",
    "outputId": "c17ff653-a21a-47c1-aa32-5c1a57dca2f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1, 512])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0e2NmLV4keZr"
   },
   "outputs": [],
   "source": [
    "x1=enc_outputs.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNWzGufkkqc5",
    "outputId": "0c5408f6-1d35-4fc9-8e68-ba610da33452"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 512])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRnrJSYOlan_",
    "outputId": "256a9c87-0152-4e5d-db9f-ddf726a641c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0358,  0.0350,  1.3098,  ..., -0.2427, -0.8998,  1.1425],\n",
       "         [ 2.0358,  0.0350,  1.3098,  ..., -0.2427, -0.8998,  1.1425]],\n",
       "\n",
       "        [[-0.3532,  0.5222,  0.1360,  ...,  0.6693, -0.7690, -0.5097],\n",
       "         [-0.3532,  0.5222,  0.1360,  ...,  0.6693, -0.7690, -0.5097]],\n",
       "\n",
       "        [[-0.6686,  0.8590,  0.5581,  ...,  1.3988,  0.0475, -1.9611],\n",
       "         [-0.6686,  0.8590,  0.5581,  ...,  1.3988,  0.0475, -1.9611]],\n",
       "\n",
       "        [[ 1.2501,  0.0552, -0.3065,  ..., -0.4092, -0.3561, -0.3484],\n",
       "         [ 0.7179,  0.3352, -0.9570,  ..., -0.3286, -0.4114, -0.9273]],\n",
       "\n",
       "        [[-0.7196,  2.0046,  0.3455,  ...,  1.0214, -0.0042, -1.3199],\n",
       "         [-0.7196,  2.0046,  0.3455,  ...,  1.0214, -0.0042, -1.3199]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zN9PPM1Yks5Q",
    "outputId": "640fa36c-1c27-42b6-991a-e56bb43bfdf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsigsdhik43a",
    "outputId": "213d8b8d-ce5a-460b-ce80-9c63599e0cd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 512])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[:x1.size(0),:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCnu4TH6lSX-",
    "outputId": "70ae492d-6936-4ebf-89e1-6b3ff2b4b2fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00]],\n",
       "\n",
       "        [[ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "           1.0366e-04,  1.0000e+00]],\n",
       "\n",
       "        [[ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "           2.0733e-04,  1.0000e+00]],\n",
       "\n",
       "        [[ 1.4112e-01, -9.8999e-01,  2.4509e-01,  ...,  1.0000e+00,\n",
       "           3.1099e-04,  1.0000e+00]],\n",
       "\n",
       "        [[-7.5680e-01, -6.5364e-01, -6.5717e-01,  ...,  1.0000e+00,\n",
       "           4.1465e-04,  1.0000e+00]]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[:x1.size(0),:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlE4loZalHYK",
    "outputId": "cd732dc8-224a-4808-f929-7f8df25fa0be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 512])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pe[:x1.size(0),:]+x1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:17:32.324332Z",
     "start_time": "2022-04-18T00:17:32.302389Z"
    },
    "id": "Qfb25qQ6Nngh"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model) #[9,512]\n",
    "        self.pos_emb = PositionalEncoding(d_model) \n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_intpus: [batch_size, src_len]\n",
    "        enc_outputs: [batsh_size, src_len, d_model]\n",
    "        \n",
    "        '''\n",
    "        # pad 和mask都是对原输入词矩阵做 其实并没有要矩阵的信息只是为了获得维度\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) # [batch_size, tgt_len, d_model] [ 2,,6,512 ]\n",
    "        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1).cuda() # [batch_size, tgt_len, d_model] [2,6,512]\n",
    "        # Decoder输入序列的pad mask矩阵（这个例子中decoder是没有加pad的，实际应用中都是有pad填充的） 输入端dec_inputs没有 P\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda() # [batch_size, tgt_len, tgt_len] [2,6,6]\n",
    "         # Masked Self_Attention：当前时刻是看不到未来的信息的\n",
    " \n",
    "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda() # [batch_size, tgt_len, tgt_len][2,6,6] 0,1矩阵看不到的为1 看到的为0\n",
    "        # Decoder中把两种mask矩阵相加（既屏蔽了pad的信息，也屏蔽了未来时刻的信息）\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0).cuda() # [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "        # tgt_len =q  ,src_len=k  ## 这个做的是交互注意力机制中的mask矩阵，enc的输入是k，我去看这个k里面哪些是pad符号，给到后面的模型；注意哦，我q肯定也是有pad符号，但是这里我不在意的\n",
    "#  这个mask主要用于encoder-decoder attention层\n",
    "\n",
    "     # get_attn_pad_mask主要是enc_inputs的pad mask矩阵(因为enc是处理K,V的，求Attention时是用v1,v2,..vm去加权的，要把pad对应的v_i的相关系数设为0，这样注意力就不会关注pad向量)\n",
    "        # dec_inputs只是提供expand的size的 \n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) # [batc_size, tgt_len, src_len] ?\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:56:01.498259Z",
     "start_time": "2022-04-18T00:56:01.267969Z"
    }
   },
   "outputs": [],
   "source": [
    "aa = get_attn_pad_mask(dec_inputs, dec_inputs).cuda()\n",
    "bb=dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T01:03:47.556577Z",
     "start_time": "2022-04-18T01:03:47.330785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False]],\n",
       "\n",
       "        [[False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False]]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T01:03:50.315624Z",
     "start_time": "2022-04-18T01:03:50.295678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0]]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T01:03:35.578331Z",
     "start_time": "2022-04-18T01:03:35.574351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 6]), torch.Size([2, 6, 6]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape,bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T01:03:15.089513Z",
     "start_time": "2022-04-18T01:03:14.860882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False]],\n",
       "\n",
       "        [[False,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False]]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gt((aa +bb), 0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:56:09.471140Z",
     "start_time": "2022-04-18T00:56:09.458152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False]],\n",
       "\n",
       "        [[False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False]]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa # Decoder输入序列的pad mask矩阵（这个例子中decoder是没有加pad的，实际应用中都是有pad填充的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chbzeUn5bNHb"
   },
   "outputs": [],
   "source": [
    "tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiOf1cCkbTgs",
    "outputId": "071f3aaa-a27f-486b-bbe4-8f1b6decaa4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(9, 512)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_emb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnHld7lrbbBG"
   },
   "outputs": [],
   "source": [
    "pos_emb = PositionalEncoding(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Ch5gV-sbeL7",
    "outputId": "a0f3c375-0207-4393-b08d-247bc7fe1e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionalEncoding(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:17:37.064390Z",
     "start_time": "2022-04-18T00:17:35.341324Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "_HztXZCwNpBb",
    "outputId": "a56b10f9-7d43-4073-cf5e-11b897222e2c"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder().cuda()\n",
    "        self.decoder = Decoder().cuda()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).cuda()# 输出层 d_model 是我们解码层每个token输出的维度大小，之后会做一个 tgt_vocab_size 大小的softmax\n",
    "# 一个德语单词被翻译成英语，它会对应为那个单词，所以这里输入就是一个单词在词表中的维度，这里的维度是512，在词表中一个单词的维度是512。\n",
    "#如果一句话有n个单词，那么在翻译的整个过程中就会调用n次这个全连接函数。然后假设英语单词有100000个，那么这儿的tgt_vocab_size就是1000000个 \n",
    "# 到达这儿，就好像是一个分类任务，看这个单词属于这100000个类中的哪一个类，最后全连接分类的结果然后再进行一个softmax就会得到这100000个单词每个单词的概率。\n",
    "#哪个单词的概率最大，那么我们就把这个德语单词翻译成那个单词。也就是我们这儿的projection就是那个德语单词被翻译成英语单词的词。 \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        '''\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, \n",
    "        src_len]\n",
    "        dec_outputs: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\n",
    "        # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
    "        '''\n",
    "        # 这里有两个数据进行输入，一个是enc_inputs 形状为[batch_size, src_len]，主要是作为编码段的输入，\n",
    "        # 一个dec_inputs，形状为[batch_size, tgt_len]，主要是作为解码端的输入\n",
    "        # enc_outputs就是主要的输出，enc_self_attns这里没记错的是QK转置相乘之后softmax之后的矩阵值，代表的是每个单词和其他单词相关性；\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)#enc_inputs？\n",
    "        # dec_outputs做映射到词表大小\n",
    "        dec_logits = self.projection(dec_outputs) \n",
    "        \n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "\n",
    "model = Transformer().cuda()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:19:23.827701Z",
     "start_time": "2022-04-18T00:19:23.808751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  tgt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:19:36.778204Z",
     "start_time": "2022-04-18T00:19:36.771247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T00:18:55.863634Z",
     "start_time": "2022-04-18T00:17:44.678969Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNemnO18h6PV",
    "outputId": "3cfb2c91-65d6-4f25-eeb2-64e85f1c67fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss = 2.306256\n",
      "Epoch: 0002 loss = 2.222728\n",
      "Epoch: 0003 loss = 1.922032\n",
      "Epoch: 0004 loss = 1.692269\n",
      "Epoch: 0005 loss = 1.544990\n",
      "Epoch: 0006 loss = 1.389395\n",
      "Epoch: 0007 loss = 1.115644\n",
      "Epoch: 0008 loss = 0.914461\n",
      "Epoch: 0009 loss = 0.707665\n",
      "Epoch: 0010 loss = 0.475579\n",
      "Epoch: 0011 loss = 0.453225\n",
      "Epoch: 0012 loss = 0.372238\n",
      "Epoch: 0013 loss = 0.310289\n",
      "Epoch: 0014 loss = 0.255047\n",
      "Epoch: 0015 loss = 0.212950\n",
      "Epoch: 0016 loss = 0.138464\n",
      "Epoch: 0017 loss = 0.119408\n",
      "Epoch: 0018 loss = 0.081709\n",
      "Epoch: 0019 loss = 0.060753\n",
      "Epoch: 0020 loss = 0.055467\n",
      "Epoch: 0021 loss = 0.043069\n",
      "Epoch: 0022 loss = 0.041861\n",
      "Epoch: 0023 loss = 0.040490\n",
      "Epoch: 0024 loss = 0.041463\n",
      "Epoch: 0025 loss = 0.035719\n",
      "Epoch: 0026 loss = 0.033651\n",
      "Epoch: 0027 loss = 0.026825\n",
      "Epoch: 0028 loss = 0.023173\n",
      "Epoch: 0029 loss = 0.017695\n",
      "Epoch: 0030 loss = 0.017236\n",
      "Epoch: 0031 loss = 0.013711\n",
      "Epoch: 0032 loss = 0.016845\n",
      "Epoch: 0033 loss = 0.009329\n",
      "Epoch: 0034 loss = 0.010111\n",
      "Epoch: 0035 loss = 0.007688\n",
      "Epoch: 0036 loss = 0.005332\n",
      "Epoch: 0037 loss = 0.006743\n",
      "Epoch: 0038 loss = 0.004486\n",
      "Epoch: 0039 loss = 0.003913\n",
      "Epoch: 0040 loss = 0.002184\n",
      "Epoch: 0041 loss = 0.003118\n",
      "Epoch: 0042 loss = 0.002807\n",
      "Epoch: 0043 loss = 0.002780\n",
      "Epoch: 0044 loss = 0.001838\n",
      "Epoch: 0045 loss = 0.002303\n",
      "Epoch: 0046 loss = 0.001989\n",
      "Epoch: 0047 loss = 0.001532\n",
      "Epoch: 0048 loss = 0.003699\n",
      "Epoch: 0049 loss = 0.001138\n",
      "Epoch: 0050 loss = 0.000959\n",
      "Epoch: 0051 loss = 0.001353\n",
      "Epoch: 0052 loss = 0.001158\n",
      "Epoch: 0053 loss = 0.001033\n",
      "Epoch: 0054 loss = 0.000850\n",
      "Epoch: 0055 loss = 0.001065\n",
      "Epoch: 0056 loss = 0.000967\n",
      "Epoch: 0057 loss = 0.001168\n",
      "Epoch: 0058 loss = 0.000835\n",
      "Epoch: 0059 loss = 0.000780\n",
      "Epoch: 0060 loss = 0.000782\n",
      "Epoch: 0061 loss = 0.000649\n",
      "Epoch: 0062 loss = 0.000804\n",
      "Epoch: 0063 loss = 0.000974\n",
      "Epoch: 0064 loss = 0.000951\n",
      "Epoch: 0065 loss = 0.000609\n",
      "Epoch: 0066 loss = 0.001485\n",
      "Epoch: 0067 loss = 0.001020\n",
      "Epoch: 0068 loss = 0.001372\n",
      "Epoch: 0069 loss = 0.001255\n",
      "Epoch: 0070 loss = 0.001075\n",
      "Epoch: 0071 loss = 0.001872\n",
      "Epoch: 0072 loss = 0.001377\n",
      "Epoch: 0073 loss = 0.000830\n",
      "Epoch: 0074 loss = 0.001183\n",
      "Epoch: 0075 loss = 0.001207\n",
      "Epoch: 0076 loss = 0.001470\n",
      "Epoch: 0077 loss = 0.001130\n",
      "Epoch: 0078 loss = 0.001332\n",
      "Epoch: 0079 loss = 0.001591\n",
      "Epoch: 0080 loss = 0.001585\n",
      "Epoch: 0081 loss = 0.001115\n",
      "Epoch: 0082 loss = 0.001174\n",
      "Epoch: 0083 loss = 0.000894\n",
      "Epoch: 0084 loss = 0.001112\n",
      "Epoch: 0085 loss = 0.001031\n",
      "Epoch: 0086 loss = 0.001185\n",
      "Epoch: 0087 loss = 0.000949\n",
      "Epoch: 0088 loss = 0.000985\n",
      "Epoch: 0089 loss = 0.000915\n",
      "Epoch: 0090 loss = 0.000932\n",
      "Epoch: 0091 loss = 0.000878\n",
      "Epoch: 0092 loss = 0.000924\n",
      "Epoch: 0093 loss = 0.000987\n",
      "Epoch: 0094 loss = 0.000609\n",
      "Epoch: 0095 loss = 0.000385\n",
      "Epoch: 0096 loss = 0.000419\n",
      "Epoch: 0097 loss = 0.000383\n",
      "Epoch: 0098 loss = 0.000361\n",
      "Epoch: 0099 loss = 0.000462\n",
      "Epoch: 0100 loss = 0.000418\n",
      "Epoch: 0101 loss = 0.000397\n",
      "Epoch: 0102 loss = 0.000375\n",
      "Epoch: 0103 loss = 0.000191\n",
      "Epoch: 0104 loss = 0.000218\n",
      "Epoch: 0105 loss = 0.000211\n",
      "Epoch: 0106 loss = 0.000158\n",
      "Epoch: 0107 loss = 0.000185\n",
      "Epoch: 0108 loss = 0.000226\n",
      "Epoch: 0109 loss = 0.000170\n",
      "Epoch: 0110 loss = 0.000122\n",
      "Epoch: 0111 loss = 0.000145\n",
      "Epoch: 0112 loss = 0.000123\n",
      "Epoch: 0113 loss = 0.000107\n",
      "Epoch: 0114 loss = 0.000102\n",
      "Epoch: 0115 loss = 0.000112\n",
      "Epoch: 0116 loss = 0.000079\n",
      "Epoch: 0117 loss = 0.000082\n",
      "Epoch: 0118 loss = 0.000104\n",
      "Epoch: 0119 loss = 0.000103\n",
      "Epoch: 0120 loss = 0.000074\n",
      "Epoch: 0121 loss = 0.000067\n",
      "Epoch: 0122 loss = 0.000052\n",
      "Epoch: 0123 loss = 0.000067\n",
      "Epoch: 0124 loss = 0.000098\n",
      "Epoch: 0125 loss = 0.000066\n",
      "Epoch: 0126 loss = 0.000052\n",
      "Epoch: 0127 loss = 0.000057\n",
      "Epoch: 0128 loss = 0.000061\n",
      "Epoch: 0129 loss = 0.000035\n",
      "Epoch: 0130 loss = 0.000064\n",
      "Epoch: 0131 loss = 0.000040\n",
      "Epoch: 0132 loss = 0.000080\n",
      "Epoch: 0133 loss = 0.000082\n",
      "Epoch: 0134 loss = 0.000068\n",
      "Epoch: 0135 loss = 0.000067\n",
      "Epoch: 0136 loss = 0.000067\n",
      "Epoch: 0137 loss = 0.000043\n",
      "Epoch: 0138 loss = 0.000081\n",
      "Epoch: 0139 loss = 0.000059\n",
      "Epoch: 0140 loss = 0.000044\n",
      "Epoch: 0141 loss = 0.000068\n",
      "Epoch: 0142 loss = 0.000076\n",
      "Epoch: 0143 loss = 0.000054\n",
      "Epoch: 0144 loss = 0.000059\n",
      "Epoch: 0145 loss = 0.000078\n",
      "Epoch: 0146 loss = 0.000055\n",
      "Epoch: 0147 loss = 0.000054\n",
      "Epoch: 0148 loss = 0.000068\n",
      "Epoch: 0149 loss = 0.000064\n",
      "Epoch: 0150 loss = 0.000060\n",
      "Epoch: 0151 loss = 0.000057\n",
      "Epoch: 0152 loss = 0.000073\n",
      "Epoch: 0153 loss = 0.000070\n",
      "Epoch: 0154 loss = 0.000056\n",
      "Epoch: 0155 loss = 0.000070\n",
      "Epoch: 0156 loss = 0.000072\n",
      "Epoch: 0157 loss = 0.000072\n",
      "Epoch: 0158 loss = 0.000059\n",
      "Epoch: 0159 loss = 0.000046\n",
      "Epoch: 0160 loss = 0.000060\n",
      "Epoch: 0161 loss = 0.000038\n",
      "Epoch: 0162 loss = 0.000063\n",
      "Epoch: 0163 loss = 0.000089\n",
      "Epoch: 0164 loss = 0.000074\n",
      "Epoch: 0165 loss = 0.000065\n",
      "Epoch: 0166 loss = 0.000037\n",
      "Epoch: 0167 loss = 0.000052\n",
      "Epoch: 0168 loss = 0.000083\n",
      "Epoch: 0169 loss = 0.000079\n",
      "Epoch: 0170 loss = 0.000094\n",
      "Epoch: 0171 loss = 0.000070\n",
      "Epoch: 0172 loss = 0.000040\n",
      "Epoch: 0173 loss = 0.000056\n",
      "Epoch: 0174 loss = 0.000048\n",
      "Epoch: 0175 loss = 0.000042\n",
      "Epoch: 0176 loss = 0.000041\n",
      "Epoch: 0177 loss = 0.000046\n",
      "Epoch: 0178 loss = 0.000074\n",
      "Epoch: 0179 loss = 0.000036\n",
      "Epoch: 0180 loss = 0.000063\n",
      "Epoch: 0181 loss = 0.000078\n",
      "Epoch: 0182 loss = 0.000089\n",
      "Epoch: 0183 loss = 0.000075\n",
      "Epoch: 0184 loss = 0.000055\n",
      "Epoch: 0185 loss = 0.000060\n",
      "Epoch: 0186 loss = 0.000071\n",
      "Epoch: 0187 loss = 0.000064\n",
      "Epoch: 0188 loss = 0.000061\n",
      "Epoch: 0189 loss = 0.000061\n",
      "Epoch: 0190 loss = 0.000030\n",
      "Epoch: 0191 loss = 0.000059\n",
      "Epoch: 0192 loss = 0.000052\n",
      "Epoch: 0193 loss = 0.000049\n",
      "Epoch: 0194 loss = 0.000082\n",
      "Epoch: 0195 loss = 0.000067\n",
      "Epoch: 0196 loss = 0.000071\n",
      "Epoch: 0197 loss = 0.000042\n",
      "Epoch: 0198 loss = 0.000062\n",
      "Epoch: 0199 loss = 0.000051\n",
      "Epoch: 0200 loss = 0.000059\n",
      "Epoch: 0201 loss = 0.000053\n",
      "Epoch: 0202 loss = 0.000042\n",
      "Epoch: 0203 loss = 0.000067\n",
      "Epoch: 0204 loss = 0.000019\n",
      "Epoch: 0205 loss = 0.000026\n",
      "Epoch: 0206 loss = 0.000047\n",
      "Epoch: 0207 loss = 0.000060\n",
      "Epoch: 0208 loss = 0.000152\n",
      "Epoch: 0209 loss = 0.000074\n",
      "Epoch: 0210 loss = 0.000042\n",
      "Epoch: 0211 loss = 0.000070\n",
      "Epoch: 0212 loss = 0.000037\n",
      "Epoch: 0213 loss = 0.000066\n",
      "Epoch: 0214 loss = 0.000107\n",
      "Epoch: 0215 loss = 0.000091\n",
      "Epoch: 0216 loss = 0.000026\n",
      "Epoch: 0217 loss = 0.000083\n",
      "Epoch: 0218 loss = 0.000034\n",
      "Epoch: 0219 loss = 0.000120\n",
      "Epoch: 0220 loss = 0.000085\n",
      "Epoch: 0221 loss = 0.000064\n",
      "Epoch: 0222 loss = 0.000059\n",
      "Epoch: 0223 loss = 0.000063\n",
      "Epoch: 0224 loss = 0.000048\n",
      "Epoch: 0225 loss = 0.000043\n",
      "Epoch: 0226 loss = 0.000076\n",
      "Epoch: 0227 loss = 0.000087\n",
      "Epoch: 0228 loss = 0.000071\n",
      "Epoch: 0229 loss = 0.000066\n",
      "Epoch: 0230 loss = 0.000045\n",
      "Epoch: 0231 loss = 0.000060\n",
      "Epoch: 0232 loss = 0.000047\n",
      "Epoch: 0233 loss = 0.000055\n",
      "Epoch: 0234 loss = 0.000069\n",
      "Epoch: 0235 loss = 0.000050\n",
      "Epoch: 0236 loss = 0.000061\n",
      "Epoch: 0237 loss = 0.000097\n",
      "Epoch: 0238 loss = 0.000102\n",
      "Epoch: 0239 loss = 0.000046\n",
      "Epoch: 0240 loss = 0.000066\n",
      "Epoch: 0241 loss = 0.000097\n",
      "Epoch: 0242 loss = 0.000050\n",
      "Epoch: 0243 loss = 0.000082\n",
      "Epoch: 0244 loss = 0.000081\n",
      "Epoch: 0245 loss = 0.000039\n",
      "Epoch: 0246 loss = 0.000059\n",
      "Epoch: 0247 loss = 0.000069\n",
      "Epoch: 0248 loss = 0.000085\n",
      "Epoch: 0249 loss = 0.000052\n",
      "Epoch: 0250 loss = 0.000077\n",
      "Epoch: 0251 loss = 0.000071\n",
      "Epoch: 0252 loss = 0.000111\n",
      "Epoch: 0253 loss = 0.000061\n",
      "Epoch: 0254 loss = 0.000080\n",
      "Epoch: 0255 loss = 0.000075\n",
      "Epoch: 0256 loss = 0.000127\n",
      "Epoch: 0257 loss = 0.000051\n",
      "Epoch: 0258 loss = 0.000068\n",
      "Epoch: 0259 loss = 0.000117\n",
      "Epoch: 0260 loss = 0.000042\n",
      "Epoch: 0261 loss = 0.000144\n",
      "Epoch: 0262 loss = 0.000043\n",
      "Epoch: 0263 loss = 0.000032\n",
      "Epoch: 0264 loss = 0.000180\n",
      "Epoch: 0265 loss = 0.000078\n",
      "Epoch: 0266 loss = 0.000073\n",
      "Epoch: 0267 loss = 0.000029\n",
      "Epoch: 0268 loss = 0.000030\n",
      "Epoch: 0269 loss = 0.000035\n",
      "Epoch: 0270 loss = 0.000021\n",
      "Epoch: 0271 loss = 0.000019\n",
      "Epoch: 0272 loss = 0.000028\n",
      "Epoch: 0273 loss = 0.000069\n",
      "Epoch: 0274 loss = 0.000018\n",
      "Epoch: 0275 loss = 0.000038\n",
      "Epoch: 0276 loss = 0.000053\n",
      "Epoch: 0277 loss = 0.000046\n",
      "Epoch: 0278 loss = 0.000030\n",
      "Epoch: 0279 loss = 0.000034\n",
      "Epoch: 0280 loss = 0.000022\n",
      "Epoch: 0281 loss = 0.000044\n",
      "Epoch: 0282 loss = 0.000023\n",
      "Epoch: 0283 loss = 0.000071\n",
      "Epoch: 0284 loss = 0.000032\n",
      "Epoch: 0285 loss = 0.000028\n",
      "Epoch: 0286 loss = 0.000025\n",
      "Epoch: 0287 loss = 0.000038\n",
      "Epoch: 0288 loss = 0.000035\n",
      "Epoch: 0289 loss = 0.000011\n",
      "Epoch: 0290 loss = 0.000059\n",
      "Epoch: 0291 loss = 0.000031\n",
      "Epoch: 0292 loss = 0.000042\n",
      "Epoch: 0293 loss = 0.000011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0294 loss = 0.000086\n",
      "Epoch: 0295 loss = 0.000044\n",
      "Epoch: 0296 loss = 0.000015\n",
      "Epoch: 0297 loss = 0.000013\n",
      "Epoch: 0298 loss = 0.000031\n",
      "Epoch: 0299 loss = 0.000012\n",
      "Epoch: 0300 loss = 0.000013\n",
      "Epoch: 0301 loss = 0.000020\n",
      "Epoch: 0302 loss = 0.000008\n",
      "Epoch: 0303 loss = 0.000017\n",
      "Epoch: 0304 loss = 0.000013\n",
      "Epoch: 0305 loss = 0.000020\n",
      "Epoch: 0306 loss = 0.000026\n",
      "Epoch: 0307 loss = 0.000031\n",
      "Epoch: 0308 loss = 0.000030\n",
      "Epoch: 0309 loss = 0.000015\n",
      "Epoch: 0310 loss = 0.000011\n",
      "Epoch: 0311 loss = 0.000013\n",
      "Epoch: 0312 loss = 0.000011\n",
      "Epoch: 0313 loss = 0.000024\n",
      "Epoch: 0314 loss = 0.000007\n",
      "Epoch: 0315 loss = 0.000018\n",
      "Epoch: 0316 loss = 0.000011\n",
      "Epoch: 0317 loss = 0.000012\n",
      "Epoch: 0318 loss = 0.000007\n",
      "Epoch: 0319 loss = 0.000013\n",
      "Epoch: 0320 loss = 0.000015\n",
      "Epoch: 0321 loss = 0.000026\n",
      "Epoch: 0322 loss = 0.000016\n",
      "Epoch: 0323 loss = 0.000018\n",
      "Epoch: 0324 loss = 0.000011\n",
      "Epoch: 0325 loss = 0.000015\n",
      "Epoch: 0326 loss = 0.000013\n",
      "Epoch: 0327 loss = 0.000015\n",
      "Epoch: 0328 loss = 0.000014\n",
      "Epoch: 0329 loss = 0.000007\n",
      "Epoch: 0330 loss = 0.000016\n",
      "Epoch: 0331 loss = 0.000005\n",
      "Epoch: 0332 loss = 0.000018\n",
      "Epoch: 0333 loss = 0.000007\n",
      "Epoch: 0334 loss = 0.000009\n",
      "Epoch: 0335 loss = 0.000013\n",
      "Epoch: 0336 loss = 0.000010\n",
      "Epoch: 0337 loss = 0.000008\n",
      "Epoch: 0338 loss = 0.000015\n",
      "Epoch: 0339 loss = 0.000009\n",
      "Epoch: 0340 loss = 0.000013\n",
      "Epoch: 0341 loss = 0.000008\n",
      "Epoch: 0342 loss = 0.000009\n",
      "Epoch: 0343 loss = 0.000009\n",
      "Epoch: 0344 loss = 0.000010\n",
      "Epoch: 0345 loss = 0.000006\n",
      "Epoch: 0346 loss = 0.000009\n",
      "Epoch: 0347 loss = 0.000005\n",
      "Epoch: 0348 loss = 0.000010\n",
      "Epoch: 0349 loss = 0.000013\n",
      "Epoch: 0350 loss = 0.000012\n",
      "Epoch: 0351 loss = 0.000018\n",
      "Epoch: 0352 loss = 0.000023\n",
      "Epoch: 0353 loss = 0.000011\n",
      "Epoch: 0354 loss = 0.000008\n",
      "Epoch: 0355 loss = 0.000011\n",
      "Epoch: 0356 loss = 0.000007\n",
      "Epoch: 0357 loss = 0.000011\n",
      "Epoch: 0358 loss = 0.000004\n",
      "Epoch: 0359 loss = 0.000010\n",
      "Epoch: 0360 loss = 0.000010\n",
      "Epoch: 0361 loss = 0.000006\n",
      "Epoch: 0362 loss = 0.000010\n",
      "Epoch: 0363 loss = 0.000006\n",
      "Epoch: 0364 loss = 0.000014\n",
      "Epoch: 0365 loss = 0.000005\n",
      "Epoch: 0366 loss = 0.000007\n",
      "Epoch: 0367 loss = 0.000004\n",
      "Epoch: 0368 loss = 0.000008\n",
      "Epoch: 0369 loss = 0.000007\n",
      "Epoch: 0370 loss = 0.000012\n",
      "Epoch: 0371 loss = 0.000006\n",
      "Epoch: 0372 loss = 0.000007\n",
      "Epoch: 0373 loss = 0.000006\n",
      "Epoch: 0374 loss = 0.000006\n",
      "Epoch: 0375 loss = 0.000003\n",
      "Epoch: 0376 loss = 0.000017\n",
      "Epoch: 0377 loss = 0.000005\n",
      "Epoch: 0378 loss = 0.000011\n",
      "Epoch: 0379 loss = 0.000012\n",
      "Epoch: 0380 loss = 0.000009\n",
      "Epoch: 0381 loss = 0.000011\n",
      "Epoch: 0382 loss = 0.000005\n",
      "Epoch: 0383 loss = 0.000007\n",
      "Epoch: 0384 loss = 0.000005\n",
      "Epoch: 0385 loss = 0.000003\n",
      "Epoch: 0386 loss = 0.000012\n",
      "Epoch: 0387 loss = 0.000010\n",
      "Epoch: 0388 loss = 0.000007\n",
      "Epoch: 0389 loss = 0.000003\n",
      "Epoch: 0390 loss = 0.000006\n",
      "Epoch: 0391 loss = 0.000007\n",
      "Epoch: 0392 loss = 0.000012\n",
      "Epoch: 0393 loss = 0.000008\n",
      "Epoch: 0394 loss = 0.000008\n",
      "Epoch: 0395 loss = 0.000007\n",
      "Epoch: 0396 loss = 0.000005\n",
      "Epoch: 0397 loss = 0.000005\n",
      "Epoch: 0398 loss = 0.000005\n",
      "Epoch: 0399 loss = 0.000006\n",
      "Epoch: 0400 loss = 0.000010\n",
      "Epoch: 0401 loss = 0.000007\n",
      "Epoch: 0402 loss = 0.000010\n",
      "Epoch: 0403 loss = 0.000011\n",
      "Epoch: 0404 loss = 0.000008\n",
      "Epoch: 0405 loss = 0.000007\n",
      "Epoch: 0406 loss = 0.000006\n",
      "Epoch: 0407 loss = 0.000004\n",
      "Epoch: 0408 loss = 0.000009\n",
      "Epoch: 0409 loss = 0.000008\n",
      "Epoch: 0410 loss = 0.000009\n",
      "Epoch: 0411 loss = 0.000011\n",
      "Epoch: 0412 loss = 0.000013\n",
      "Epoch: 0413 loss = 0.000015\n",
      "Epoch: 0414 loss = 0.000005\n",
      "Epoch: 0415 loss = 0.000004\n",
      "Epoch: 0416 loss = 0.000005\n",
      "Epoch: 0417 loss = 0.000005\n",
      "Epoch: 0418 loss = 0.000004\n",
      "Epoch: 0419 loss = 0.000004\n",
      "Epoch: 0420 loss = 0.000005\n",
      "Epoch: 0421 loss = 0.000004\n",
      "Epoch: 0422 loss = 0.000008\n",
      "Epoch: 0423 loss = 0.000005\n",
      "Epoch: 0424 loss = 0.000006\n",
      "Epoch: 0425 loss = 0.000006\n",
      "Epoch: 0426 loss = 0.000012\n",
      "Epoch: 0427 loss = 0.000006\n",
      "Epoch: 0428 loss = 0.000008\n",
      "Epoch: 0429 loss = 0.000004\n",
      "Epoch: 0430 loss = 0.000010\n",
      "Epoch: 0431 loss = 0.000006\n",
      "Epoch: 0432 loss = 0.000004\n",
      "Epoch: 0433 loss = 0.000003\n",
      "Epoch: 0434 loss = 0.000006\n",
      "Epoch: 0435 loss = 0.000004\n",
      "Epoch: 0436 loss = 0.000004\n",
      "Epoch: 0437 loss = 0.000007\n",
      "Epoch: 0438 loss = 0.000005\n",
      "Epoch: 0439 loss = 0.000004\n",
      "Epoch: 0440 loss = 0.000003\n",
      "Epoch: 0441 loss = 0.000004\n",
      "Epoch: 0442 loss = 0.000005\n",
      "Epoch: 0443 loss = 0.000003\n",
      "Epoch: 0444 loss = 0.000004\n",
      "Epoch: 0445 loss = 0.000007\n",
      "Epoch: 0446 loss = 0.000006\n",
      "Epoch: 0447 loss = 0.000004\n",
      "Epoch: 0448 loss = 0.000013\n",
      "Epoch: 0449 loss = 0.000004\n",
      "Epoch: 0450 loss = 0.000009\n",
      "Epoch: 0451 loss = 0.000004\n",
      "Epoch: 0452 loss = 0.000006\n",
      "Epoch: 0453 loss = 0.000003\n",
      "Epoch: 0454 loss = 0.000005\n",
      "Epoch: 0455 loss = 0.000006\n",
      "Epoch: 0456 loss = 0.000006\n",
      "Epoch: 0457 loss = 0.000007\n",
      "Epoch: 0458 loss = 0.000003\n",
      "Epoch: 0459 loss = 0.000007\n",
      "Epoch: 0460 loss = 0.000005\n",
      "Epoch: 0461 loss = 0.000006\n",
      "Epoch: 0462 loss = 0.000004\n",
      "Epoch: 0463 loss = 0.000003\n",
      "Epoch: 0464 loss = 0.000005\n",
      "Epoch: 0465 loss = 0.000003\n",
      "Epoch: 0466 loss = 0.000006\n",
      "Epoch: 0467 loss = 0.000003\n",
      "Epoch: 0468 loss = 0.000006\n",
      "Epoch: 0469 loss = 0.000004\n",
      "Epoch: 0470 loss = 0.000006\n",
      "Epoch: 0471 loss = 0.000005\n",
      "Epoch: 0472 loss = 0.000004\n",
      "Epoch: 0473 loss = 0.000004\n",
      "Epoch: 0474 loss = 0.000002\n",
      "Epoch: 0475 loss = 0.000007\n",
      "Epoch: 0476 loss = 0.000005\n",
      "Epoch: 0477 loss = 0.000004\n",
      "Epoch: 0478 loss = 0.000004\n",
      "Epoch: 0479 loss = 0.000006\n",
      "Epoch: 0480 loss = 0.000004\n",
      "Epoch: 0481 loss = 0.000010\n",
      "Epoch: 0482 loss = 0.000005\n",
      "Epoch: 0483 loss = 0.000005\n",
      "Epoch: 0484 loss = 0.000007\n",
      "Epoch: 0485 loss = 0.000004\n",
      "Epoch: 0486 loss = 0.000004\n",
      "Epoch: 0487 loss = 0.000003\n",
      "Epoch: 0488 loss = 0.000003\n",
      "Epoch: 0489 loss = 0.000005\n",
      "Epoch: 0490 loss = 0.000004\n",
      "Epoch: 0491 loss = 0.000006\n",
      "Epoch: 0492 loss = 0.000004\n",
      "Epoch: 0493 loss = 0.000004\n",
      "Epoch: 0494 loss = 0.000006\n",
      "Epoch: 0495 loss = 0.000007\n",
      "Epoch: 0496 loss = 0.000003\n",
      "Epoch: 0497 loss = 0.000006\n",
      "Epoch: 0498 loss = 0.000006\n",
      "Epoch: 0499 loss = 0.000002\n",
      "Epoch: 0500 loss = 0.000005\n",
      "Epoch: 0501 loss = 0.000003\n",
      "Epoch: 0502 loss = 0.000005\n",
      "Epoch: 0503 loss = 0.000007\n",
      "Epoch: 0504 loss = 0.000006\n",
      "Epoch: 0505 loss = 0.000006\n",
      "Epoch: 0506 loss = 0.000004\n",
      "Epoch: 0507 loss = 0.000003\n",
      "Epoch: 0508 loss = 0.000005\n",
      "Epoch: 0509 loss = 0.000004\n",
      "Epoch: 0510 loss = 0.000003\n",
      "Epoch: 0511 loss = 0.000007\n",
      "Epoch: 0512 loss = 0.000004\n",
      "Epoch: 0513 loss = 0.000004\n",
      "Epoch: 0514 loss = 0.000003\n",
      "Epoch: 0515 loss = 0.000003\n",
      "Epoch: 0516 loss = 0.000006\n",
      "Epoch: 0517 loss = 0.000003\n",
      "Epoch: 0518 loss = 0.000008\n",
      "Epoch: 0519 loss = 0.000009\n",
      "Epoch: 0520 loss = 0.000010\n",
      "Epoch: 0521 loss = 0.000005\n",
      "Epoch: 0522 loss = 0.000003\n",
      "Epoch: 0523 loss = 0.000004\n",
      "Epoch: 0524 loss = 0.000002\n",
      "Epoch: 0525 loss = 0.000005\n",
      "Epoch: 0526 loss = 0.000007\n",
      "Epoch: 0527 loss = 0.000004\n",
      "Epoch: 0528 loss = 0.000003\n",
      "Epoch: 0529 loss = 0.000003\n",
      "Epoch: 0530 loss = 0.000011\n",
      "Epoch: 0531 loss = 0.000007\n",
      "Epoch: 0532 loss = 0.000005\n",
      "Epoch: 0533 loss = 0.000005\n",
      "Epoch: 0534 loss = 0.000003\n",
      "Epoch: 0535 loss = 0.000005\n",
      "Epoch: 0536 loss = 0.000003\n",
      "Epoch: 0537 loss = 0.000005\n",
      "Epoch: 0538 loss = 0.000005\n",
      "Epoch: 0539 loss = 0.000004\n",
      "Epoch: 0540 loss = 0.000004\n",
      "Epoch: 0541 loss = 0.000003\n",
      "Epoch: 0542 loss = 0.000007\n",
      "Epoch: 0543 loss = 0.000005\n",
      "Epoch: 0544 loss = 0.000003\n",
      "Epoch: 0545 loss = 0.000003\n",
      "Epoch: 0546 loss = 0.000003\n",
      "Epoch: 0547 loss = 0.000003\n",
      "Epoch: 0548 loss = 0.000005\n",
      "Epoch: 0549 loss = 0.000003\n",
      "Epoch: 0550 loss = 0.000008\n",
      "Epoch: 0551 loss = 0.000003\n",
      "Epoch: 0552 loss = 0.000005\n",
      "Epoch: 0553 loss = 0.000005\n",
      "Epoch: 0554 loss = 0.000003\n",
      "Epoch: 0555 loss = 0.000003\n",
      "Epoch: 0556 loss = 0.000008\n",
      "Epoch: 0557 loss = 0.000004\n",
      "Epoch: 0558 loss = 0.000003\n",
      "Epoch: 0559 loss = 0.000005\n",
      "Epoch: 0560 loss = 0.000005\n",
      "Epoch: 0561 loss = 0.000003\n",
      "Epoch: 0562 loss = 0.000003\n",
      "Epoch: 0563 loss = 0.000003\n",
      "Epoch: 0564 loss = 0.000006\n",
      "Epoch: 0565 loss = 0.000005\n",
      "Epoch: 0566 loss = 0.000003\n",
      "Epoch: 0567 loss = 0.000006\n",
      "Epoch: 0568 loss = 0.000003\n",
      "Epoch: 0569 loss = 0.000003\n",
      "Epoch: 0570 loss = 0.000004\n",
      "Epoch: 0571 loss = 0.000007\n",
      "Epoch: 0572 loss = 0.000004\n",
      "Epoch: 0573 loss = 0.000004\n",
      "Epoch: 0574 loss = 0.000007\n",
      "Epoch: 0575 loss = 0.000005\n",
      "Epoch: 0576 loss = 0.000002\n",
      "Epoch: 0577 loss = 0.000003\n",
      "Epoch: 0578 loss = 0.000004\n",
      "Epoch: 0579 loss = 0.000003\n",
      "Epoch: 0580 loss = 0.000003\n",
      "Epoch: 0581 loss = 0.000003\n",
      "Epoch: 0582 loss = 0.000003\n",
      "Epoch: 0583 loss = 0.000006\n",
      "Epoch: 0584 loss = 0.000004\n",
      "Epoch: 0585 loss = 0.000004\n",
      "Epoch: 0586 loss = 0.000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0587 loss = 0.000003\n",
      "Epoch: 0588 loss = 0.000003\n",
      "Epoch: 0589 loss = 0.000003\n",
      "Epoch: 0590 loss = 0.000003\n",
      "Epoch: 0591 loss = 0.000002\n",
      "Epoch: 0592 loss = 0.000004\n",
      "Epoch: 0593 loss = 0.000003\n",
      "Epoch: 0594 loss = 0.000004\n",
      "Epoch: 0595 loss = 0.000003\n",
      "Epoch: 0596 loss = 0.000006\n",
      "Epoch: 0597 loss = 0.000003\n",
      "Epoch: 0598 loss = 0.000005\n",
      "Epoch: 0599 loss = 0.000004\n",
      "Epoch: 0600 loss = 0.000006\n",
      "Epoch: 0601 loss = 0.000004\n",
      "Epoch: 0602 loss = 0.000004\n",
      "Epoch: 0603 loss = 0.000006\n",
      "Epoch: 0604 loss = 0.000003\n",
      "Epoch: 0605 loss = 0.000005\n",
      "Epoch: 0606 loss = 0.000003\n",
      "Epoch: 0607 loss = 0.000004\n",
      "Epoch: 0608 loss = 0.000003\n",
      "Epoch: 0609 loss = 0.000003\n",
      "Epoch: 0610 loss = 0.000003\n",
      "Epoch: 0611 loss = 0.000003\n",
      "Epoch: 0612 loss = 0.000004\n",
      "Epoch: 0613 loss = 0.000002\n",
      "Epoch: 0614 loss = 0.000003\n",
      "Epoch: 0615 loss = 0.000003\n",
      "Epoch: 0616 loss = 0.000005\n",
      "Epoch: 0617 loss = 0.000003\n",
      "Epoch: 0618 loss = 0.000003\n",
      "Epoch: 0619 loss = 0.000003\n",
      "Epoch: 0620 loss = 0.000004\n",
      "Epoch: 0621 loss = 0.000003\n",
      "Epoch: 0622 loss = 0.000003\n",
      "Epoch: 0623 loss = 0.000003\n",
      "Epoch: 0624 loss = 0.000004\n",
      "Epoch: 0625 loss = 0.000003\n",
      "Epoch: 0626 loss = 0.000002\n",
      "Epoch: 0627 loss = 0.000003\n",
      "Epoch: 0628 loss = 0.000003\n",
      "Epoch: 0629 loss = 0.000003\n",
      "Epoch: 0630 loss = 0.000006\n",
      "Epoch: 0631 loss = 0.000004\n",
      "Epoch: 0632 loss = 0.000004\n",
      "Epoch: 0633 loss = 0.000003\n",
      "Epoch: 0634 loss = 0.000005\n",
      "Epoch: 0635 loss = 0.000003\n",
      "Epoch: 0636 loss = 0.000003\n",
      "Epoch: 0637 loss = 0.000004\n",
      "Epoch: 0638 loss = 0.000005\n",
      "Epoch: 0639 loss = 0.000006\n",
      "Epoch: 0640 loss = 0.000005\n",
      "Epoch: 0641 loss = 0.000008\n",
      "Epoch: 0642 loss = 0.000006\n",
      "Epoch: 0643 loss = 0.000002\n",
      "Epoch: 0644 loss = 0.000004\n",
      "Epoch: 0645 loss = 0.000003\n",
      "Epoch: 0646 loss = 0.000005\n",
      "Epoch: 0647 loss = 0.000002\n",
      "Epoch: 0648 loss = 0.000002\n",
      "Epoch: 0649 loss = 0.000002\n",
      "Epoch: 0650 loss = 0.000004\n",
      "Epoch: 0651 loss = 0.000003\n",
      "Epoch: 0652 loss = 0.000006\n",
      "Epoch: 0653 loss = 0.000004\n",
      "Epoch: 0654 loss = 0.000005\n",
      "Epoch: 0655 loss = 0.000002\n",
      "Epoch: 0656 loss = 0.000003\n",
      "Epoch: 0657 loss = 0.000003\n",
      "Epoch: 0658 loss = 0.000003\n",
      "Epoch: 0659 loss = 0.000005\n",
      "Epoch: 0660 loss = 0.000002\n",
      "Epoch: 0661 loss = 0.000004\n",
      "Epoch: 0662 loss = 0.000002\n",
      "Epoch: 0663 loss = 0.000004\n",
      "Epoch: 0664 loss = 0.000002\n",
      "Epoch: 0665 loss = 0.000004\n",
      "Epoch: 0666 loss = 0.000003\n",
      "Epoch: 0667 loss = 0.000002\n",
      "Epoch: 0668 loss = 0.000003\n",
      "Epoch: 0669 loss = 0.000006\n",
      "Epoch: 0670 loss = 0.000003\n",
      "Epoch: 0671 loss = 0.000003\n",
      "Epoch: 0672 loss = 0.000003\n",
      "Epoch: 0673 loss = 0.000003\n",
      "Epoch: 0674 loss = 0.000002\n",
      "Epoch: 0675 loss = 0.000005\n",
      "Epoch: 0676 loss = 0.000001\n",
      "Epoch: 0677 loss = 0.000002\n",
      "Epoch: 0678 loss = 0.000002\n",
      "Epoch: 0679 loss = 0.000004\n",
      "Epoch: 0680 loss = 0.000004\n",
      "Epoch: 0681 loss = 0.000002\n",
      "Epoch: 0682 loss = 0.000002\n",
      "Epoch: 0683 loss = 0.000002\n",
      "Epoch: 0684 loss = 0.000004\n",
      "Epoch: 0685 loss = 0.000006\n",
      "Epoch: 0686 loss = 0.000003\n",
      "Epoch: 0687 loss = 0.000002\n",
      "Epoch: 0688 loss = 0.000004\n",
      "Epoch: 0689 loss = 0.000002\n",
      "Epoch: 0690 loss = 0.000002\n",
      "Epoch: 0691 loss = 0.000003\n",
      "Epoch: 0692 loss = 0.000003\n",
      "Epoch: 0693 loss = 0.000002\n",
      "Epoch: 0694 loss = 0.000002\n",
      "Epoch: 0695 loss = 0.000002\n",
      "Epoch: 0696 loss = 0.000002\n",
      "Epoch: 0697 loss = 0.000003\n",
      "Epoch: 0698 loss = 0.000002\n",
      "Epoch: 0699 loss = 0.000002\n",
      "Epoch: 0700 loss = 0.000003\n",
      "Epoch: 0701 loss = 0.000003\n",
      "Epoch: 0702 loss = 0.000003\n",
      "Epoch: 0703 loss = 0.000003\n",
      "Epoch: 0704 loss = 0.000002\n",
      "Epoch: 0705 loss = 0.000002\n",
      "Epoch: 0706 loss = 0.000002\n",
      "Epoch: 0707 loss = 0.000005\n",
      "Epoch: 0708 loss = 0.000003\n",
      "Epoch: 0709 loss = 0.000003\n",
      "Epoch: 0710 loss = 0.000002\n",
      "Epoch: 0711 loss = 0.000002\n",
      "Epoch: 0712 loss = 0.000002\n",
      "Epoch: 0713 loss = 0.000001\n",
      "Epoch: 0714 loss = 0.000002\n",
      "Epoch: 0715 loss = 0.000002\n",
      "Epoch: 0716 loss = 0.000003\n",
      "Epoch: 0717 loss = 0.000003\n",
      "Epoch: 0718 loss = 0.000003\n",
      "Epoch: 0719 loss = 0.000003\n",
      "Epoch: 0720 loss = 0.000005\n",
      "Epoch: 0721 loss = 0.000003\n",
      "Epoch: 0722 loss = 0.000002\n",
      "Epoch: 0723 loss = 0.000003\n",
      "Epoch: 0724 loss = 0.000002\n",
      "Epoch: 0725 loss = 0.000003\n",
      "Epoch: 0726 loss = 0.000001\n",
      "Epoch: 0727 loss = 0.000004\n",
      "Epoch: 0728 loss = 0.000002\n",
      "Epoch: 0729 loss = 0.000003\n",
      "Epoch: 0730 loss = 0.000003\n",
      "Epoch: 0731 loss = 0.000002\n",
      "Epoch: 0732 loss = 0.000002\n",
      "Epoch: 0733 loss = 0.000003\n",
      "Epoch: 0734 loss = 0.000004\n",
      "Epoch: 0735 loss = 0.000003\n",
      "Epoch: 0736 loss = 0.000003\n",
      "Epoch: 0737 loss = 0.000004\n",
      "Epoch: 0738 loss = 0.000003\n",
      "Epoch: 0739 loss = 0.000003\n",
      "Epoch: 0740 loss = 0.000002\n",
      "Epoch: 0741 loss = 0.000003\n",
      "Epoch: 0742 loss = 0.000002\n",
      "Epoch: 0743 loss = 0.000003\n",
      "Epoch: 0744 loss = 0.000003\n",
      "Epoch: 0745 loss = 0.000002\n",
      "Epoch: 0746 loss = 0.000003\n",
      "Epoch: 0747 loss = 0.000002\n",
      "Epoch: 0748 loss = 0.000003\n",
      "Epoch: 0749 loss = 0.000002\n",
      "Epoch: 0750 loss = 0.000002\n",
      "Epoch: 0751 loss = 0.000003\n",
      "Epoch: 0752 loss = 0.000002\n",
      "Epoch: 0753 loss = 0.000002\n",
      "Epoch: 0754 loss = 0.000004\n",
      "Epoch: 0755 loss = 0.000002\n",
      "Epoch: 0756 loss = 0.000003\n",
      "Epoch: 0757 loss = 0.000002\n",
      "Epoch: 0758 loss = 0.000003\n",
      "Epoch: 0759 loss = 0.000002\n",
      "Epoch: 0760 loss = 0.000002\n",
      "Epoch: 0761 loss = 0.000003\n",
      "Epoch: 0762 loss = 0.000003\n",
      "Epoch: 0763 loss = 0.000003\n",
      "Epoch: 0764 loss = 0.000002\n",
      "Epoch: 0765 loss = 0.000003\n",
      "Epoch: 0766 loss = 0.000005\n",
      "Epoch: 0767 loss = 0.000005\n",
      "Epoch: 0768 loss = 0.000002\n",
      "Epoch: 0769 loss = 0.000002\n",
      "Epoch: 0770 loss = 0.000002\n",
      "Epoch: 0771 loss = 0.000003\n",
      "Epoch: 0772 loss = 0.000003\n",
      "Epoch: 0773 loss = 0.000002\n",
      "Epoch: 0774 loss = 0.000003\n",
      "Epoch: 0775 loss = 0.000002\n",
      "Epoch: 0776 loss = 0.000004\n",
      "Epoch: 0777 loss = 0.000003\n",
      "Epoch: 0778 loss = 0.000003\n",
      "Epoch: 0779 loss = 0.000003\n",
      "Epoch: 0780 loss = 0.000002\n",
      "Epoch: 0781 loss = 0.000002\n",
      "Epoch: 0782 loss = 0.000002\n",
      "Epoch: 0783 loss = 0.000003\n",
      "Epoch: 0784 loss = 0.000002\n",
      "Epoch: 0785 loss = 0.000002\n",
      "Epoch: 0786 loss = 0.000002\n",
      "Epoch: 0787 loss = 0.000005\n",
      "Epoch: 0788 loss = 0.000002\n",
      "Epoch: 0789 loss = 0.000002\n",
      "Epoch: 0790 loss = 0.000004\n",
      "Epoch: 0791 loss = 0.000002\n",
      "Epoch: 0792 loss = 0.000005\n",
      "Epoch: 0793 loss = 0.000002\n",
      "Epoch: 0794 loss = 0.000002\n",
      "Epoch: 0795 loss = 0.000003\n",
      "Epoch: 0796 loss = 0.000005\n",
      "Epoch: 0797 loss = 0.000003\n",
      "Epoch: 0798 loss = 0.000003\n",
      "Epoch: 0799 loss = 0.000002\n",
      "Epoch: 0800 loss = 0.000002\n",
      "Epoch: 0801 loss = 0.000002\n",
      "Epoch: 0802 loss = 0.000003\n",
      "Epoch: 0803 loss = 0.000002\n",
      "Epoch: 0804 loss = 0.000003\n",
      "Epoch: 0805 loss = 0.000002\n",
      "Epoch: 0806 loss = 0.000002\n",
      "Epoch: 0807 loss = 0.000002\n",
      "Epoch: 0808 loss = 0.000004\n",
      "Epoch: 0809 loss = 0.000002\n",
      "Epoch: 0810 loss = 0.000002\n",
      "Epoch: 0811 loss = 0.000002\n",
      "Epoch: 0812 loss = 0.000002\n",
      "Epoch: 0813 loss = 0.000002\n",
      "Epoch: 0814 loss = 0.000002\n",
      "Epoch: 0815 loss = 0.000002\n",
      "Epoch: 0816 loss = 0.000002\n",
      "Epoch: 0817 loss = 0.000004\n",
      "Epoch: 0818 loss = 0.000005\n",
      "Epoch: 0819 loss = 0.000002\n",
      "Epoch: 0820 loss = 0.000001\n",
      "Epoch: 0821 loss = 0.000002\n",
      "Epoch: 0822 loss = 0.000002\n",
      "Epoch: 0823 loss = 0.000002\n",
      "Epoch: 0824 loss = 0.000003\n",
      "Epoch: 0825 loss = 0.000003\n",
      "Epoch: 0826 loss = 0.000003\n",
      "Epoch: 0827 loss = 0.000001\n",
      "Epoch: 0828 loss = 0.000002\n",
      "Epoch: 0829 loss = 0.000002\n",
      "Epoch: 0830 loss = 0.000005\n",
      "Epoch: 0831 loss = 0.000002\n",
      "Epoch: 0832 loss = 0.000001\n",
      "Epoch: 0833 loss = 0.000002\n",
      "Epoch: 0834 loss = 0.000003\n",
      "Epoch: 0835 loss = 0.000002\n",
      "Epoch: 0836 loss = 0.000002\n",
      "Epoch: 0837 loss = 0.000003\n",
      "Epoch: 0838 loss = 0.000001\n",
      "Epoch: 0839 loss = 0.000001\n",
      "Epoch: 0840 loss = 0.000002\n",
      "Epoch: 0841 loss = 0.000001\n",
      "Epoch: 0842 loss = 0.000002\n",
      "Epoch: 0843 loss = 0.000003\n",
      "Epoch: 0844 loss = 0.000002\n",
      "Epoch: 0845 loss = 0.000003\n",
      "Epoch: 0846 loss = 0.000002\n",
      "Epoch: 0847 loss = 0.000005\n",
      "Epoch: 0848 loss = 0.000002\n",
      "Epoch: 0849 loss = 0.000003\n",
      "Epoch: 0850 loss = 0.000002\n",
      "Epoch: 0851 loss = 0.000002\n",
      "Epoch: 0852 loss = 0.000002\n",
      "Epoch: 0853 loss = 0.000002\n",
      "Epoch: 0854 loss = 0.000002\n",
      "Epoch: 0855 loss = 0.000002\n",
      "Epoch: 0856 loss = 0.000002\n",
      "Epoch: 0857 loss = 0.000002\n",
      "Epoch: 0858 loss = 0.000001\n",
      "Epoch: 0859 loss = 0.000003\n",
      "Epoch: 0860 loss = 0.000002\n",
      "Epoch: 0861 loss = 0.000002\n",
      "Epoch: 0862 loss = 0.000002\n",
      "Epoch: 0863 loss = 0.000003\n",
      "Epoch: 0864 loss = 0.000003\n",
      "Epoch: 0865 loss = 0.000002\n",
      "Epoch: 0866 loss = 0.000002\n",
      "Epoch: 0867 loss = 0.000003\n",
      "Epoch: 0868 loss = 0.000002\n",
      "Epoch: 0869 loss = 0.000003\n",
      "Epoch: 0870 loss = 0.000002\n",
      "Epoch: 0871 loss = 0.000002\n",
      "Epoch: 0872 loss = 0.000001\n",
      "Epoch: 0873 loss = 0.000003\n",
      "Epoch: 0874 loss = 0.000002\n",
      "Epoch: 0875 loss = 0.000002\n",
      "Epoch: 0876 loss = 0.000002\n",
      "Epoch: 0877 loss = 0.000002\n",
      "Epoch: 0878 loss = 0.000004\n",
      "Epoch: 0879 loss = 0.000003\n",
      "Epoch: 0880 loss = 0.000003\n",
      "Epoch: 0881 loss = 0.000001\n",
      "Epoch: 0882 loss = 0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0883 loss = 0.000003\n",
      "Epoch: 0884 loss = 0.000002\n",
      "Epoch: 0885 loss = 0.000001\n",
      "Epoch: 0886 loss = 0.000003\n",
      "Epoch: 0887 loss = 0.000002\n",
      "Epoch: 0888 loss = 0.000002\n",
      "Epoch: 0889 loss = 0.000002\n",
      "Epoch: 0890 loss = 0.000002\n",
      "Epoch: 0891 loss = 0.000002\n",
      "Epoch: 0892 loss = 0.000003\n",
      "Epoch: 0893 loss = 0.000002\n",
      "Epoch: 0894 loss = 0.000002\n",
      "Epoch: 0895 loss = 0.000001\n",
      "Epoch: 0896 loss = 0.000003\n",
      "Epoch: 0897 loss = 0.000003\n",
      "Epoch: 0898 loss = 0.000003\n",
      "Epoch: 0899 loss = 0.000003\n",
      "Epoch: 0900 loss = 0.000003\n",
      "Epoch: 0901 loss = 0.000002\n",
      "Epoch: 0902 loss = 0.000002\n",
      "Epoch: 0903 loss = 0.000002\n",
      "Epoch: 0904 loss = 0.000002\n",
      "Epoch: 0905 loss = 0.000001\n",
      "Epoch: 0906 loss = 0.000002\n",
      "Epoch: 0907 loss = 0.000002\n",
      "Epoch: 0908 loss = 0.000001\n",
      "Epoch: 0909 loss = 0.000003\n",
      "Epoch: 0910 loss = 0.000003\n",
      "Epoch: 0911 loss = 0.000002\n",
      "Epoch: 0912 loss = 0.000003\n",
      "Epoch: 0913 loss = 0.000002\n",
      "Epoch: 0914 loss = 0.000002\n",
      "Epoch: 0915 loss = 0.000002\n",
      "Epoch: 0916 loss = 0.000002\n",
      "Epoch: 0917 loss = 0.000001\n",
      "Epoch: 0918 loss = 0.000002\n",
      "Epoch: 0919 loss = 0.000003\n",
      "Epoch: 0920 loss = 0.000002\n",
      "Epoch: 0921 loss = 0.000003\n",
      "Epoch: 0922 loss = 0.000005\n",
      "Epoch: 0923 loss = 0.000001\n",
      "Epoch: 0924 loss = 0.000003\n",
      "Epoch: 0925 loss = 0.000002\n",
      "Epoch: 0926 loss = 0.000002\n",
      "Epoch: 0927 loss = 0.000002\n",
      "Epoch: 0928 loss = 0.000001\n",
      "Epoch: 0929 loss = 0.000003\n",
      "Epoch: 0930 loss = 0.000002\n",
      "Epoch: 0931 loss = 0.000002\n",
      "Epoch: 0932 loss = 0.000003\n",
      "Epoch: 0933 loss = 0.000003\n",
      "Epoch: 0934 loss = 0.000003\n",
      "Epoch: 0935 loss = 0.000003\n",
      "Epoch: 0936 loss = 0.000002\n",
      "Epoch: 0937 loss = 0.000002\n",
      "Epoch: 0938 loss = 0.000002\n",
      "Epoch: 0939 loss = 0.000003\n",
      "Epoch: 0940 loss = 0.000002\n",
      "Epoch: 0941 loss = 0.000002\n",
      "Epoch: 0942 loss = 0.000002\n",
      "Epoch: 0943 loss = 0.000001\n",
      "Epoch: 0944 loss = 0.000003\n",
      "Epoch: 0945 loss = 0.000002\n",
      "Epoch: 0946 loss = 0.000003\n",
      "Epoch: 0947 loss = 0.000002\n",
      "Epoch: 0948 loss = 0.000003\n",
      "Epoch: 0949 loss = 0.000003\n",
      "Epoch: 0950 loss = 0.000001\n",
      "Epoch: 0951 loss = 0.000002\n",
      "Epoch: 0952 loss = 0.000002\n",
      "Epoch: 0953 loss = 0.000002\n",
      "Epoch: 0954 loss = 0.000003\n",
      "Epoch: 0955 loss = 0.000003\n",
      "Epoch: 0956 loss = 0.000002\n",
      "Epoch: 0957 loss = 0.000002\n",
      "Epoch: 0958 loss = 0.000004\n",
      "Epoch: 0959 loss = 0.000003\n",
      "Epoch: 0960 loss = 0.000003\n",
      "Epoch: 0961 loss = 0.000001\n",
      "Epoch: 0962 loss = 0.000002\n",
      "Epoch: 0963 loss = 0.000001\n",
      "Epoch: 0964 loss = 0.000003\n",
      "Epoch: 0965 loss = 0.000002\n",
      "Epoch: 0966 loss = 0.000003\n",
      "Epoch: 0967 loss = 0.000002\n",
      "Epoch: 0968 loss = 0.000002\n",
      "Epoch: 0969 loss = 0.000003\n",
      "Epoch: 0970 loss = 0.000002\n",
      "Epoch: 0971 loss = 0.000003\n",
      "Epoch: 0972 loss = 0.000003\n",
      "Epoch: 0973 loss = 0.000002\n",
      "Epoch: 0974 loss = 0.000003\n",
      "Epoch: 0975 loss = 0.000002\n",
      "Epoch: 0976 loss = 0.000002\n",
      "Epoch: 0977 loss = 0.000001\n",
      "Epoch: 0978 loss = 0.000003\n",
      "Epoch: 0979 loss = 0.000002\n",
      "Epoch: 0980 loss = 0.000002\n",
      "Epoch: 0981 loss = 0.000003\n",
      "Epoch: 0982 loss = 0.000002\n",
      "Epoch: 0983 loss = 0.000002\n",
      "Epoch: 0984 loss = 0.000003\n",
      "Epoch: 0985 loss = 0.000003\n",
      "Epoch: 0986 loss = 0.000002\n",
      "Epoch: 0987 loss = 0.000002\n",
      "Epoch: 0988 loss = 0.000003\n",
      "Epoch: 0989 loss = 0.000002\n",
      "Epoch: 0990 loss = 0.000003\n",
      "Epoch: 0991 loss = 0.000002\n",
      "Epoch: 0992 loss = 0.000003\n",
      "Epoch: 0993 loss = 0.000002\n",
      "Epoch: 0994 loss = 0.000001\n",
      "Epoch: 0995 loss = 0.000002\n",
      "Epoch: 0996 loss = 0.000002\n",
      "Epoch: 0997 loss = 0.000001\n",
      "Epoch: 0998 loss = 0.000002\n",
      "Epoch: 0999 loss = 0.000001\n",
      "Epoch: 1000 loss = 0.000004\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
    "      '''\n",
    "      enc_inputs: [batch_size, src_len]\n",
    "      dec_inputs: [batch_size, tgt_len]\n",
    "      dec_outputs: [batch_size, tgt_len]\n",
    "      '''\n",
    "      enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs.cuda()\n",
    "      # outputs: [batch_size * tgt_len, tgt_vocab_size]\n",
    "      outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "      loss = criterion(outputs, dec_outputs.view(-1))\n",
    "      print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGLlWp3gCl1X"
   },
   "outputs": [],
   "source": [
    "def greedy_decoder(model, enc_input, start_symbol):\n",
    "    \"\"\"\n",
    "    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\n",
    "    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\n",
    "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
    "    :param model: Transformer Model\n",
    "    :param enc_input: The encoder input\n",
    "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\n",
    "    :return: The target input\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
    "    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n",
    "    terminal = False\n",
    "    next_symbol = start_symbol\n",
    "    while not terminal:         \n",
    "        dec_input = torch.cat([dec_input.detach(),torch.tensor([[next_symbol]],dtype=enc_input.dtype).cuda()],-1)\n",
    "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
    "        projected = model.projection(dec_outputs)\n",
    "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "        next_word = prob.data[-1]\n",
    "        next_symbol = next_word\n",
    "        if next_symbol == tgt_vocab[\".\"]:\n",
    "            terminal = True\n",
    "        print(next_word)            \n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFsf8gHrgAl1"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "enc_inputs, _, _ = next(iter(loader))\n",
    "enc_inputs = enc_inputs.cuda()\n",
    "for i in range(len(enc_inputs)):\n",
    "    greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab[\"S\"])\n",
    "    predict, _, _, _ = model(enc_inputs[i].view(1, -1), greedy_dec_input)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2kTP5eQCPzB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "“Transformer-Torch”的副本",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
