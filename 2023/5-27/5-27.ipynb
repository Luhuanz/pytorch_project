{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ea52d1-55d3-4bbe-91db-8fb0e42ffd86",
   "metadata": {},
   "source": [
    "## Logistic Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f144e0-a079-4186-8c18-176eb78c95fc",
   "metadata": {},
   "source": [
    "$$ H(X) = \\frac{1}{1+e^{-W^T X}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5677c97-e57c-4970-8b0d-d5b08badd044",
   "metadata": {},
   "source": [
    "$$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4666ffca-5dac-4d89-a8e6-b7b6ae2f0edf",
   "metadata": {},
   "source": [
    "Descent\n",
    "$$ W := W - \\alpha \\frac{\\partial}{\\partial W} cost(W) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243e0522-d598-4742-a023-2237aa3f23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e3d39d-9efd-4d03-8087-539d16cfa3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f89a8191990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adebbbc0-121c-4311-aeb9-ea8b972fc4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=[[1,2],\n",
    "        [2,3],\n",
    "        [3,1],\n",
    "        [4,3],\n",
    "        [5,3],\n",
    "        [6,2],\n",
    "       ]\n",
    "y_data=[[0],[0],[0],[1],[1],[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a04f68-c1a4-446d-a6c1-21a56936efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=torch.FloatTensor(x_data)\n",
    "y_train=torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031873e9-82dd-4375-ab6b-045859d2d145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "723d03e6-4c54-486c-8c2f-08e199bed1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4101986c-e9a9-44d4-9f3e-5eaed15a7dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.matmul(W)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f66ec70-02c1-4385-8faa-d526b9859733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.FloatTensor([1]\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f5443a-5452-478f-9d94-4acd32c17a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=torch.zeros((2,1),requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "y_pred=1/(1+torch.exp(-(x_train.matmul(W)+b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa346f2-7f62-49b5-931c-95c6d65efb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e501662a-1b39-45ba-b514-418891a6a119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(y_train[0]*torch.log(y_pred[0])+(1-y_train[0])*torch.log(1-y_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7a1a7d-4e69-4760-9b40-d20cbc9c1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=-(y_train*torch.log(y_pred)+(1-y_train)*torch.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "130a96f6-2f3a-4fc4-86ce-8f137656608e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6931],\n",
       "        [0.6931],\n",
       "        [0.6931],\n",
       "        [0.6931],\n",
       "        [0.6931],\n",
       "        [0.6931]], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "250d64e6-67dc-4409-852f-7f8d04989900",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce72c339-0a17-4e03-88c0-b8f951d3a1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a26f788f-a7ca-4a03-9f5a-59382ace7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=[[1,2],\n",
    "        [2,3],\n",
    "        [3,1],\n",
    "        [4,3],\n",
    "        [5,3],\n",
    "        [6,2],\n",
    "       ]\n",
    "y_data=[[0],[0],[0],[1],[1],[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eaf0e5a-09fa-4cef-b3cf-8806000f31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=torch.FloatTensor(x_data)\n",
    "y_train=torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fe82996-2265-4779-9766-d3fa1f6c3e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cde94bd-15eb-433c-8b74-66881432eb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:    0/1000  loss： 0.693147 \n",
      "epochs:  100/1000  loss： 0.653620 \n",
      "epochs:  200/1000  loss： 0.634803 \n",
      "epochs:  300/1000  loss： 0.623416 \n",
      "epochs:  400/1000  loss： 0.614912 \n",
      "epochs:  500/1000  loss： 0.607670 \n",
      "epochs:  600/1000  loss： 0.601084 \n",
      "epochs:  700/1000  loss： 0.594916 \n",
      "epochs:  800/1000  loss： 0.589064 \n",
      "epochs:  900/1000  loss： 0.583481 \n",
      "epochs: 1000/1000  loss： 0.578142 \n"
     ]
    }
   ],
   "source": [
    "W=torch.zeros((2,1),requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "\n",
    "optimizer=optim.SGD([W,b],lr=1e-3)\n",
    "epochs=1000\n",
    "for i in range(epochs+1):\n",
    "    y_pred=1/(1+torch.exp(-(x_train.matmul(W)+b)))\n",
    "    loss=torch.mean(-(y_train*torch.log(y_pred)+(1-y_train)*torch.log(1-y_pred)))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%100==0:\n",
    "        print(\"epochs: {:4d}/{}  loss： {:.6f} \" .format(i,epochs,loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0dfca2c-ff1c-44bd-b3a7-9d20f6b7847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:    0/1000  loss： 0.693147 \n",
      "epochs:  100/1000  loss： 0.653620 \n",
      "epochs:  200/1000  loss： 0.634803 \n",
      "epochs:  300/1000  loss： 0.623416 \n",
      "epochs:  400/1000  loss： 0.614912 \n",
      "epochs:  500/1000  loss： 0.607670 \n",
      "epochs:  600/1000  loss： 0.601084 \n",
      "epochs:  700/1000  loss： 0.594916 \n",
      "epochs:  800/1000  loss： 0.589064 \n",
      "epochs:  900/1000  loss： 0.583481 \n",
      "epochs: 1000/1000  loss： 0.578142 \n"
     ]
    }
   ],
   "source": [
    "W=torch.zeros((2,1),requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "\n",
    "optimizer=optim.SGD([W,b],lr=1e-3)\n",
    "epochs=1000\n",
    "for i in range(epochs+1):\n",
    "    y_pred=torch.sigmoid(x_train.matmul(W)+b)\n",
    "    loss=F.binary_cross_entropy(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%100==0:\n",
    "        print(\"epochs: {:4d}/{}  loss： {:.6f} \" .format(i,epochs,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31a24337-5347-40d4-be53-cf3be20e5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f855fde8-2ef1-4d4e-bafe-abcc148286a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4141935d-0f96-4e8d-951a-4a2835605361",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy=np.loadtxt('data-03-diabetes.csv',delimiter=',',dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b51ba14-dc71-4c11-a92b-b64771c4df59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759, 9)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e7e94b0-5f6c-4a2e-b5f3-0c5b379dd8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=xy[:,0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebd9f3ec-51cc-44a9-9229-9aec224c6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data=xy[:,-1] # 一维 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10998a24-167a-4141-9d0e-22a14577a61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "944fe7c7-2f89-4f70-83bd-370470c2ad89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy[:, [-1]].shape # 二维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7a760b3-624b-4c63-9ec0-a4d696c4204f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759, 8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3d5551c-5772-478b-b352-e0d32870697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data=xy[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f62bc5f-2a7e-469c-9e54-254ade13718a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "077887bc-92cc-45f8-b3b0-e5ce9c8e9b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=torch.FloatTensor(x_data)\n",
    "y_train=torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44158551-5de9-4464-8bff-c803cda6cdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([759, 8])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44a98fb2-8291-4355-8fe1-8012468016a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.693147\n",
      "Epoch   10/100 Cost: 0.572727\n",
      "Epoch   20/100 Cost: 0.539493\n",
      "Epoch   30/100 Cost: 0.519708\n",
      "Epoch   40/100 Cost: 0.507066\n",
      "Epoch   50/100 Cost: 0.498539\n",
      "Epoch   60/100 Cost: 0.492549\n",
      "Epoch   70/100 Cost: 0.488209\n",
      "Epoch   80/100 Cost: 0.484985\n",
      "Epoch   90/100 Cost: 0.482543\n",
      "Epoch  100/100 Cost: 0.480661\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((8, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
    "    cost = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 10번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18482398-363e-41b4-b373-9167ad479e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.693147\n",
      "Epoch   10/100 Cost: 0.572727\n",
      "Epoch   20/100 Cost: 0.539493\n",
      "Epoch   30/100 Cost: 0.519708\n",
      "Epoch   40/100 Cost: 0.507066\n",
      "Epoch   50/100 Cost: 0.498539\n",
      "Epoch   60/100 Cost: 0.492549\n",
      "Epoch   70/100 Cost: 0.488209\n",
      "Epoch   80/100 Cost: 0.484985\n",
      "Epoch   90/100 Cost: 0.482543\n",
      "Epoch  100/100 Cost: 0.480661\n"
     ]
    }
   ],
   "source": [
    " \n",
    "W = torch.zeros((8, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    " \n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "   \n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    " \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7239ac71-c056-4976-9241-7e4d13c762a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4103],\n",
      "        [0.9242],\n",
      "        [0.2300],\n",
      "        [0.9411],\n",
      "        [0.1772]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_pred=torch.sigmoid(x_train.matmul(W)+b)\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9150d72-ebbe-46bd-92f5-fa6458e9f4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "pred=y_pred>=torch.FloatTensor([0.5])\n",
    "print(pred[:5].int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fddcaeb9-0809-41aa-b384-cf0cf6c76544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "afaaf6db-34ed-44a5-9609-03ef84810617",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=pred==y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "308ad09e-59e7-43e5-9666-fd08acb8d7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([759, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e07db7e-c49b-459f-8e8f-1186724891d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=correct.sum().item()/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e92570f6-5147-4325-ad1e-b452e71c26fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.766798418972332"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b5b24849-9120-4db7-b2b5-48a455ad8646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0039d3fa-c643-4192-91ad-64d99cb36a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759, 8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49377cc3-ea92-47cd-9173-e0056d96ec62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35b97ff8-bb95-4184-a877-b1e4252b123f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:    0/1000  loss:0.723400 acc:0.4177\n",
      "epochs:  100/1000  loss:0.479715 acc:0.7668\n",
      "epochs:  200/1000  loss:0.473650 acc:0.7721\n",
      "epochs:  300/1000  loss:0.472434 acc:0.7721\n",
      "epochs:  400/1000  loss:0.472014 acc:0.7708\n",
      "epochs:  500/1000  loss:0.471834 acc:0.7694\n",
      "epochs:  600/1000  loss:0.471749 acc:0.7681\n",
      "epochs:  700/1000  loss:0.471709 acc:0.7681\n",
      "epochs:  800/1000  loss:0.471689 acc:0.7694\n",
      "epochs:  900/1000  loss:0.471680 acc:0.7694\n",
      "epochs: 1000/1000  loss:0.471675 acc:0.7694\n"
     ]
    }
   ],
   "source": [
    "x_train=torch.FloatTensor(x_data)\n",
    "y_train=torch.FloatTensor(y_data)\n",
    "\n",
    "class Bin_classification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(8,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "    \n",
    "model=Bin_classification()\n",
    "optimizer=optim.SGD(model.parameters(),lr=1)\n",
    "epochs=1000\n",
    "for i in range(epochs+1):\n",
    "    y_pred=model(x_train)\n",
    "    loss=F.binary_cross_entropy(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i %100==0:\n",
    "        pred=y_pred>=torch.FloatTensor([0.5])\n",
    "        correct=pred.int()==y_train\n",
    "        acc=torch.sum(correct)/len(correct)\n",
    "   \n",
    "        print(\"epochs: {:4d}/{}  loss:{:.6f} acc:{:.4f}\".format(i,epochs,loss.item(),acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b56981-ef7d-43ef-9159-5382f8db10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5-27 8.30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79e09a-c268-4214-a756-b3ff6c669d0d",
   "metadata": {},
   "source": [
    "$$ P(class=i) = \\frac{e^i}{\\sum e^i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b20e506-8180-423c-92ef-97678f8d5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=torch.FloatTensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7ec9e247-6776-48cc-a8b3-66e06f2b7817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "43a477ef-5787-4124-843b-2f5b9d325f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_=F.softmax(z,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c7870d9d-2da4-481c-8885-a500c999fd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b75c3-0482-4519-a82e-07b9d279c7ae",
   "metadata": {},
   "source": [
    ".\n",
    "$$ L = \\frac{1}{N} \\sum - y \\log(\\hat{y}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "728377ce-db70-46d3-a093-dcba8e8a05fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=torch.rand(3,5,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6126f1bb-8810-4988-acdf-425927c054de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5228b44b-996c-4e16-b5b6-bd44d4281fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7442, 0.5285, 0.6642, 0.6099, 0.6818],\n",
       "        [0.7479, 0.0369, 0.7517, 0.1484, 0.1227],\n",
       "        [0.5304, 0.4148, 0.7937, 0.2104, 0.0555]], requires_grad=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62ac5037-3b3a-4645-8c8a-2de9fa85ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=F.softmax(z,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc75f46e-96a1-4af4-9934-5b31b9043464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "y=torch.randint(5,(3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "160a588b-285c-4b83-9f93-f44568014587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot=torch.zeros_like(y_pred)\n",
    "y_one_hot.scatter_(1,y.unsqueeze(1),1) #在维度1上按照y.unsqueeze()位置上非零值 填充 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3a361499-caaf-4ab7-985d-5f6477f026da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=(y_one_hot*-torch.log(y_pred)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7d97b155-5262-471d-8305-ec89f4c3cb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9011, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "15e82b61-af77-42d9-a164-22ceba46844e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5135, -1.7293, -1.5936, -1.6478, -1.5760],\n",
       "        [-1.2752, -1.9861, -1.2714, -1.8746, -1.9003],\n",
       "        [-1.5129, -1.6286, -1.2497, -1.8329, -1.9878]], grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(F.softmax(z,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "806a9ec4-0061-4063-ac4a-8530d2d0a363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5135, -1.7293, -1.5936, -1.6478, -1.5760],\n",
       "        [-1.2752, -1.9861, -1.2714, -1.8746, -1.9003],\n",
       "        [-1.5129, -1.6286, -1.2497, -1.8329, -1.9878]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4e38c318-6bcc-4ec7-9767-dc316506032f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9011, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_one_hot*-torch.log(F.softmax(z,dim=1))).sum(dim=1).mean() # loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9600061f-e68c-4a6a-94f2-37b1efbf0190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9011, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_one_hot*-F.log_softmax(z,dim=1)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0d0c3cf9-1bc4-4c70-acc4-5a8c5a5b0f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9011, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(F.log_softmax(z,dim=1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c9cd31c6-36c2-4473-b0f3-6f87a9ef6e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9011, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z, y)# = F.nll_loss(F.log_softmax(z,dim=1),y)  (y_one_hot*-torch.log(softmax(z,dim=1))).sum(dim=1).mean() F.nll_loss torch.log  F.log_softmax  F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3d5f9816-53fd-43bb-9f1c-7489aac31e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fb7089fd-e31c-4f61-8a1f-d6c2c5b0f750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d74b5903-64a7-4032-a787-2be7dc15a556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d21c4976-f999-487a-983a-2a0ea40781cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "W=torch.zeros((4,3),requires_grad=True)    # 分类问题 3分类\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "optimizer=optim.SGD([W,b],lr=0.1)\n",
    "nb_epochs=1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    y_pred=torch.softmax(x_train.matmul(W)+b,dim=1)\n",
    "    y_one_hot=torch.zeros_like(y_pred)\n",
    "    y_one_hot.scatter_(1,y_train.unsqueeze(1),1)\n",
    "    loss=(y_one_hot*-torch.log(y_pred)).sum(dim=1).mean()\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, loss.item()\n",
    "        ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7dd1d35d-1aed-4e69-8606-46a53ec148c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_170/1404615435.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "W=torch.zeros((4,3),requires_grad=True)    # 分类问题 3分类\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "optimizer=optim.SGD([W,b],lr=0.1)\n",
    "nb_epochs=1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    y_pred=torch.softmax(x_train.matmul(W)+b,dim=1)\n",
    "#     y_one_hot=torch.zeros_like(y_pred)\n",
    "#     y_one_hot.scatter_(1,y_train.unsqueeze(1),1)\n",
    "#     loss=(y_one_hot*-torch.log(y_pred)).sum(dim=1).mean()\n",
    "    F.cross_entropy(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, loss.item()\n",
    "        ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "939869a8-48f6-4406-830a-55221779586e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.761050\n",
      "Epoch  200/1000 Cost: 0.689991\n",
      "Epoch  300/1000 Cost: 0.643229\n",
      "Epoch  400/1000 Cost: 0.604117\n",
      "Epoch  500/1000 Cost: 0.568255\n",
      "Epoch  600/1000 Cost: 0.533922\n",
      "Epoch  700/1000 Cost: 0.500291\n",
      "Epoch  800/1000 Cost: 0.466908\n",
      "Epoch  900/1000 Cost: 0.433507\n",
      "Epoch 1000/1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "W=torch.zeros((4,3),requires_grad=True)    # 分类问题 多分类\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "optimizer=optim.SGD([W,b],lr=0.1)\n",
    "nb_epochs=1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    y_pred=F.log_softmax(x_train.matmul(W)+b,dim=1)\n",
    "    loss=F.cross_entropy(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, loss.item()\n",
    "        ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ff95d-c2f0-4159-892a-41e8aaaac61c",
   "metadata": {},
   "source": [
    "## Fancy Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1e5b9a87-3feb-4de2-b9bd-8f63667b8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy=np.loadtxt('data-04-zoo.csv',delimiter=',',dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cd324c24-a910-49fd-9508-fe5f77186ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 17)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ac5f9fe3-70b7-4e68-b064-8360bb360d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(xy[:, 0:-1])\n",
    "y_train = torch.LongTensor(xy[:, [-1]]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c1c046b8-109b-410c-b754-1bf48842175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = torch.LongTensor(xy[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9810e5d8-1e54-47f0-aee2-4e65da3b1de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1e3f1671-ccd6-45bd-8731-5532064fcbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e745b87c-2f2a-4d4e-8e15-5ab3a2f8d67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 3, 0, 0, 0, 0, 3, 3, 0, 0, 1, 3, 6, 6, 6, 1, 0, 3, 0, 1, 1, 0, 1,\n",
       "        5, 4, 4, 0, 0, 0, 5, 0, 0, 1, 3, 0, 0, 1, 3, 5, 5, 1, 5, 1, 0, 0, 6, 0,\n",
       "        0, 0, 0, 5, 4, 6, 0, 0, 1, 1, 1, 1, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        6, 3, 0, 0, 2, 6, 1, 1, 2, 6, 3, 1, 0, 6, 3, 1, 5, 4, 2, 2, 3, 0, 0, 1,\n",
       "        0, 5, 0, 6, 1])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "47f58ccb-6840-4749-a0cf-f80ef381eeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 3, 0, 0, 0, 0, 3, 3, 0, 0, 1, 3, 6, 6, 6, 1, 0, 3, 0, 1, 1, 0, 1,\n",
       "        5, 4, 4, 0, 0, 0, 5, 0, 0, 1, 3, 0, 0, 1, 3, 5, 5, 1, 5, 1, 0, 0, 6, 0,\n",
       "        0, 0, 0, 5, 4, 6, 0, 0, 1, 1, 1, 1, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        6, 3, 0, 0, 2, 6, 1, 1, 2, 6, 3, 1, 0, 6, 3, 1, 5, 4, 2, 2, 3, 0, 0, 1,\n",
       "        0, 5, 0, 6, 1])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ # 7分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "23b557d1-9b88-44f7-8e83-58393249d8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c1b02a18-f918-485e-8918-7fd96e388e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 16])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7a9db169-20d2-41da-9916-5d6d9f6aef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.945909\n",
      "Epoch  100/1000 Cost: 0.471836\n",
      "Epoch  200/1000 Cost: 0.326327\n",
      "Epoch  300/1000 Cost: 0.257839\n",
      "Epoch  400/1000 Cost: 0.215762\n",
      "Epoch  500/1000 Cost: 0.186603\n",
      "Epoch  600/1000 Cost: 0.164898\n",
      "Epoch  700/1000 Cost: 0.147955\n",
      "Epoch  800/1000 Cost: 0.134278\n",
      "Epoch  900/1000 Cost: 0.122962\n",
      "Epoch 1000/1000 Cost: 0.113422\n"
     ]
    }
   ],
   "source": [
    "W=torch.zeros((16,7),requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "optimizer=optim.SGD([W,b],lr=0.1)\n",
    "epochs=1000\n",
    "for i in range(epochs+1):\n",
    "#     y_=F.softmax(x_train.matmul(W)+b,dim=1)\n",
    "#     y_pred=F.log_softmax(x_train.matmul(W)+b,dim=1)\n",
    "    loss=F.cross_entropy(x_train.matmul(W)+b,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            i, nb_epochs, loss.item()\n",
    "        ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8b3669fb-5bd7-4039-92b1-1bb47f054bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1, 2, 1],\n",
    "                             [1, 3, 2],\n",
    "                             [1, 3, 4],\n",
    "                             [1, 5, 5],\n",
    "                             [1, 7, 5],\n",
    "                             [1, 2, 5],\n",
    "                             [1, 6, 6],\n",
    "                             [1, 7, 7]\n",
    "                            ])\n",
    "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9a929ea3-a558-4a24-9295-02fd7278963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.FloatTensor([[2, 1, 1], [3, 1, 2], [3, 3, 4]])\n",
    "y_test = torch.LongTensor([2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bb174558-1645-4460-8c73-e3baa72a405f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "35c1b9d9-d628-4556-9e70-c7a0dcfca076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "441c7b33-2b5c-4254-9b06-22c08d93ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 3)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "750c0632-df2c-4c4f-97ef-5f9d27dbb538",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7736dace-7b5b-4bcc-bcf9-41c9527f4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0d70d5a2-e354-4d3b-9cfe-b8c15850aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,x_train,y_train):\n",
    "    nb_epochs=20\n",
    "    for epoch in range(nb_epochs):\n",
    "        y_pred=model(x_train)\n",
    "        loss=F.cross_entropy(y_pred,y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, loss.item()\n",
    "        ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "06327066-4cac-4cc5-8570-2eb234ea4f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: torch.return_types.max(\n",
      "values=tensor([0.5000, 0.8000]),\n",
      "indices=tensor([1, 0]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "prediction = torch.tensor([[0.2, 0.5, 0.3], [0.8, 0.1, 0.1]])\n",
    "\n",
    "# 提取预测结果的最大值索引\n",
    "predicted_labels = prediction.max(1)\n",
    "\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "be4b7ef1-dd13-493e-8fec-82a5c7b0a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, optimizer, x_test, y_test):\n",
    "    y_pred=model(x_test)\n",
    "    y_pr_class=y_pred.max(1)[1] #沿着第 1 维（即类别维度）取最大值 [1]要索引数组\n",
    "    correct_count=(y_pr_class==y_test).sum().item()\n",
    "    print(\"acc:{:.4f}\".format(correct_count/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bbb25b17-0904-4d62-a3af-e2d88050f436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 1.564796\n",
      "Epoch    1/20 Cost: 1.128972\n",
      "Epoch    2/20 Cost: 1.076972\n",
      "Epoch    3/20 Cost: 1.065789\n",
      "Epoch    4/20 Cost: 1.055707\n",
      "Epoch    5/20 Cost: 1.046614\n",
      "Epoch    6/20 Cost: 1.038079\n",
      "Epoch    7/20 Cost: 1.029999\n",
      "Epoch    8/20 Cost: 1.022254\n",
      "Epoch    9/20 Cost: 1.014796\n",
      "Epoch   10/20 Cost: 1.007578\n",
      "Epoch   11/20 Cost: 1.000576\n",
      "Epoch   12/20 Cost: 0.993768\n",
      "Epoch   13/20 Cost: 0.987139\n",
      "Epoch   14/20 Cost: 0.980677\n",
      "Epoch   15/20 Cost: 0.974372\n",
      "Epoch   16/20 Cost: 0.968217\n",
      "Epoch   17/20 Cost: 0.962203\n",
      "Epoch   18/20 Cost: 0.956326\n",
      "Epoch   19/20 Cost: 0.950578\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c33e08d5-34ba-4ea5-af64-f97bfab06341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.6667\n"
     ]
    }
   ],
   "source": [
    "test(model, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a9639-ae48-4e1c-b308-ceef45e74d9c",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "513edf06-8083-4b0f-a980-f5dcb9640785",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a84cc2da-2ede-4d9c-9afb-46f057934340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "34489659-6383-4f3b-b1e2-693963494ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d82293cb-ed33-45d7-9a4b-4267a808b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu=x_train.mean(dim=0) # 不同数据的同位置 一个batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cf328c33-25e6-442d-b773-4f388b0f8ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([84.8000, 84.6000, 85.6000])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e373b247-a6f0-40d4-8f48-867bd72bff51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.0"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(73+80+75)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "88fbefe9-0979-4fc5-bec1-b9cd58878fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=x_train.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "84252362-4f77-469c-861a-70e53175491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x_train=(x_train-mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "fa2351ea-e0a1-4236-bded-41ed6d25a103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3324d823-660a-4a81-aa76-8f218f000ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5ac404a8-eb60-4ea9-923a-f497a33dd1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b802ca6f-294d-4b27-b726-04571108b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5527069f-9900-42d7-95ff-023071c32fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        # cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ebc68ea2-b1e7-441a-9400-35a7b852ab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Cost: 29823.191406\n",
      "Epoch    1/20 Cost: 18943.023438\n",
      "Epoch    2/20 Cost: 12081.884766\n",
      "Epoch    3/20 Cost: 7720.489258\n",
      "Epoch    4/20 Cost: 4937.842773\n",
      "Epoch    5/20 Cost: 3159.458496\n",
      "Epoch    6/20 Cost: 2022.017578\n",
      "Epoch    7/20 Cost: 1294.260376\n",
      "Epoch    8/20 Cost: 828.549683\n",
      "Epoch    9/20 Cost: 530.505005\n",
      "Epoch   10/20 Cost: 339.754089\n",
      "Epoch   11/20 Cost: 217.667816\n",
      "Epoch   12/20 Cost: 139.525848\n",
      "Epoch   13/20 Cost: 89.508568\n",
      "Epoch   14/20 Cost: 57.490990\n",
      "Epoch   15/20 Cost: 36.993412\n",
      "Epoch   16/20 Cost: 23.868979\n",
      "Epoch   17/20 Cost: 15.463486\n",
      "Epoch   18/20 Cost: 10.078440\n",
      "Epoch   19/20 Cost: 6.626630\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, norm_x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28696cd-1812-42ba-8a2f-cac326659503",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c437ba22-1675-4d44-8ef0-e71cd386bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_regularization(model,optimizer,x_train,y_train):\n",
    "    nb_epochs=20\n",
    "    for epoch in range(nb_epochs):\n",
    "        y_pred=model(x_train)\n",
    "        loss=F.mse_loss(y_pred,y_train)\n",
    "        l2_reg=0\n",
    "        for param in model.parameters():\n",
    "            l2_reg+=torch.norm(param)\n",
    "        loss+=l2_reg\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch+1, nb_epochs, loss.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3b586e07-c5ba-45cd-93c8-390bc219815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/20 Cost: 29586.859375\n",
      "Epoch    2/20 Cost: 18871.623047\n",
      "Epoch    3/20 Cost: 12106.733398\n",
      "Epoch    4/20 Cost: 7803.884277\n",
      "Epoch    5/20 Cost: 5057.823242\n",
      "Epoch    6/20 Cost: 3302.598877\n",
      "Epoch    7/20 Cost: 2179.907715\n",
      "Epoch    8/20 Cost: 1461.571899\n",
      "Epoch    9/20 Cost: 1001.887329\n",
      "Epoch   10/20 Cost: 707.700928\n",
      "Epoch   11/20 Cost: 519.421997\n",
      "Epoch   12/20 Cost: 398.920410\n",
      "Epoch   13/20 Cost: 321.795837\n",
      "Epoch   14/20 Cost: 272.432343\n",
      "Epoch   15/20 Cost: 240.836166\n",
      "Epoch   16/20 Cost: 220.611053\n",
      "Epoch   17/20 Cost: 207.663879\n",
      "Epoch   18/20 Cost: 199.374588\n",
      "Epoch   19/20 Cost: 194.066467\n",
      "Epoch   20/20 Cost: 190.666565\n"
     ]
    }
   ],
   "source": [
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "train_with_regularization(model, optimizer, norm_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "094ebefb-10e9-4a4d-975a-7ec6a6c1e74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "eea8465d-ef1b-43f0-9b84-7c42dcee3ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=y_train.numpy() \n",
    "np.unique(a) \n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1817d746-1988-4a58-a960-3106797bc48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 29507.916016\n",
      "Epoch    1/1000 Cost: 18820.560547\n",
      "Epoch    2/1000 Cost: 12073.867188\n",
      "Epoch    3/1000 Cost: 7782.849121\n",
      "Epoch    4/1000 Cost: 5044.410156\n",
      "Epoch    5/1000 Cost: 3294.075439\n",
      "Epoch    6/1000 Cost: 2174.515137\n",
      "Epoch    7/1000 Cost: 1458.181396\n",
      "Epoch    8/1000 Cost: 999.775146\n",
      "Epoch    9/1000 Cost: 706.403503\n",
      "Epoch   10/1000 Cost: 518.643250\n",
      "Epoch   11/1000 Cost: 398.471008\n",
      "Epoch   12/1000 Cost: 321.554749\n",
      "Epoch   13/1000 Cost: 272.322083\n",
      "Epoch   14/1000 Cost: 240.807251\n",
      "Epoch   15/1000 Cost: 220.632233\n",
      "Epoch   16/1000 Cost: 207.714935\n",
      "Epoch   17/1000 Cost: 199.442886\n",
      "Epoch   18/1000 Cost: 194.143890\n",
      "Epoch   19/1000 Cost: 190.748138\n",
      "Epoch   20/1000 Cost: 188.570511\n",
      "Epoch   21/1000 Cost: 187.172791\n",
      "Epoch   22/1000 Cost: 186.274384\n",
      "Epoch   23/1000 Cost: 185.695755\n",
      "Epoch   24/1000 Cost: 185.322006\n",
      "Epoch   25/1000 Cost: 185.079529\n",
      "Epoch   26/1000 Cost: 184.921219\n",
      "Epoch   27/1000 Cost: 184.816986\n",
      "Epoch   28/1000 Cost: 184.747498\n",
      "Epoch   29/1000 Cost: 184.700378\n",
      "Epoch   30/1000 Cost: 184.667725\n",
      "Epoch   31/1000 Cost: 184.644470\n",
      "Epoch   32/1000 Cost: 184.627335\n",
      "Epoch   33/1000 Cost: 184.614212\n",
      "Epoch   34/1000 Cost: 184.603821\n",
      "Epoch   35/1000 Cost: 184.595261\n",
      "Epoch   36/1000 Cost: 184.587967\n",
      "Epoch   37/1000 Cost: 184.581604\n",
      "Epoch   38/1000 Cost: 184.575897\n",
      "Epoch   39/1000 Cost: 184.570709\n",
      "Epoch   40/1000 Cost: 184.565918\n",
      "Epoch   41/1000 Cost: 184.561478\n",
      "Epoch   42/1000 Cost: 184.557327\n",
      "Epoch   43/1000 Cost: 184.553436\n",
      "Epoch   44/1000 Cost: 184.549744\n",
      "Epoch   45/1000 Cost: 184.546295\n",
      "Epoch   46/1000 Cost: 184.543015\n",
      "Epoch   47/1000 Cost: 184.539917\n",
      "Epoch   48/1000 Cost: 184.536987\n",
      "Epoch   49/1000 Cost: 184.534210\n",
      "Epoch   50/1000 Cost: 184.531586\n",
      "Epoch   51/1000 Cost: 184.529099\n",
      "Epoch   52/1000 Cost: 184.526749\n",
      "Epoch   53/1000 Cost: 184.524506\n",
      "Epoch   54/1000 Cost: 184.522385\n",
      "Epoch   55/1000 Cost: 184.520386\n",
      "Epoch   56/1000 Cost: 184.518478\n",
      "Epoch   57/1000 Cost: 184.516663\n",
      "Epoch   58/1000 Cost: 184.514969\n",
      "Epoch   59/1000 Cost: 184.513351\n",
      "Epoch   60/1000 Cost: 184.511826\n",
      "Epoch   61/1000 Cost: 184.510361\n",
      "Epoch   62/1000 Cost: 184.508987\n",
      "Epoch   63/1000 Cost: 184.507675\n",
      "Epoch   64/1000 Cost: 184.506439\n",
      "Epoch   65/1000 Cost: 184.505264\n",
      "Epoch   66/1000 Cost: 184.504150\n",
      "Epoch   67/1000 Cost: 184.503113\n",
      "Epoch   68/1000 Cost: 184.502106\n",
      "Epoch   69/1000 Cost: 184.501160\n",
      "Epoch   70/1000 Cost: 184.500275\n",
      "Epoch   71/1000 Cost: 184.499420\n",
      "Epoch   72/1000 Cost: 184.498596\n",
      "Epoch   73/1000 Cost: 184.497849\n",
      "Epoch   74/1000 Cost: 184.497116\n",
      "Epoch   75/1000 Cost: 184.496445\n",
      "Epoch   76/1000 Cost: 184.495789\n",
      "Epoch   77/1000 Cost: 184.495178\n",
      "Epoch   78/1000 Cost: 184.494568\n",
      "Epoch   79/1000 Cost: 184.494034\n",
      "Epoch   80/1000 Cost: 184.493500\n",
      "Epoch   81/1000 Cost: 184.493011\n",
      "Epoch   82/1000 Cost: 184.492523\n",
      "Epoch   83/1000 Cost: 184.492081\n",
      "Epoch   84/1000 Cost: 184.491669\n",
      "Epoch   85/1000 Cost: 184.491241\n",
      "Epoch   86/1000 Cost: 184.490891\n",
      "Epoch   87/1000 Cost: 184.490524\n",
      "Epoch   88/1000 Cost: 184.490189\n",
      "Epoch   89/1000 Cost: 184.489853\n",
      "Epoch   90/1000 Cost: 184.489563\n",
      "Epoch   91/1000 Cost: 184.489258\n",
      "Epoch   92/1000 Cost: 184.488983\n",
      "Epoch   93/1000 Cost: 184.488724\n",
      "Epoch   94/1000 Cost: 184.488464\n",
      "Epoch   95/1000 Cost: 184.488235\n",
      "Epoch   96/1000 Cost: 184.488007\n",
      "Epoch   97/1000 Cost: 184.487793\n",
      "Epoch   98/1000 Cost: 184.487595\n",
      "Epoch   99/1000 Cost: 184.487411\n",
      "Epoch  100/1000 Cost: 184.487228\n",
      "Epoch  101/1000 Cost: 184.487061\n",
      "Epoch  102/1000 Cost: 184.486908\n",
      "Epoch  103/1000 Cost: 184.486740\n",
      "Epoch  104/1000 Cost: 184.486588\n",
      "Epoch  105/1000 Cost: 184.486450\n",
      "Epoch  106/1000 Cost: 184.486328\n",
      "Epoch  107/1000 Cost: 184.486206\n",
      "Epoch  108/1000 Cost: 184.486099\n",
      "Epoch  109/1000 Cost: 184.485977\n",
      "Epoch  110/1000 Cost: 184.485870\n",
      "Epoch  111/1000 Cost: 184.485779\n",
      "Epoch  112/1000 Cost: 184.485672\n",
      "Epoch  113/1000 Cost: 184.485580\n",
      "Epoch  114/1000 Cost: 184.485504\n",
      "Epoch  115/1000 Cost: 184.485413\n",
      "Epoch  116/1000 Cost: 184.485336\n",
      "Epoch  117/1000 Cost: 184.485275\n",
      "Epoch  118/1000 Cost: 184.485199\n",
      "Epoch  119/1000 Cost: 184.485138\n",
      "Epoch  120/1000 Cost: 184.485077\n",
      "Epoch  121/1000 Cost: 184.485016\n",
      "Epoch  122/1000 Cost: 184.484955\n",
      "Epoch  123/1000 Cost: 184.484909\n",
      "Epoch  124/1000 Cost: 184.484863\n",
      "Epoch  125/1000 Cost: 184.484818\n",
      "Epoch  126/1000 Cost: 184.484756\n",
      "Epoch  127/1000 Cost: 184.484726\n",
      "Epoch  128/1000 Cost: 184.484680\n",
      "Epoch  129/1000 Cost: 184.484650\n",
      "Epoch  130/1000 Cost: 184.484604\n",
      "Epoch  131/1000 Cost: 184.484573\n",
      "Epoch  132/1000 Cost: 184.484543\n",
      "Epoch  133/1000 Cost: 184.484512\n",
      "Epoch  134/1000 Cost: 184.484482\n",
      "Epoch  135/1000 Cost: 184.484451\n",
      "Epoch  136/1000 Cost: 184.484421\n",
      "Epoch  137/1000 Cost: 184.484390\n",
      "Epoch  138/1000 Cost: 184.484375\n",
      "Epoch  139/1000 Cost: 184.484360\n",
      "Epoch  140/1000 Cost: 184.484344\n",
      "Epoch  141/1000 Cost: 184.484299\n",
      "Epoch  142/1000 Cost: 184.484283\n",
      "Epoch  143/1000 Cost: 184.484268\n",
      "Epoch  144/1000 Cost: 184.484253\n",
      "Epoch  145/1000 Cost: 184.484238\n",
      "Epoch  146/1000 Cost: 184.484222\n",
      "Epoch  147/1000 Cost: 184.484207\n",
      "Epoch  148/1000 Cost: 184.484192\n",
      "Epoch  149/1000 Cost: 184.484192\n",
      "Epoch  150/1000 Cost: 184.484161\n",
      "Epoch  151/1000 Cost: 184.484146\n",
      "Epoch  152/1000 Cost: 184.484146\n",
      "Epoch  153/1000 Cost: 184.484131\n",
      "Epoch  154/1000 Cost: 184.484131\n",
      "Epoch  155/1000 Cost: 184.484116\n",
      "Epoch  156/1000 Cost: 184.484100\n",
      "Epoch  157/1000 Cost: 184.484100\n",
      "Epoch  158/1000 Cost: 184.484085\n",
      "Epoch  159/1000 Cost: 184.484085\n",
      "Epoch  160/1000 Cost: 184.484070\n",
      "Epoch  161/1000 Cost: 184.484070\n",
      "Epoch  162/1000 Cost: 184.484055\n",
      "Epoch  163/1000 Cost: 184.484055\n",
      "Epoch  164/1000 Cost: 184.484039\n",
      "Epoch  165/1000 Cost: 184.484039\n",
      "Epoch  166/1000 Cost: 184.484055\n",
      "Epoch  167/1000 Cost: 184.484039\n",
      "Epoch  168/1000 Cost: 184.484024\n",
      "Epoch  169/1000 Cost: 184.484024\n",
      "Epoch  170/1000 Cost: 184.484009\n",
      "Epoch  171/1000 Cost: 184.484009\n",
      "Epoch  172/1000 Cost: 184.484009\n",
      "Epoch  173/1000 Cost: 184.483994\n",
      "Epoch  174/1000 Cost: 184.483994\n",
      "Epoch  175/1000 Cost: 184.483994\n",
      "Epoch  176/1000 Cost: 184.483994\n",
      "Epoch  177/1000 Cost: 184.483994\n",
      "Epoch  178/1000 Cost: 184.483994\n",
      "Epoch  179/1000 Cost: 184.483978\n",
      "Epoch  180/1000 Cost: 184.483978\n",
      "Epoch  181/1000 Cost: 184.483978\n",
      "Epoch  182/1000 Cost: 184.483978\n",
      "Epoch  183/1000 Cost: 184.483978\n",
      "Epoch  184/1000 Cost: 184.483963\n",
      "Epoch  185/1000 Cost: 184.483978\n",
      "Epoch  186/1000 Cost: 184.483963\n",
      "Epoch  187/1000 Cost: 184.483978\n",
      "Epoch  188/1000 Cost: 184.483978\n",
      "Epoch  189/1000 Cost: 184.483963\n",
      "Epoch  190/1000 Cost: 184.483963\n",
      "Epoch  191/1000 Cost: 184.483963\n",
      "Epoch  192/1000 Cost: 184.483948\n",
      "Epoch  193/1000 Cost: 184.483963\n",
      "Epoch  194/1000 Cost: 184.483948\n",
      "Epoch  195/1000 Cost: 184.483963\n",
      "Epoch  196/1000 Cost: 184.483963\n",
      "Epoch  197/1000 Cost: 184.483948\n",
      "Epoch  198/1000 Cost: 184.483963\n",
      "Epoch  199/1000 Cost: 184.483948\n",
      "Epoch  200/1000 Cost: 184.483963\n",
      "Epoch  201/1000 Cost: 184.483948\n",
      "Epoch  202/1000 Cost: 184.483948\n",
      "Epoch  203/1000 Cost: 184.483948\n",
      "Epoch  204/1000 Cost: 184.483948\n",
      "Epoch  205/1000 Cost: 184.483932\n",
      "Epoch  206/1000 Cost: 184.483948\n",
      "Epoch  207/1000 Cost: 184.483948\n",
      "Epoch  208/1000 Cost: 184.483948\n",
      "Epoch  209/1000 Cost: 184.483932\n",
      "Epoch  210/1000 Cost: 184.483948\n",
      "Epoch  211/1000 Cost: 184.483948\n",
      "Epoch  212/1000 Cost: 184.483948\n",
      "Epoch  213/1000 Cost: 184.483932\n",
      "Epoch  214/1000 Cost: 184.483932\n",
      "Epoch  215/1000 Cost: 184.483932\n",
      "Epoch  216/1000 Cost: 184.483932\n",
      "Epoch  217/1000 Cost: 184.483932\n",
      "Epoch  218/1000 Cost: 184.483948\n",
      "Epoch  219/1000 Cost: 184.483948\n",
      "Epoch  220/1000 Cost: 184.483932\n",
      "Epoch  221/1000 Cost: 184.483932\n",
      "Epoch  222/1000 Cost: 184.483932\n",
      "Epoch  223/1000 Cost: 184.483932\n",
      "Epoch  224/1000 Cost: 184.483948\n",
      "Epoch  225/1000 Cost: 184.483932\n",
      "Epoch  226/1000 Cost: 184.483932\n",
      "Epoch  227/1000 Cost: 184.483932\n",
      "Epoch  228/1000 Cost: 184.483948\n",
      "Epoch  229/1000 Cost: 184.483932\n",
      "Epoch  230/1000 Cost: 184.483932\n",
      "Epoch  231/1000 Cost: 184.483932\n",
      "Epoch  232/1000 Cost: 184.483932\n",
      "Epoch  233/1000 Cost: 184.483932\n",
      "Epoch  234/1000 Cost: 184.483932\n",
      "Epoch  235/1000 Cost: 184.483932\n",
      "Epoch  236/1000 Cost: 184.483932\n",
      "Epoch  237/1000 Cost: 184.483932\n",
      "Epoch  238/1000 Cost: 184.483932\n",
      "Epoch  239/1000 Cost: 184.483932\n",
      "Epoch  240/1000 Cost: 184.483932\n",
      "Epoch  241/1000 Cost: 184.483932\n",
      "Epoch  242/1000 Cost: 184.483932\n",
      "Epoch  243/1000 Cost: 184.483932\n",
      "Epoch  244/1000 Cost: 184.483932\n",
      "Epoch  245/1000 Cost: 184.483932\n",
      "Epoch  246/1000 Cost: 184.483917\n",
      "Epoch  247/1000 Cost: 184.483932\n",
      "Epoch  248/1000 Cost: 184.483932\n",
      "Epoch  249/1000 Cost: 184.483932\n",
      "Epoch  250/1000 Cost: 184.483917\n",
      "Epoch  251/1000 Cost: 184.483932\n",
      "Epoch  252/1000 Cost: 184.483932\n",
      "Epoch  253/1000 Cost: 184.483932\n",
      "Epoch  254/1000 Cost: 184.483917\n",
      "Epoch  255/1000 Cost: 184.483917\n",
      "Epoch  256/1000 Cost: 184.483932\n",
      "Epoch  257/1000 Cost: 184.483932\n",
      "Epoch  258/1000 Cost: 184.483932\n",
      "Epoch  259/1000 Cost: 184.483917\n",
      "Epoch  260/1000 Cost: 184.483917\n",
      "Epoch  261/1000 Cost: 184.483932\n",
      "Epoch  262/1000 Cost: 184.483917\n",
      "Epoch  263/1000 Cost: 184.483932\n",
      "Epoch  264/1000 Cost: 184.483917\n",
      "Epoch  265/1000 Cost: 184.483932\n",
      "Epoch  266/1000 Cost: 184.483917\n",
      "Epoch  267/1000 Cost: 184.483932\n",
      "Epoch  268/1000 Cost: 184.483917\n",
      "Epoch  269/1000 Cost: 184.483932\n",
      "Epoch  270/1000 Cost: 184.483932\n",
      "Epoch  271/1000 Cost: 184.483932\n",
      "Epoch  272/1000 Cost: 184.483932\n",
      "Epoch  273/1000 Cost: 184.483917\n",
      "Epoch  274/1000 Cost: 184.483932\n",
      "Epoch  275/1000 Cost: 184.483917\n",
      "Epoch  276/1000 Cost: 184.483932\n",
      "Epoch  277/1000 Cost: 184.483932\n",
      "Epoch  278/1000 Cost: 184.483917\n",
      "Epoch  279/1000 Cost: 184.483932\n",
      "Epoch  280/1000 Cost: 184.483932\n",
      "Epoch  281/1000 Cost: 184.483917\n",
      "Epoch  282/1000 Cost: 184.483932\n",
      "Epoch  283/1000 Cost: 184.483932\n",
      "Epoch  284/1000 Cost: 184.483917\n",
      "Epoch  285/1000 Cost: 184.483932\n",
      "Epoch  286/1000 Cost: 184.483917\n",
      "Epoch  287/1000 Cost: 184.483917\n",
      "Epoch  288/1000 Cost: 184.483917\n",
      "Epoch  289/1000 Cost: 184.483932\n",
      "Epoch  290/1000 Cost: 184.483932\n",
      "Epoch  291/1000 Cost: 184.483917\n",
      "Epoch  292/1000 Cost: 184.483917\n",
      "Epoch  293/1000 Cost: 184.483932\n",
      "Epoch  294/1000 Cost: 184.483932\n",
      "Epoch  295/1000 Cost: 184.483917\n",
      "Epoch  296/1000 Cost: 184.483917\n",
      "Epoch  297/1000 Cost: 184.483917\n",
      "Epoch  298/1000 Cost: 184.483932\n",
      "Epoch  299/1000 Cost: 184.483932\n",
      "Epoch  300/1000 Cost: 184.483932\n",
      "Epoch  301/1000 Cost: 184.483917\n",
      "Epoch  302/1000 Cost: 184.483917\n",
      "Epoch  303/1000 Cost: 184.483932\n",
      "Epoch  304/1000 Cost: 184.483932\n",
      "Epoch  305/1000 Cost: 184.483917\n",
      "Epoch  306/1000 Cost: 184.483917\n",
      "Epoch  307/1000 Cost: 184.483917\n",
      "Epoch  308/1000 Cost: 184.483917\n",
      "Epoch  309/1000 Cost: 184.483917\n",
      "Epoch  310/1000 Cost: 184.483932\n",
      "Epoch  311/1000 Cost: 184.483932\n",
      "Epoch  312/1000 Cost: 184.483932\n",
      "Epoch  313/1000 Cost: 184.483917\n",
      "Epoch  314/1000 Cost: 184.483917\n",
      "Epoch  315/1000 Cost: 184.483917\n",
      "Epoch  316/1000 Cost: 184.483917\n",
      "Epoch  317/1000 Cost: 184.483932\n",
      "Epoch  318/1000 Cost: 184.483932\n",
      "Epoch  319/1000 Cost: 184.483932\n",
      "Epoch  320/1000 Cost: 184.483932\n",
      "Epoch  321/1000 Cost: 184.483932\n",
      "Epoch  322/1000 Cost: 184.483932\n",
      "Epoch  323/1000 Cost: 184.483917\n",
      "Epoch  324/1000 Cost: 184.483917\n",
      "Epoch  325/1000 Cost: 184.483917\n",
      "Epoch  326/1000 Cost: 184.483932\n",
      "Epoch  327/1000 Cost: 184.483932\n",
      "Epoch  328/1000 Cost: 184.483932\n",
      "Epoch  329/1000 Cost: 184.483932\n",
      "Epoch  330/1000 Cost: 184.483932\n",
      "Epoch  331/1000 Cost: 184.483917\n",
      "Epoch  332/1000 Cost: 184.483917\n",
      "Epoch  333/1000 Cost: 184.483917\n",
      "Epoch  334/1000 Cost: 184.483932\n",
      "Epoch  335/1000 Cost: 184.483932\n",
      "Epoch  336/1000 Cost: 184.483917\n",
      "Epoch  337/1000 Cost: 184.483917\n",
      "Epoch  338/1000 Cost: 184.483917\n",
      "Epoch  339/1000 Cost: 184.483917\n",
      "Epoch  340/1000 Cost: 184.483932\n",
      "Epoch  341/1000 Cost: 184.483932\n",
      "Epoch  342/1000 Cost: 184.483932\n",
      "Epoch  343/1000 Cost: 184.483932\n",
      "Epoch  344/1000 Cost: 184.483932\n",
      "Epoch  345/1000 Cost: 184.483932\n",
      "Epoch  346/1000 Cost: 184.483932\n",
      "Epoch  347/1000 Cost: 184.483932\n",
      "Epoch  348/1000 Cost: 184.483917\n",
      "Epoch  349/1000 Cost: 184.483917\n",
      "Epoch  350/1000 Cost: 184.483932\n",
      "Epoch  351/1000 Cost: 184.483932\n",
      "Epoch  352/1000 Cost: 184.483932\n",
      "Epoch  353/1000 Cost: 184.483917\n",
      "Epoch  354/1000 Cost: 184.483917\n",
      "Epoch  355/1000 Cost: 184.483917\n",
      "Epoch  356/1000 Cost: 184.483917\n",
      "Epoch  357/1000 Cost: 184.483917\n",
      "Epoch  358/1000 Cost: 184.483917\n",
      "Epoch  359/1000 Cost: 184.483917\n",
      "Epoch  360/1000 Cost: 184.483917\n",
      "Epoch  361/1000 Cost: 184.483917\n",
      "Epoch  362/1000 Cost: 184.483917\n",
      "Epoch  363/1000 Cost: 184.483917\n",
      "Epoch  364/1000 Cost: 184.483917\n",
      "Epoch  365/1000 Cost: 184.483917\n",
      "Epoch  366/1000 Cost: 184.483932\n",
      "Epoch  367/1000 Cost: 184.483932\n",
      "Epoch  368/1000 Cost: 184.483932\n",
      "Epoch  369/1000 Cost: 184.483932\n",
      "Epoch  370/1000 Cost: 184.483932\n",
      "Epoch  371/1000 Cost: 184.483917\n",
      "Epoch  372/1000 Cost: 184.483932\n",
      "Epoch  373/1000 Cost: 184.483932\n",
      "Epoch  374/1000 Cost: 184.483932\n",
      "Epoch  375/1000 Cost: 184.483932\n",
      "Epoch  376/1000 Cost: 184.483932\n",
      "Epoch  377/1000 Cost: 184.483932\n",
      "Epoch  378/1000 Cost: 184.483932\n",
      "Epoch  379/1000 Cost: 184.483932\n",
      "Epoch  380/1000 Cost: 184.483932\n",
      "Epoch  381/1000 Cost: 184.483932\n",
      "Epoch  382/1000 Cost: 184.483932\n",
      "Epoch  383/1000 Cost: 184.483932\n",
      "Epoch  384/1000 Cost: 184.483932\n",
      "Epoch  385/1000 Cost: 184.483932\n",
      "Epoch  386/1000 Cost: 184.483932\n",
      "Epoch  387/1000 Cost: 184.483932\n",
      "Epoch  388/1000 Cost: 184.483917\n",
      "Epoch  389/1000 Cost: 184.483932\n",
      "Epoch  390/1000 Cost: 184.483917\n",
      "Epoch  391/1000 Cost: 184.483917\n",
      "Epoch  392/1000 Cost: 184.483932\n",
      "Epoch  393/1000 Cost: 184.483917\n",
      "Epoch  394/1000 Cost: 184.483932\n",
      "Epoch  395/1000 Cost: 184.483932\n",
      "Epoch  396/1000 Cost: 184.483932\n",
      "Epoch  397/1000 Cost: 184.483932\n",
      "Epoch  398/1000 Cost: 184.483932\n",
      "Epoch  399/1000 Cost: 184.483932\n",
      "Epoch  400/1000 Cost: 184.483932\n",
      "Epoch  401/1000 Cost: 184.483932\n",
      "Epoch  402/1000 Cost: 184.483932\n",
      "Epoch  403/1000 Cost: 184.483932\n",
      "Epoch  404/1000 Cost: 184.483932\n",
      "Epoch  405/1000 Cost: 184.483917\n",
      "Epoch  406/1000 Cost: 184.483917\n",
      "Epoch  407/1000 Cost: 184.483917\n",
      "Epoch  408/1000 Cost: 184.483917\n",
      "Epoch  409/1000 Cost: 184.483917\n",
      "Epoch  410/1000 Cost: 184.483917\n",
      "Epoch  411/1000 Cost: 184.483917\n",
      "Epoch  412/1000 Cost: 184.483917\n",
      "Epoch  413/1000 Cost: 184.483932\n",
      "Epoch  414/1000 Cost: 184.483932\n",
      "Epoch  415/1000 Cost: 184.483917\n",
      "Epoch  416/1000 Cost: 184.483932\n",
      "Epoch  417/1000 Cost: 184.483932\n",
      "Epoch  418/1000 Cost: 184.483932\n",
      "Epoch  419/1000 Cost: 184.483932\n",
      "Epoch  420/1000 Cost: 184.483932\n",
      "Epoch  421/1000 Cost: 184.483932\n",
      "Epoch  422/1000 Cost: 184.483932\n",
      "Epoch  423/1000 Cost: 184.483932\n",
      "Epoch  424/1000 Cost: 184.483932\n",
      "Epoch  425/1000 Cost: 184.483932\n",
      "Epoch  426/1000 Cost: 184.483932\n",
      "Epoch  427/1000 Cost: 184.483932\n",
      "Epoch  428/1000 Cost: 184.483932\n",
      "Epoch  429/1000 Cost: 184.483932\n",
      "Epoch  430/1000 Cost: 184.483932\n",
      "Epoch  431/1000 Cost: 184.483932\n",
      "Epoch  432/1000 Cost: 184.483932\n",
      "Epoch  433/1000 Cost: 184.483932\n",
      "Epoch  434/1000 Cost: 184.483932\n",
      "Epoch  435/1000 Cost: 184.483932\n",
      "Epoch  436/1000 Cost: 184.483932\n",
      "Epoch  437/1000 Cost: 184.483917\n",
      "Epoch  438/1000 Cost: 184.483917\n",
      "Epoch  439/1000 Cost: 184.483917\n",
      "Epoch  440/1000 Cost: 184.483917\n",
      "Epoch  441/1000 Cost: 184.483917\n",
      "Epoch  442/1000 Cost: 184.483917\n",
      "Epoch  443/1000 Cost: 184.483917\n",
      "Epoch  444/1000 Cost: 184.483917\n",
      "Epoch  445/1000 Cost: 184.483917\n",
      "Epoch  446/1000 Cost: 184.483917\n",
      "Epoch  447/1000 Cost: 184.483917\n",
      "Epoch  448/1000 Cost: 184.483917\n",
      "Epoch  449/1000 Cost: 184.483917\n",
      "Epoch  450/1000 Cost: 184.483917\n",
      "Epoch  451/1000 Cost: 184.483932\n",
      "Epoch  452/1000 Cost: 184.483932\n",
      "Epoch  453/1000 Cost: 184.483932\n",
      "Epoch  454/1000 Cost: 184.483917\n",
      "Epoch  455/1000 Cost: 184.483932\n",
      "Epoch  456/1000 Cost: 184.483932\n",
      "Epoch  457/1000 Cost: 184.483932\n",
      "Epoch  458/1000 Cost: 184.483932\n",
      "Epoch  459/1000 Cost: 184.483932\n",
      "Epoch  460/1000 Cost: 184.483932\n",
      "Epoch  461/1000 Cost: 184.483932\n",
      "Epoch  462/1000 Cost: 184.483932\n",
      "Epoch  463/1000 Cost: 184.483932\n",
      "Epoch  464/1000 Cost: 184.483932\n",
      "Epoch  465/1000 Cost: 184.483932\n",
      "Epoch  466/1000 Cost: 184.483932\n",
      "Epoch  467/1000 Cost: 184.483932\n",
      "Epoch  468/1000 Cost: 184.483932\n",
      "Epoch  469/1000 Cost: 184.483932\n",
      "Epoch  470/1000 Cost: 184.483932\n",
      "Epoch  471/1000 Cost: 184.483932\n",
      "Epoch  472/1000 Cost: 184.483932\n",
      "Epoch  473/1000 Cost: 184.483932\n",
      "Epoch  474/1000 Cost: 184.483932\n",
      "Epoch  475/1000 Cost: 184.483932\n",
      "Epoch  476/1000 Cost: 184.483932\n",
      "Epoch  477/1000 Cost: 184.483932\n",
      "Epoch  478/1000 Cost: 184.483932\n",
      "Epoch  479/1000 Cost: 184.483932\n",
      "Epoch  480/1000 Cost: 184.483932\n",
      "Epoch  481/1000 Cost: 184.483932\n",
      "Epoch  482/1000 Cost: 184.483917\n",
      "Epoch  483/1000 Cost: 184.483932\n",
      "Epoch  484/1000 Cost: 184.483917\n",
      "Epoch  485/1000 Cost: 184.483932\n",
      "Epoch  486/1000 Cost: 184.483917\n",
      "Epoch  487/1000 Cost: 184.483917\n",
      "Epoch  488/1000 Cost: 184.483917\n",
      "Epoch  489/1000 Cost: 184.483917\n",
      "Epoch  490/1000 Cost: 184.483932\n",
      "Epoch  491/1000 Cost: 184.483917\n",
      "Epoch  492/1000 Cost: 184.483917\n",
      "Epoch  493/1000 Cost: 184.483917\n",
      "Epoch  494/1000 Cost: 184.483917\n",
      "Epoch  495/1000 Cost: 184.483917\n",
      "Epoch  496/1000 Cost: 184.483932\n",
      "Epoch  497/1000 Cost: 184.483917\n",
      "Epoch  498/1000 Cost: 184.483932\n",
      "Epoch  499/1000 Cost: 184.483917\n",
      "Epoch  500/1000 Cost: 184.483932\n",
      "Epoch  501/1000 Cost: 184.483932\n",
      "Epoch  502/1000 Cost: 184.483932\n",
      "Epoch  503/1000 Cost: 184.483932\n",
      "Epoch  504/1000 Cost: 184.483932\n",
      "Epoch  505/1000 Cost: 184.483932\n",
      "Epoch  506/1000 Cost: 184.483932\n",
      "Epoch  507/1000 Cost: 184.483932\n",
      "Epoch  508/1000 Cost: 184.483932\n",
      "Epoch  509/1000 Cost: 184.483932\n",
      "Epoch  510/1000 Cost: 184.483932\n",
      "Epoch  511/1000 Cost: 184.483932\n",
      "Epoch  512/1000 Cost: 184.483932\n",
      "Epoch  513/1000 Cost: 184.483932\n",
      "Epoch  514/1000 Cost: 184.483932\n",
      "Epoch  515/1000 Cost: 184.483932\n",
      "Epoch  516/1000 Cost: 184.483932\n",
      "Epoch  517/1000 Cost: 184.483932\n",
      "Epoch  518/1000 Cost: 184.483932\n",
      "Epoch  519/1000 Cost: 184.483932\n",
      "Epoch  520/1000 Cost: 184.483932\n",
      "Epoch  521/1000 Cost: 184.483917\n",
      "Epoch  522/1000 Cost: 184.483917\n",
      "Epoch  523/1000 Cost: 184.483917\n",
      "Epoch  524/1000 Cost: 184.483917\n",
      "Epoch  525/1000 Cost: 184.483917\n",
      "Epoch  526/1000 Cost: 184.483917\n",
      "Epoch  527/1000 Cost: 184.483917\n",
      "Epoch  528/1000 Cost: 184.483917\n",
      "Epoch  529/1000 Cost: 184.483932\n",
      "Epoch  530/1000 Cost: 184.483932\n",
      "Epoch  531/1000 Cost: 184.483932\n",
      "Epoch  532/1000 Cost: 184.483932\n",
      "Epoch  533/1000 Cost: 184.483932\n",
      "Epoch  534/1000 Cost: 184.483932\n",
      "Epoch  535/1000 Cost: 184.483932\n",
      "Epoch  536/1000 Cost: 184.483932\n",
      "Epoch  537/1000 Cost: 184.483932\n",
      "Epoch  538/1000 Cost: 184.483932\n",
      "Epoch  539/1000 Cost: 184.483932\n",
      "Epoch  540/1000 Cost: 184.483932\n",
      "Epoch  541/1000 Cost: 184.483932\n",
      "Epoch  542/1000 Cost: 184.483932\n",
      "Epoch  543/1000 Cost: 184.483932\n",
      "Epoch  544/1000 Cost: 184.483932\n",
      "Epoch  545/1000 Cost: 184.483932\n",
      "Epoch  546/1000 Cost: 184.483932\n",
      "Epoch  547/1000 Cost: 184.483932\n",
      "Epoch  548/1000 Cost: 184.483932\n",
      "Epoch  549/1000 Cost: 184.483932\n",
      "Epoch  550/1000 Cost: 184.483932\n",
      "Epoch  551/1000 Cost: 184.483932\n",
      "Epoch  552/1000 Cost: 184.483932\n",
      "Epoch  553/1000 Cost: 184.483932\n",
      "Epoch  554/1000 Cost: 184.483932\n",
      "Epoch  555/1000 Cost: 184.483932\n",
      "Epoch  556/1000 Cost: 184.483932\n",
      "Epoch  557/1000 Cost: 184.483917\n",
      "Epoch  558/1000 Cost: 184.483932\n",
      "Epoch  559/1000 Cost: 184.483932\n",
      "Epoch  560/1000 Cost: 184.483932\n",
      "Epoch  561/1000 Cost: 184.483932\n",
      "Epoch  562/1000 Cost: 184.483932\n",
      "Epoch  563/1000 Cost: 184.483932\n",
      "Epoch  564/1000 Cost: 184.483932\n",
      "Epoch  565/1000 Cost: 184.483932\n",
      "Epoch  566/1000 Cost: 184.483932\n",
      "Epoch  567/1000 Cost: 184.483932\n",
      "Epoch  568/1000 Cost: 184.483932\n",
      "Epoch  569/1000 Cost: 184.483932\n",
      "Epoch  570/1000 Cost: 184.483932\n",
      "Epoch  571/1000 Cost: 184.483932\n",
      "Epoch  572/1000 Cost: 184.483932\n",
      "Epoch  573/1000 Cost: 184.483932\n",
      "Epoch  574/1000 Cost: 184.483932\n",
      "Epoch  575/1000 Cost: 184.483932\n",
      "Epoch  576/1000 Cost: 184.483932\n",
      "Epoch  577/1000 Cost: 184.483932\n",
      "Epoch  578/1000 Cost: 184.483932\n",
      "Epoch  579/1000 Cost: 184.483932\n",
      "Epoch  580/1000 Cost: 184.483932\n",
      "Epoch  581/1000 Cost: 184.483932\n",
      "Epoch  582/1000 Cost: 184.483932\n",
      "Epoch  583/1000 Cost: 184.483932\n",
      "Epoch  584/1000 Cost: 184.483932\n",
      "Epoch  585/1000 Cost: 184.483932\n",
      "Epoch  586/1000 Cost: 184.483932\n",
      "Epoch  587/1000 Cost: 184.483932\n",
      "Epoch  588/1000 Cost: 184.483932\n",
      "Epoch  589/1000 Cost: 184.483932\n",
      "Epoch  590/1000 Cost: 184.483932\n",
      "Epoch  591/1000 Cost: 184.483932\n",
      "Epoch  592/1000 Cost: 184.483932\n",
      "Epoch  593/1000 Cost: 184.483932\n",
      "Epoch  594/1000 Cost: 184.483932\n",
      "Epoch  595/1000 Cost: 184.483932\n",
      "Epoch  596/1000 Cost: 184.483932\n",
      "Epoch  597/1000 Cost: 184.483932\n",
      "Epoch  598/1000 Cost: 184.483932\n",
      "Epoch  599/1000 Cost: 184.483932\n",
      "Epoch  600/1000 Cost: 184.483932\n",
      "Epoch  601/1000 Cost: 184.483932\n",
      "Epoch  602/1000 Cost: 184.483932\n",
      "Epoch  603/1000 Cost: 184.483932\n",
      "Epoch  604/1000 Cost: 184.483932\n",
      "Epoch  605/1000 Cost: 184.483932\n",
      "Epoch  606/1000 Cost: 184.483932\n",
      "Epoch  607/1000 Cost: 184.483932\n",
      "Epoch  608/1000 Cost: 184.483932\n",
      "Epoch  609/1000 Cost: 184.483932\n",
      "Epoch  610/1000 Cost: 184.483932\n",
      "Epoch  611/1000 Cost: 184.483917\n",
      "Epoch  612/1000 Cost: 184.483932\n",
      "Epoch  613/1000 Cost: 184.483917\n",
      "Epoch  614/1000 Cost: 184.483932\n",
      "Epoch  615/1000 Cost: 184.483932\n",
      "Epoch  616/1000 Cost: 184.483932\n",
      "Epoch  617/1000 Cost: 184.483917\n",
      "Epoch  618/1000 Cost: 184.483917\n",
      "Epoch  619/1000 Cost: 184.483932\n",
      "Epoch  620/1000 Cost: 184.483917\n",
      "Epoch  621/1000 Cost: 184.483932\n",
      "Epoch  622/1000 Cost: 184.483932\n",
      "Epoch  623/1000 Cost: 184.483932\n",
      "Epoch  624/1000 Cost: 184.483932\n",
      "Epoch  625/1000 Cost: 184.483917\n",
      "Epoch  626/1000 Cost: 184.483917\n",
      "Epoch  627/1000 Cost: 184.483932\n",
      "Epoch  628/1000 Cost: 184.483932\n",
      "Epoch  629/1000 Cost: 184.483917\n",
      "Epoch  630/1000 Cost: 184.483932\n",
      "Epoch  631/1000 Cost: 184.483917\n",
      "Epoch  632/1000 Cost: 184.483932\n",
      "Epoch  633/1000 Cost: 184.483917\n",
      "Epoch  634/1000 Cost: 184.483932\n",
      "Epoch  635/1000 Cost: 184.483917\n",
      "Epoch  636/1000 Cost: 184.483932\n",
      "Epoch  637/1000 Cost: 184.483917\n",
      "Epoch  638/1000 Cost: 184.483917\n",
      "Epoch  639/1000 Cost: 184.483917\n",
      "Epoch  640/1000 Cost: 184.483917\n",
      "Epoch  641/1000 Cost: 184.483917\n",
      "Epoch  642/1000 Cost: 184.483917\n",
      "Epoch  643/1000 Cost: 184.483917\n",
      "Epoch  644/1000 Cost: 184.483917\n",
      "Epoch  645/1000 Cost: 184.483917\n",
      "Epoch  646/1000 Cost: 184.483917\n",
      "Epoch  647/1000 Cost: 184.483917\n",
      "Epoch  648/1000 Cost: 184.483917\n",
      "Epoch  649/1000 Cost: 184.483917\n",
      "Epoch  650/1000 Cost: 184.483917\n",
      "Epoch  651/1000 Cost: 184.483917\n",
      "Epoch  652/1000 Cost: 184.483917\n",
      "Epoch  653/1000 Cost: 184.483917\n",
      "Epoch  654/1000 Cost: 184.483917\n",
      "Epoch  655/1000 Cost: 184.483917\n",
      "Epoch  656/1000 Cost: 184.483917\n",
      "Epoch  657/1000 Cost: 184.483917\n",
      "Epoch  658/1000 Cost: 184.483917\n",
      "Epoch  659/1000 Cost: 184.483917\n",
      "Epoch  660/1000 Cost: 184.483917\n",
      "Epoch  661/1000 Cost: 184.483917\n",
      "Epoch  662/1000 Cost: 184.483917\n",
      "Epoch  663/1000 Cost: 184.483917\n",
      "Epoch  664/1000 Cost: 184.483917\n",
      "Epoch  665/1000 Cost: 184.483917\n",
      "Epoch  666/1000 Cost: 184.483917\n",
      "Epoch  667/1000 Cost: 184.483917\n",
      "Epoch  668/1000 Cost: 184.483917\n",
      "Epoch  669/1000 Cost: 184.483917\n",
      "Epoch  670/1000 Cost: 184.483917\n",
      "Epoch  671/1000 Cost: 184.483917\n",
      "Epoch  672/1000 Cost: 184.483917\n",
      "Epoch  673/1000 Cost: 184.483917\n",
      "Epoch  674/1000 Cost: 184.483917\n",
      "Epoch  675/1000 Cost: 184.483917\n",
      "Epoch  676/1000 Cost: 184.483917\n",
      "Epoch  677/1000 Cost: 184.483917\n",
      "Epoch  678/1000 Cost: 184.483917\n",
      "Epoch  679/1000 Cost: 184.483917\n",
      "Epoch  680/1000 Cost: 184.483917\n",
      "Epoch  681/1000 Cost: 184.483917\n",
      "Epoch  682/1000 Cost: 184.483917\n",
      "Epoch  683/1000 Cost: 184.483917\n",
      "Epoch  684/1000 Cost: 184.483917\n",
      "Epoch  685/1000 Cost: 184.483917\n",
      "Epoch  686/1000 Cost: 184.483917\n",
      "Epoch  687/1000 Cost: 184.483917\n",
      "Epoch  688/1000 Cost: 184.483917\n",
      "Epoch  689/1000 Cost: 184.483917\n",
      "Epoch  690/1000 Cost: 184.483917\n",
      "Epoch  691/1000 Cost: 184.483917\n",
      "Epoch  692/1000 Cost: 184.483917\n",
      "Epoch  693/1000 Cost: 184.483917\n",
      "Epoch  694/1000 Cost: 184.483917\n",
      "Epoch  695/1000 Cost: 184.483917\n",
      "Epoch  696/1000 Cost: 184.483917\n",
      "Epoch  697/1000 Cost: 184.483917\n",
      "Epoch  698/1000 Cost: 184.483917\n",
      "Epoch  699/1000 Cost: 184.483917\n",
      "Epoch  700/1000 Cost: 184.483917\n",
      "Epoch  701/1000 Cost: 184.483917\n",
      "Epoch  702/1000 Cost: 184.483917\n",
      "Epoch  703/1000 Cost: 184.483917\n",
      "Epoch  704/1000 Cost: 184.483917\n",
      "Epoch  705/1000 Cost: 184.483917\n",
      "Epoch  706/1000 Cost: 184.483917\n",
      "Epoch  707/1000 Cost: 184.483917\n",
      "Epoch  708/1000 Cost: 184.483917\n",
      "Epoch  709/1000 Cost: 184.483917\n",
      "Epoch  710/1000 Cost: 184.483917\n",
      "Epoch  711/1000 Cost: 184.483917\n",
      "Epoch  712/1000 Cost: 184.483917\n",
      "Epoch  713/1000 Cost: 184.483917\n",
      "Epoch  714/1000 Cost: 184.483917\n",
      "Epoch  715/1000 Cost: 184.483917\n",
      "Epoch  716/1000 Cost: 184.483917\n",
      "Epoch  717/1000 Cost: 184.483917\n",
      "Epoch  718/1000 Cost: 184.483917\n",
      "Epoch  719/1000 Cost: 184.483917\n",
      "Epoch  720/1000 Cost: 184.483917\n",
      "Epoch  721/1000 Cost: 184.483917\n",
      "Epoch  722/1000 Cost: 184.483917\n",
      "Epoch  723/1000 Cost: 184.483917\n",
      "Epoch  724/1000 Cost: 184.483917\n",
      "Epoch  725/1000 Cost: 184.483917\n",
      "Epoch  726/1000 Cost: 184.483917\n",
      "Epoch  727/1000 Cost: 184.483917\n",
      "Epoch  728/1000 Cost: 184.483917\n",
      "Epoch  729/1000 Cost: 184.483917\n",
      "Epoch  730/1000 Cost: 184.483917\n",
      "Epoch  731/1000 Cost: 184.483917\n",
      "Epoch  732/1000 Cost: 184.483917\n",
      "Epoch  733/1000 Cost: 184.483917\n",
      "Epoch  734/1000 Cost: 184.483917\n",
      "Epoch  735/1000 Cost: 184.483917\n",
      "Epoch  736/1000 Cost: 184.483917\n",
      "Epoch  737/1000 Cost: 184.483917\n",
      "Epoch  738/1000 Cost: 184.483917\n",
      "Epoch  739/1000 Cost: 184.483917\n",
      "Epoch  740/1000 Cost: 184.483917\n",
      "Epoch  741/1000 Cost: 184.483917\n",
      "Epoch  742/1000 Cost: 184.483917\n",
      "Epoch  743/1000 Cost: 184.483917\n",
      "Epoch  744/1000 Cost: 184.483917\n",
      "Epoch  745/1000 Cost: 184.483917\n",
      "Epoch  746/1000 Cost: 184.483917\n",
      "Epoch  747/1000 Cost: 184.483917\n",
      "Epoch  748/1000 Cost: 184.483917\n",
      "Epoch  749/1000 Cost: 184.483917\n",
      "Epoch  750/1000 Cost: 184.483917\n",
      "Epoch  751/1000 Cost: 184.483932\n",
      "Epoch  752/1000 Cost: 184.483917\n",
      "Epoch  753/1000 Cost: 184.483917\n",
      "Epoch  754/1000 Cost: 184.483917\n",
      "Epoch  755/1000 Cost: 184.483932\n",
      "Epoch  756/1000 Cost: 184.483917\n",
      "Epoch  757/1000 Cost: 184.483917\n",
      "Epoch  758/1000 Cost: 184.483917\n",
      "Epoch  759/1000 Cost: 184.483917\n",
      "Epoch  760/1000 Cost: 184.483917\n",
      "Epoch  761/1000 Cost: 184.483917\n",
      "Epoch  762/1000 Cost: 184.483917\n",
      "Epoch  763/1000 Cost: 184.483932\n",
      "Epoch  764/1000 Cost: 184.483917\n",
      "Epoch  765/1000 Cost: 184.483917\n",
      "Epoch  766/1000 Cost: 184.483917\n",
      "Epoch  767/1000 Cost: 184.483932\n",
      "Epoch  768/1000 Cost: 184.483917\n",
      "Epoch  769/1000 Cost: 184.483917\n",
      "Epoch  770/1000 Cost: 184.483932\n",
      "Epoch  771/1000 Cost: 184.483917\n",
      "Epoch  772/1000 Cost: 184.483917\n",
      "Epoch  773/1000 Cost: 184.483932\n",
      "Epoch  774/1000 Cost: 184.483917\n",
      "Epoch  775/1000 Cost: 184.483932\n",
      "Epoch  776/1000 Cost: 184.483917\n",
      "Epoch  777/1000 Cost: 184.483917\n",
      "Epoch  778/1000 Cost: 184.483932\n",
      "Epoch  779/1000 Cost: 184.483917\n",
      "Epoch  780/1000 Cost: 184.483932\n",
      "Epoch  781/1000 Cost: 184.483917\n",
      "Epoch  782/1000 Cost: 184.483917\n",
      "Epoch  783/1000 Cost: 184.483932\n",
      "Epoch  784/1000 Cost: 184.483917\n",
      "Epoch  785/1000 Cost: 184.483932\n",
      "Epoch  786/1000 Cost: 184.483917\n",
      "Epoch  787/1000 Cost: 184.483932\n",
      "Epoch  788/1000 Cost: 184.483917\n",
      "Epoch  789/1000 Cost: 184.483917\n",
      "Epoch  790/1000 Cost: 184.483932\n",
      "Epoch  791/1000 Cost: 184.483917\n",
      "Epoch  792/1000 Cost: 184.483932\n",
      "Epoch  793/1000 Cost: 184.483917\n",
      "Epoch  794/1000 Cost: 184.483917\n",
      "Epoch  795/1000 Cost: 184.483917\n",
      "Epoch  796/1000 Cost: 184.483932\n",
      "Epoch  797/1000 Cost: 184.483917\n",
      "Epoch  798/1000 Cost: 184.483917\n",
      "Epoch  799/1000 Cost: 184.483932\n",
      "Epoch  800/1000 Cost: 184.483917\n",
      "Epoch  801/1000 Cost: 184.483917\n",
      "Epoch  802/1000 Cost: 184.483917\n",
      "Epoch  803/1000 Cost: 184.483917\n",
      "Epoch  804/1000 Cost: 184.483917\n",
      "Epoch  805/1000 Cost: 184.483917\n",
      "Epoch  806/1000 Cost: 184.483917\n",
      "Epoch  807/1000 Cost: 184.483917\n",
      "Epoch  808/1000 Cost: 184.483917\n",
      "Epoch  809/1000 Cost: 184.483917\n",
      "Epoch  810/1000 Cost: 184.483917\n",
      "Epoch  811/1000 Cost: 184.483917\n",
      "Epoch  812/1000 Cost: 184.483917\n",
      "Epoch  813/1000 Cost: 184.483917\n",
      "Epoch  814/1000 Cost: 184.483917\n",
      "Epoch  815/1000 Cost: 184.483917\n",
      "Epoch  816/1000 Cost: 184.483917\n",
      "Epoch  817/1000 Cost: 184.483917\n",
      "Epoch  818/1000 Cost: 184.483917\n",
      "Epoch  819/1000 Cost: 184.483917\n",
      "Epoch  820/1000 Cost: 184.483932\n",
      "Epoch  821/1000 Cost: 184.483917\n",
      "Epoch  822/1000 Cost: 184.483917\n",
      "Epoch  823/1000 Cost: 184.483917\n",
      "Epoch  824/1000 Cost: 184.483917\n",
      "Epoch  825/1000 Cost: 184.483932\n",
      "Epoch  826/1000 Cost: 184.483917\n",
      "Epoch  827/1000 Cost: 184.483932\n",
      "Epoch  828/1000 Cost: 184.483917\n",
      "Epoch  829/1000 Cost: 184.483917\n",
      "Epoch  830/1000 Cost: 184.483932\n",
      "Epoch  831/1000 Cost: 184.483917\n",
      "Epoch  832/1000 Cost: 184.483917\n",
      "Epoch  833/1000 Cost: 184.483932\n",
      "Epoch  834/1000 Cost: 184.483917\n",
      "Epoch  835/1000 Cost: 184.483917\n",
      "Epoch  836/1000 Cost: 184.483932\n",
      "Epoch  837/1000 Cost: 184.483917\n",
      "Epoch  838/1000 Cost: 184.483917\n",
      "Epoch  839/1000 Cost: 184.483917\n",
      "Epoch  840/1000 Cost: 184.483932\n",
      "Epoch  841/1000 Cost: 184.483917\n",
      "Epoch  842/1000 Cost: 184.483917\n",
      "Epoch  843/1000 Cost: 184.483932\n",
      "Epoch  844/1000 Cost: 184.483917\n",
      "Epoch  845/1000 Cost: 184.483917\n",
      "Epoch  846/1000 Cost: 184.483917\n",
      "Epoch  847/1000 Cost: 184.483932\n",
      "Epoch  848/1000 Cost: 184.483917\n",
      "Epoch  849/1000 Cost: 184.483917\n",
      "Epoch  850/1000 Cost: 184.483932\n",
      "Epoch  851/1000 Cost: 184.483917\n",
      "Epoch  852/1000 Cost: 184.483932\n",
      "Epoch  853/1000 Cost: 184.483917\n",
      "Epoch  854/1000 Cost: 184.483917\n",
      "Epoch  855/1000 Cost: 184.483932\n",
      "Epoch  856/1000 Cost: 184.483917\n",
      "Epoch  857/1000 Cost: 184.483917\n",
      "Epoch  858/1000 Cost: 184.483932\n",
      "Epoch  859/1000 Cost: 184.483917\n",
      "Epoch  860/1000 Cost: 184.483917\n",
      "Epoch  861/1000 Cost: 184.483932\n",
      "Epoch  862/1000 Cost: 184.483917\n",
      "Epoch  863/1000 Cost: 184.483932\n",
      "Epoch  864/1000 Cost: 184.483917\n",
      "Epoch  865/1000 Cost: 184.483917\n",
      "Epoch  866/1000 Cost: 184.483932\n",
      "Epoch  867/1000 Cost: 184.483932\n",
      "Epoch  868/1000 Cost: 184.483917\n",
      "Epoch  869/1000 Cost: 184.483917\n",
      "Epoch  870/1000 Cost: 184.483917\n",
      "Epoch  871/1000 Cost: 184.483932\n",
      "Epoch  872/1000 Cost: 184.483932\n",
      "Epoch  873/1000 Cost: 184.483917\n",
      "Epoch  874/1000 Cost: 184.483917\n",
      "Epoch  875/1000 Cost: 184.483932\n",
      "Epoch  876/1000 Cost: 184.483917\n",
      "Epoch  877/1000 Cost: 184.483917\n",
      "Epoch  878/1000 Cost: 184.483932\n",
      "Epoch  879/1000 Cost: 184.483917\n",
      "Epoch  880/1000 Cost: 184.483917\n",
      "Epoch  881/1000 Cost: 184.483932\n",
      "Epoch  882/1000 Cost: 184.483917\n",
      "Epoch  883/1000 Cost: 184.483917\n",
      "Epoch  884/1000 Cost: 184.483917\n",
      "Epoch  885/1000 Cost: 184.483917\n",
      "Epoch  886/1000 Cost: 184.483917\n",
      "Epoch  887/1000 Cost: 184.483917\n",
      "Epoch  888/1000 Cost: 184.483917\n",
      "Epoch  889/1000 Cost: 184.483917\n",
      "Epoch  890/1000 Cost: 184.483917\n",
      "Epoch  891/1000 Cost: 184.483917\n",
      "Epoch  892/1000 Cost: 184.483917\n",
      "Epoch  893/1000 Cost: 184.483917\n",
      "Epoch  894/1000 Cost: 184.483917\n",
      "Epoch  895/1000 Cost: 184.483917\n",
      "Epoch  896/1000 Cost: 184.483917\n",
      "Epoch  897/1000 Cost: 184.483917\n",
      "Epoch  898/1000 Cost: 184.483917\n",
      "Epoch  899/1000 Cost: 184.483917\n",
      "Epoch  900/1000 Cost: 184.483917\n",
      "Epoch  901/1000 Cost: 184.483917\n",
      "Epoch  902/1000 Cost: 184.483917\n",
      "Epoch  903/1000 Cost: 184.483917\n",
      "Epoch  904/1000 Cost: 184.483917\n",
      "Epoch  905/1000 Cost: 184.483917\n",
      "Epoch  906/1000 Cost: 184.483917\n",
      "Epoch  907/1000 Cost: 184.483917\n",
      "Epoch  908/1000 Cost: 184.483917\n",
      "Epoch  909/1000 Cost: 184.483917\n",
      "Epoch  910/1000 Cost: 184.483917\n",
      "Epoch  911/1000 Cost: 184.483917\n",
      "Epoch  912/1000 Cost: 184.483917\n",
      "Epoch  913/1000 Cost: 184.483917\n",
      "Epoch  914/1000 Cost: 184.483917\n",
      "Epoch  915/1000 Cost: 184.483917\n",
      "Epoch  916/1000 Cost: 184.483917\n",
      "Epoch  917/1000 Cost: 184.483917\n",
      "Epoch  918/1000 Cost: 184.483917\n",
      "Epoch  919/1000 Cost: 184.483917\n",
      "Epoch  920/1000 Cost: 184.483917\n",
      "Epoch  921/1000 Cost: 184.483917\n",
      "Epoch  922/1000 Cost: 184.483917\n",
      "Epoch  923/1000 Cost: 184.483917\n",
      "Epoch  924/1000 Cost: 184.483917\n",
      "Epoch  925/1000 Cost: 184.483917\n",
      "Epoch  926/1000 Cost: 184.483917\n",
      "Epoch  927/1000 Cost: 184.483917\n",
      "Epoch  928/1000 Cost: 184.483917\n",
      "Epoch  929/1000 Cost: 184.483917\n",
      "Epoch  930/1000 Cost: 184.483917\n",
      "Epoch  931/1000 Cost: 184.483917\n",
      "Epoch  932/1000 Cost: 184.483917\n",
      "Epoch  933/1000 Cost: 184.483917\n",
      "Epoch  934/1000 Cost: 184.483917\n",
      "Epoch  935/1000 Cost: 184.483917\n",
      "Epoch  936/1000 Cost: 184.483917\n",
      "Epoch  937/1000 Cost: 184.483917\n",
      "Epoch  938/1000 Cost: 184.483917\n",
      "Epoch  939/1000 Cost: 184.483917\n",
      "Epoch  940/1000 Cost: 184.483917\n",
      "Epoch  941/1000 Cost: 184.483917\n",
      "Epoch  942/1000 Cost: 184.483917\n",
      "Epoch  943/1000 Cost: 184.483917\n",
      "Epoch  944/1000 Cost: 184.483917\n",
      "Epoch  945/1000 Cost: 184.483917\n",
      "Epoch  946/1000 Cost: 184.483917\n",
      "Epoch  947/1000 Cost: 184.483917\n",
      "Epoch  948/1000 Cost: 184.483917\n",
      "Epoch  949/1000 Cost: 184.483917\n",
      "Epoch  950/1000 Cost: 184.483917\n",
      "Epoch  951/1000 Cost: 184.483917\n",
      "Epoch  952/1000 Cost: 184.483917\n",
      "Epoch  953/1000 Cost: 184.483917\n",
      "Epoch  954/1000 Cost: 184.483917\n",
      "Epoch  955/1000 Cost: 184.483917\n",
      "Epoch  956/1000 Cost: 184.483917\n",
      "Epoch  957/1000 Cost: 184.483917\n",
      "Epoch  958/1000 Cost: 184.483917\n",
      "Epoch  959/1000 Cost: 184.483917\n",
      "Epoch  960/1000 Cost: 184.483917\n",
      "Epoch  961/1000 Cost: 184.483917\n",
      "Epoch  962/1000 Cost: 184.483917\n",
      "Epoch  963/1000 Cost: 184.483917\n",
      "Epoch  964/1000 Cost: 184.483917\n",
      "Epoch  965/1000 Cost: 184.483917\n",
      "Epoch  966/1000 Cost: 184.483917\n",
      "Epoch  967/1000 Cost: 184.483917\n",
      "Epoch  968/1000 Cost: 184.483917\n",
      "Epoch  969/1000 Cost: 184.483917\n",
      "Epoch  970/1000 Cost: 184.483917\n",
      "Epoch  971/1000 Cost: 184.483917\n",
      "Epoch  972/1000 Cost: 184.483917\n",
      "Epoch  973/1000 Cost: 184.483917\n",
      "Epoch  974/1000 Cost: 184.483917\n",
      "Epoch  975/1000 Cost: 184.483917\n",
      "Epoch  976/1000 Cost: 184.483917\n",
      "Epoch  977/1000 Cost: 184.483917\n",
      "Epoch  978/1000 Cost: 184.483917\n",
      "Epoch  979/1000 Cost: 184.483917\n",
      "Epoch  980/1000 Cost: 184.483917\n",
      "Epoch  981/1000 Cost: 184.483917\n",
      "Epoch  982/1000 Cost: 184.483917\n",
      "Epoch  983/1000 Cost: 184.483917\n",
      "Epoch  984/1000 Cost: 184.483917\n",
      "Epoch  985/1000 Cost: 184.483917\n",
      "Epoch  986/1000 Cost: 184.483917\n",
      "Epoch  987/1000 Cost: 184.483917\n",
      "Epoch  988/1000 Cost: 184.483917\n",
      "Epoch  989/1000 Cost: 184.483917\n",
      "Epoch  990/1000 Cost: 184.483917\n",
      "Epoch  991/1000 Cost: 184.483917\n",
      "Epoch  992/1000 Cost: 184.483917\n",
      "Epoch  993/1000 Cost: 184.483917\n",
      "Epoch  994/1000 Cost: 184.483917\n",
      "Epoch  995/1000 Cost: 184.483917\n",
      "Epoch  996/1000 Cost: 184.483917\n",
      "Epoch  997/1000 Cost: 184.483917\n",
      "Epoch  998/1000 Cost: 184.483917\n",
      "Epoch  999/1000 Cost: 184.483917\n",
      "Epoch 1000/1000 Cost: 184.483917\n"
     ]
    }
   ],
   "source": [
    "# data progress\n",
    "mu=x_train.mean(dim=0)\n",
    "sigma=x_train.std(dim=0)\n",
    "norm_x_train=(x_train-mu)/sigma\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(3,1)\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model=Model()\n",
    "optimizer=optim.SGD(model.parameters(),lr=0.1)\n",
    "epcohs=1000\n",
    "for i in range(epochs+1):\n",
    "    y_pred=model(norm_x_train)\n",
    "    loss=F.mse_loss(y_pred,y_train)\n",
    "    l2=0\n",
    "    for param in model.parameters():\n",
    "        l2+=torch.norm(param)\n",
    "    loss+=l2\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            i, epoch, loss.item()\n",
    "        ))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a22de-9e2d-4286-9c5d-4b27ef833f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
