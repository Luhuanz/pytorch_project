# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
""" Utils to interact with the Triton Inference Server
"""
#Tritonæ˜¯NVIDIAå¼€æºçš„é«˜æ€§èƒ½æ¨ç†æœåŠ¡å™¨ï¼Œå¯ä»¥ç”¨äºå¿«é€Ÿéƒ¨ç½²å’Œæ‰©å±•æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¨ç†æœåŠ¡ã€‚
# å®ƒæ”¯æŒå¤šç§æ¡†æ¶å’Œæ¨¡å‹æ ¼å¼ï¼Œå¯ä»¥é€šè¿‡gRPCæˆ–HTTPè¿›è¡Œé€šä¿¡ï¼Œå¹¶æä¾›äº†çµæ´»çš„æ‰©å±•å’Œè‡ªå®šä¹‰åŠŸèƒ½ã€‚
#Triton Inference Serveræ˜¯ä¸€ä¸ªé«˜æ€§èƒ½å¼€æºæ¨ç†æœåŠ¡å™¨ï¼Œå¯ä»¥ç”¨äºæ¨ç†æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
import typing
from urllib.parse import urlparse

import torch


class TritonRemoteModel:
    """ A wrapper over a model served by the Triton Inference Server. It can
    be configured to communicate over GRPC or HTTP. It accepts Torch Tensors
    as input and returns them as outputs.
    """

    def __init__(self, url: str):
#ritonRemoteModel æ˜¯ä¸€ä¸ªåŒ…è£…å™¨ï¼Œç”¨äºä¸ Triton æ¨ç†æœåŠ¡å™¨ä¸Šæä¾›çš„æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚å®ƒå¯ä»¥é…ç½®ä¸ºé€šè¿‡ GRPC æˆ– HTTP è¿›è¡Œé€šä¿¡ã€‚
# å®ƒæ¥å— Torch å¼ é‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å®ƒä»¬ä½œä¸ºè¾“å‡ºè¿”å›ã€‚
        """
        Keyword arguments:
        url: Fully qualified address of the Triton server - for e.g. grpc://localhost:8000
        """

        parsed_url = urlparse(url) #è¯¥å‡½æ•°é¦–å…ˆè§£æ urlï¼Œ
# åˆ¤æ–­ä½¿ç”¨ GRPC è¿˜æ˜¯ HTTP é€šä¿¡ï¼ŒPython çš„æ¡ä»¶è¯­å¥æ¥åˆ¤æ–­ Triton æ¨ç†æœåŠ¡å™¨çš„é€šä¿¡åè®®ã€‚
        if parsed_url.scheme == "grpc":
    #å¯¼å…¥ tritonclient.grpc æ¨¡å—ä¸­çš„ InferenceServerClient å’Œ InferInput ç±»ã€‚
            from tritonclient.grpc import InferenceServerClient, InferInput
#ç”¨äºä¸å…¶è¿›è¡Œé€šä¿¡å¹¶è¿›è¡Œæ¨ç†ä»»åŠ¡ã€‚å®ƒæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œä¸€ä¸ªæ˜¯ä½¿ç”¨GRPCåè®®è¿›è¡Œé€šä¿¡çš„ç‰ˆæœ¬ï¼Œå¦ä¸€ä¸ªæ˜¯ä½¿ç”¨HTTPåè®®è¿›è¡Œé€šä¿¡çš„ç‰ˆæœ¬ã€‚
            self.client = InferenceServerClient(parsed_url.netloc)  # Triton GRPC client
#model_repository æ˜¯è·å– Triton Inference Server ä¸Šæ¨¡å‹ä»“åº“çš„ç´¢å¼•ã€‚
# å®ƒåŒ…å«æœ‰å…³ä»“åº“ä¸­å¯ç”¨æ¨¡å‹çš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œä¾‹å¦‚æ¨¡å‹åç§°ã€ç‰ˆæœ¬ç­‰ã€‚è¿™ä¸ªä¿¡æ¯ä¼šåœ¨åç»­çš„æ“ä½œä¸­ä½¿ç”¨åˆ°ï¼Œä¾‹å¦‚è·å–æ¨¡å‹å…ƒæ•°æ®å’Œè¾“å…¥/è¾“å‡ºæ•°æ®çš„å½¢çŠ¶ã€ç±»å‹ç­‰ã€‚
            model_repository = self.client.get_model_repository_index()
#è¡Œä»£ç æ˜¯è·å–Tritonæ¨ç†æœåŠ¡å™¨ä¸Šçš„æ¨¡å‹åˆ—è¡¨ï¼Œç„¶åä»ä¸­è·å–ç¬¬ä¸€ä¸ªæ¨¡å‹çš„åç§°ï¼Œå­˜å‚¨åœ¨TritonRemoteModelå¯¹è±¡çš„self.model_nameå±æ€§ä¸­ã€‚
            self.model_name = model_repository.models[0].name
#ä» Triton æ¨ç†æœåŠ¡å™¨ä¸­è·å–æ¨¡å‹å…ƒæ•°æ®ï¼ˆmetadataï¼‰ã€‚self.model_name å­˜å‚¨äº†æ¨ç†æœåŠ¡å™¨ä¸Šç¬¬ä¸€ä¸ªæ¨¡å‹çš„åç§°ï¼Œå®ƒæ˜¯é€šè¿‡ model_repository è·å–çš„ã€‚as_json=True å‚æ•°ç”¨äºå°†å…ƒæ•°æ®ä»¥ JSON æ ¼å¼è¿”å›ã€‚
            self.metadata = self.client.get_model_metadata(self.model_name, as_json=True)
#è¯¥å‡½æ•°é€šè¿‡éå† metadata['inputs'] çš„æ¯ä¸ªå…ƒç´ ï¼Œä¸ºæ¯ä¸ªè¾“å…¥å ä½ç¬¦åˆ›å»ºä¸€ä¸ª InferInput å¯¹è±¡ï¼Œ
# å…¶ä¸­ i['name'] æ˜¯å ä½ç¬¦çš„åç§°ï¼Œ[int(s) for s in i["shape"]] æ˜¯å ä½ç¬¦çš„å½¢çŠ¶ï¼Œi['datatype'] æ˜¯å ä½ç¬¦çš„æ•°æ®ç±»å‹ã€‚
    # æœ€ç»ˆï¼Œå‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰è¾“å…¥å ä½ç¬¦çš„åˆ—è¡¨ã€‚
            def create_input_placeholders() -> typing.List[InferInput]:
                return [
                    InferInput(i['name'], [int(s) for s in i["shape"]], i['datatype']) for i in self.metadata['inputs']]

        else:
            from tritonclient.http import InferenceServerClient, InferInput

            self.client = InferenceServerClient(parsed_url.netloc)  # Triton HTTP client
            model_repository = self.client.get_model_repository_index()
            self.model_name = model_repository[0]['name']
            self.metadata = self.client.get_model_metadata(self.model_name)

            def create_input_placeholders() -> typing.List[InferInput]:
                return [
                    InferInput(i['name'], [int(s) for s in i["shape"]], i['datatype']) for i in self.metadata['inputs']]

        self._create_input_placeholders_fn = create_input_placeholders
#
    @property
#runtime æ–¹æ³•è¿”å›æ¨¡å‹æ‰€åœ¨çš„è¿è¡Œæ—¶ã€‚åœ¨ Triton Inference Server ä¸­ï¼Œæ¨¡å‹æ˜¯ç”±ç‰¹å®šçš„åç«¯ï¼ˆå¦‚ TensorFlowï¼‰æˆ–å¹³å°ï¼ˆå¦‚ Pythonï¼‰æ”¯æŒçš„ï¼Œè€Œè¿™äº›åç«¯æˆ–å¹³å°å¯ä»¥è¢«ç§°ä¸ºæ¨¡å‹çš„è¿è¡Œæ—¶ã€‚
    def runtime(self):
        """Returns the model runtime"""
        return self.metadata.get("backend", self.metadata.get("platform"))
#è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å®ƒæ¥å—ä»»æ„æ•°é‡çš„ä½ç½®å‚æ•°å’Œå…³é”®å­—å‚æ•°ã€‚ä½ç½®å‚æ•°å’Œå…³é”®å­—å‚æ•°åº”è¯¥åŒ¹é…æ¨¡å‹çš„è¾“å…¥é¡ºåºæˆ–è€…è¾“å…¥åç§°ã€‚
    def __call__(self, *args, **kwargs) -> typing.Union[torch.Tensor, typing.Tuple[torch.Tensor, ...]]:
        """ Invokes the model. Parameters can be provided via args or kwargs.
        args, if provided, are assumed to match the order of inputs of the model.
        kwargs are matched with the model input names.
        """
        inputs = self._create_inputs(*args, **kwargs)
        response = self.client.infer(model_name=self.model_name, inputs=inputs)
        result = []
        for output in self.metadata['outputs']:
            tensor = torch.as_tensor(response.as_numpy(output['name']))
            result.append(tensor)
        return result[0] if len(result) == 1 else result
#ç”¨äºæ ¹æ®è¾“å…¥å€¼åˆ›å»º Triton æ¨¡å‹æ¨ç†æ—¶æ‰€éœ€çš„è¾“å…¥ã€‚
    def _create_inputs(self, *args, **kwargs):
        args_len, kwargs_len = len(args), len(kwargs)
        if not args_len and not kwargs_len:
            raise RuntimeError("No inputs provided.")
        if args_len and kwargs_len:
            raise RuntimeError("Cannot specify args and kwargs at the same time")

        placeholders = self._create_input_placeholders_fn()
        if args_len:
            if args_len != len(placeholders):
                raise RuntimeError(f"Expected {len(placeholders)} inputs, got {args_len}.")
            for input, value in zip(placeholders, args):
                input.set_data_from_numpy(value.cpu().numpy())
        else:
            for input in placeholders:
                value = kwargs[input.name]
                input.set_data_from_numpy(value.cpu().numpy())
        return placeholders
