# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Auto-batch utils
"""

from copy import deepcopy

import numpy as np
import torch

from utils.general import LOGGER, colorstr
from utils.torch_utils import profile

#æ£€æŸ¥YOLOv5çš„è®­ç»ƒæ‰¹é‡å¤§å°ï¼Œå¹¶è®¡ç®—æœ€ä½³æ‰¹é‡å¤§å°ã€‚
def check_train_batch_size(model, imgsz=640, amp=True):
#åœ¨å‡½æ•°å†…éƒ¨ï¼Œå®ƒé€šè¿‡ä½¿ç”¨torch.cuda.amp.autocastæ¥æ£€æŸ¥å½“å‰çš„ç²¾åº¦æ¨¡å¼ï¼Œç„¶åä½¿ç”¨autobatchå‡½æ•°æ¥è®¡ç®—æœ€ä½³çš„æ‰¹é‡å¤§å°ï¼Œæœ€åè¿”å›è®¡ç®—å¾—åˆ°çš„æ‰¹é‡å¤§å°ã€‚
    # Check YOLOv5 training batch size
    with torch.cuda.amp.autocast(amp):
        return autobatch(deepcopy(model).train(), imgsz)  # compute optimal batch size


def autobatch(model, imgsz=640, fraction=0.8, batch_size=16):
    # Automatically estimate best YOLOv5 batch size to use `fraction` of available CUDA memory
    # Usage:
    #     import torch
    #     from utils.autobatch import autobatch
    #     model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False)
    #     print(autobatch(model))

    # Check device
    prefix = colorstr('AutoBatch: ')
#prefix æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒåŒ…å«åœ¨ colorstr() å‡½æ•°ä¸­ï¼Œè¯¥å‡½æ•°å°†å­—ç¬¦ä¸²çš„å‰ç¼€è®¾ç½®ä¸º ANSI é¢œè‰²ä»£ç ï¼Œä»¥ä½¿è¾“å‡ºå…·æœ‰ä¸åŒçš„é¢œè‰²ã€‚åœ¨
    LOGGER.info(f'{prefix}Computing optimal batch size for --imgsz {imgsz}')
    device = next(model.parameters()).device  # get model device
    if device.type == 'cpu':
        LOGGER.info(f'{prefix}CUDA not detected, using default CPU batch-size {batch_size}')
        return batch_size
#è¯¥è¯­å¥åˆ¤æ–­å½“å‰æ˜¯å¦å¼€å¯äº†cudnn.benchmarkï¼Œå¦‚æœå¼€å¯äº†ï¼Œåˆ™ä¼šè‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜çš„å·ç§¯å®ç°ç®—æ³•
    if torch.backends.cudnn.benchmark:
        LOGGER.info(f'{prefix} âš ï¸ Requires torch.backends.cudnn.benchmark=False, using default batch-size {batch_size}')
        return batch_size

    # Inspect CUDA memory
#è¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯å®šä¹‰ä¸€ä¸ªå˜é‡ gbï¼Œå®ƒè¡¨ç¤ºçš„æ˜¯ 2 çš„ 30 æ¬¡æ–¹ï¼Œå³ 1GB çš„å¤§å°ï¼Œç”¨äºæ–¹ä¾¿åœ°è¿›è¡Œå†…å­˜å¤§å°çš„è®¡ç®—ã€‚
    gb = 1 << 30  # bytes to GiB (1024 ** 3)
    d = str(device).upper()  # 'CUDA:0'
    properties = torch.cuda.get_device_properties(device)  # è·å–æŒ‡å®šè®¾å¤‡deviceçš„å±æ€§ä¿¡æ¯ã€‚s
    t = properties.total_memory / gb  # å°†è¯¥è®¾å¤‡çš„æ€»å†…å­˜å¤§å°é™¤ä»¥1024 1024 1024å¾—åˆ°æ€»å†…å­˜å¤§å°ï¼ˆä»¥GiBä¸ºå•ä½ï¼‰ã€‚
    r = torch.cuda.memory_reserved(device) / gb  # ä½¿ç”¨torch.cuda.memory_reserved(device)è·å–å·²ä¿ç•™çš„å†…å­˜å¤§å°
    a = torch.cuda.memory_allocated(device) / gb  # GiB allocated
    f = t - (r + a)  # GiB free è®¡ç®—å¯ç”¨å†…å­˜å¤§å°ï¼Œå³æ€»å†…å­˜å‡å»å·²ä¿ç•™å’Œå·²åˆ†é…å†…å­˜å¤§å°ã€‚
    LOGGER.info(f'{prefix}{d} ({properties.name}) {t:.2f}G total, {r:.2f}G reserved, {a:.2f}G allocated, {f:.2f}G free')

    # Profile batch sizes
# è·å–å½“å‰è®¾å¤‡çš„ä¸€äº›åŸºæœ¬å±æ€§ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ€»æ˜¾å­˜å¤§å°ã€å·²åˆ†é…æ˜¾å­˜å¤§å°å’Œå‰©ä½™æ˜¾å­˜å¤§å°ï¼Œä»¥ä¾¿åç»­è®¡ç®—ã€‚ç„¶åï¼Œå®šä¹‰äº†ä¸€äº›å¾…æµ‹è¯•çš„batch sizesï¼ˆ1ã€2ã€4ã€8å’Œ16ï¼‰
  #é€šè¿‡å¾ªç¯éå†å¾…æµ‹è¯•çš„batch sizesï¼Œåˆ†åˆ«å¯¹æ¨¡å‹è¿›è¡Œ3æ¬¡å‰å‘æ¨ç†ï¼Œå¹¶è®¡ç®—å¹³å‡æ¨ç†æ—¶é—´ï¼Œä»¥æ­¤æ¥è¯„ä¼°æ¨¡å‹åœ¨è¯¥batch sizeä¸‹çš„æœ€å¤§æ‰¿å—èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œè¯¥å‡½æ•°ä¼šè¿”å›æ¨¡å‹åœ¨æ¯ä¸ªæµ‹è¯•batch sizeä¸‹çš„å¹³å‡æ¨ç†æ—¶é—´ï¼Œä»¥åŠä¸€ä¸ªå»ºè®®çš„æœ€å¤§æ‰¹é‡å¤§å°ã€‚
    batch_sizes = [1, 2, 4, 8, 16]
    try:
        img = [torch.empty(b, 3, imgsz, imgsz) for b in batch_sizes]
        results = profile(img, model, n=3, device=device)
    except Exception as e:
        LOGGER.warning(f'{prefix}{e}')
#è‡ªåŠ¨è®¡ç®—é€‚åˆå½“å‰GPUçš„YOLOv5æ¨¡å‹çš„æœ€ä½³batch sizeã€‚
# å®ƒé¦–å…ˆé€šè¿‡æ£€æŸ¥å½“å‰GPUçš„å†…å­˜ä½¿ç”¨æƒ…å†µæ¥ç¡®å®šå¯ç”¨çš„å†…å­˜é‡ï¼Œç„¶åé€šè¿‡æµ‹è¯•ä¸€ç³»åˆ—é¢„å®šä¹‰çš„batch sizeæ¥æ‰¾åˆ°æœ€ä½³çš„batch sizeï¼Œ
    # ä»¥ä¾¿åœ¨ä¿è¯è®­ç»ƒé€Ÿåº¦çš„åŒæ—¶å°½å¯èƒ½åˆ©ç”¨å¯ç”¨çš„å†…å­˜ã€‚æœ€ç»ˆå®ƒä¼šè¾“å‡ºæœ€ä½³çš„batch sizeï¼Œå¹¶å°†å…¶ç”¨äºYOLOv5æ¨¡å‹çš„è®­ç»ƒã€‚
    # Fit a solution
    y = [x[2] for x in results if x]  # memory [2]
    p = np.polyfit(batch_sizes[:len(y)], y, deg=1)  # first degree polynomial fit
    b = int((f * fraction - p[1]) / p[0])  # y intercept (optimal batch size)
    if None in results:  # some sizes failed
        i = results.index(None)  # first fail index
        if b >= batch_sizes[i]:  # y intercept above failure point
            b = batch_sizes[max(i - 1, 0)]  # select prior safe point
    if b < 1 or b > 1024:  # b outside of safe range
        b = batch_size
        LOGGER.warning(f'{prefix}WARNING âš ï¸ CUDA anomaly detected, recommend restart environment and retry command.')

    fraction = (np.polyval(p, b) + r + a) / t  # actual fraction predicted
    LOGGER.info(f'{prefix}Using batch-size {b} for {d} {t * fraction:.2f}G/{t:.2f}G ({fraction * 100:.0f}%) âœ…')
    return b
