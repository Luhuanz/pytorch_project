{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSimple supervised GraphSAGE model as well as examples running the model\\non the Cora and Pubmed datasets.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# from graphsage.encoders import Encoder\n",
    "# from graphsage.aggregators import MeanAggregator\n",
    "\n",
    "from encoders import Encoder\n",
    "from aggregators import MeanAggregator\n",
    "\n",
    "\"\"\"\n",
    "Simple supervised GraphSAGE model as well as examples running the model\n",
    "on the Cora and Pubmed datasets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedGraphSage(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        # 这里面赋值为enc2(经过两层GNN)\n",
    "        self.enc = enc\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "        # 全连接参数矩阵，映射到labels num_classes维度做分类\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        # embeds实际是我们两层GNN后的输出nodes embedding\n",
    "        embeds = self.enc(nodes)\n",
    "        # 最后将nodes * hidden size 映射到 nodes * num_classes(= 7)之后做softmax计算cross entropy\n",
    "        scores = self.weight.mm(embeds)\n",
    "        return scores.t()\n",
    "\n",
    "    def loss(self, nodes, labels):\n",
    "        # 钱箱传播\n",
    "        scores = self.forward(nodes)\n",
    "        # 定义的cross entropy\n",
    "        return self.xent(scores, labels.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora():\n",
    "    # 点的数量\n",
    "    num_nodes = 2708\n",
    "    # 特征数量\n",
    "    num_feats = 1433\n",
    "    # 构建特征矩阵\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "    # 构建节点的ground truth标签\n",
    "    labels = np.empty((num_nodes,1), dtype=np.int64)\n",
    "    # 做一个点的id映射\n",
    "    node_map = {}\n",
    "    label_map = {}\n",
    "\n",
    "    # 读节点特征\n",
    "    # cora.content第一列是node id, 中间为点的特征，最后一列为label\n",
    "    # with open(\"cora/cora.content\") as fp:\n",
    "    with open(\"../cora/cora.content\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            # 特征，全部转换成float类型\n",
    "            # feat_data[i,:] = map(float, info[1:-1])\n",
    "            tmp = []\n",
    "            for ss in info[1:-1]:\n",
    "                tmp.append(float(ss))\n",
    "            feat_data[i,:] = tmp\n",
    "            \n",
    "            # 将点的id转换，映射到从0开始的。info[0]是node old id,\n",
    "            node_map[info[0]] = i\n",
    "            # info[-1]是label, 字符串, 比如'Neural_Networks'和'Rule_Learning', 转换成int来表示类\n",
    "            if not info[-1] in label_map:\n",
    "                label_map[info[-1]] = len(label_map)\n",
    "            labels[i] = label_map[info[-1]]\n",
    "\n",
    "    # 读图存储成邻接表\n",
    "    adj_lists = defaultdict(set)\n",
    "    with open(\"../cora/cora.cites\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            # 每一行是一条边\n",
    "            info = line.strip().split()\n",
    "            paper1 = node_map[info[0]]\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "    # 举例：(a, b) (a, c) (a, d) (b, c) (b, d)\n",
    "    # 存储后 {a: set(b, c, d), b: set(a, c, d), c: set(a, b), d: set(a, b)}\n",
    "    return feat_data, labels, adj_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cora():\n",
    "    # 随机数设置seed(种子)\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    # cora数据集点数\n",
    "    num_nodes = 2708\n",
    "    # 加载cora数据集, 分别是\n",
    "    # feat_data: 特征\n",
    "    # labels: 标签\n",
    "    # adj_lists: 邻接表，dict (key: node, value: neighbors set)\n",
    "    feat_data, labels, adj_lists = load_cora()\n",
    "    # 设置输入的input features矩阵X的维度 = 点的数量 * 特征维度\n",
    "    features = nn.Embedding(2708, 1433)\n",
    "    # 为矩阵X赋值，参数不更新\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "    # features.cuda()\n",
    "\n",
    "    # 一共两层GNN layer\n",
    "    # 第一层GNN\n",
    "    # 以mean的方式聚合邻居, algorithm 1 line 4\n",
    "    agg1 = MeanAggregator(features, cuda=True)\n",
    "    # 将自身和聚合邻居的向量拼接后送入到神经网络(可选是否只用聚合邻居的信息来表示), algorithm 1 line 5\n",
    "    enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "\n",
    "    # 第二层GNN\n",
    "    # 将第一层的GNN输出作为输入传进去\n",
    "    # 这里面.t()表示转置，是因为Encoder class的输出维度为embed_dim * nodes\n",
    "    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "    # enc1.embed_dim = 128, 变换后的维度还是128\n",
    "    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,\n",
    "            base_model=enc1, gcn=True, cuda=False)\n",
    "\n",
    "    # 采样的邻居点的数量\n",
    "    enc1.num_samples = 5\n",
    "    enc2.num_samples = 5\n",
    "\n",
    "    # 7分类问题\n",
    "    # enc2是经过两层GNN layer时候得到的 node embedding/features\n",
    "    graphsage = SupervisedGraphSage(7, enc2)\n",
    "    # graphsage.cuda()\n",
    "\n",
    "    # 目的是打乱节点顺序\n",
    "    rand_indices = np.random.permutation(num_nodes)\n",
    "\n",
    "    # 划分测试集、验证集、训练集\n",
    "    test = rand_indices[:1000]\n",
    "    val = rand_indices[1000:1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "\n",
    "    # 用SGD的优化，设置学习率\n",
    "    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n",
    "    # 记录每个batch训练时间\n",
    "    times = []\n",
    "    # 共训练100个batch\n",
    "    for batch in range(100):\n",
    "        # 取256个nodes作为一个batch\n",
    "        batch_nodes = train[:256]\n",
    "        # 打乱训练集的顺序，使下次迭代batch随机\n",
    "        random.shuffle(train)\n",
    "        # 记录开始时间\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        # 这个是SupervisedGraphSage里面定义的cross entropy loss\n",
    "        loss = graphsage.loss(batch_nodes, \n",
    "                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))\n",
    "        # 反向传播和更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 记录结束时间\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "        # print (batch, loss.data[0])\n",
    "        print (batch, loss.data)\n",
    "\n",
    "    # 做validation\n",
    "    val_output = graphsage.forward(val)\n",
    "    # 计算micro F1 score\n",
    "    print (\"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "    # 计算每个batch的平均训练时间\n",
    "    print (\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pubmed():\n",
    "    #hardcoded for simplicity...\n",
    "    num_nodes = 19717\n",
    "    num_feats = 500\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64)\n",
    "    node_map = {}\n",
    "    with open(\"pubmed-data/Pubmed-Diabetes.NODE.paper.tab\") as fp:\n",
    "        fp.readline()\n",
    "        feat_map = {entry.split(\":\")[1]:i-1 for i,entry in enumerate(fp.readline().split(\"\\t\"))}\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.split(\"\\t\")\n",
    "            node_map[info[0]] = i\n",
    "            labels[i] = int(info[1].split(\"=\")[1])-1\n",
    "            for word_info in info[2:-1]:\n",
    "                word_info = word_info.split(\"=\")\n",
    "                feat_data[i][feat_map[word_info[0]]] = float(word_info[1])\n",
    "    adj_lists = defaultdict(set)\n",
    "    with open(\"pubmed-data/Pubmed-Diabetes.DIRECTED.cites.tab\") as fp:\n",
    "        fp.readline()\n",
    "        fp.readline()\n",
    "        for line in fp:\n",
    "            info = line.strip().split(\"\\t\")\n",
    "            paper1 = node_map[info[1].split(\":\")[1]]\n",
    "            paper2 = node_map[info[-1].split(\":\")[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "    return feat_data, labels, adj_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pubmed():\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_nodes = 19717\n",
    "    feat_data, labels, adj_lists = load_pubmed()\n",
    "    features = nn.Embedding(19717, 500)\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "   # features.cuda()\n",
    "\n",
    "    agg1 = MeanAggregator(features, cuda=True)\n",
    "    enc1 = Encoder(features, 500, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,\n",
    "            base_model=enc1, gcn=True, cuda=False)\n",
    "    enc1.num_samples = 10\n",
    "    enc2.num_samples = 25\n",
    "\n",
    "    graphsage = SupervisedGraphSage(3, enc2)\n",
    "#    graphsage.cuda()\n",
    "    rand_indices = np.random.permutation(num_nodes)\n",
    "    test = rand_indices[:1000]\n",
    "    val = rand_indices[1000:1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "\n",
    "    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n",
    "    times = []\n",
    "    for batch in range(200):\n",
    "        batch_nodes = train[:1024]\n",
    "        random.shuffle(train)\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, \n",
    "                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "        print (batch, loss.data[0])\n",
    "\n",
    "    val_output = graphsage.forward(val) \n",
    "    print (\"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "    print (\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/envs/python37/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.9526)\n",
      "1 tensor(1.9337)\n",
      "2 tensor(1.9120)\n",
      "3 tensor(1.8948)\n",
      "4 tensor(1.8646)\n",
      "5 tensor(1.8379)\n",
      "6 tensor(1.8061)\n",
      "7 tensor(1.7765)\n",
      "8 tensor(1.7303)\n",
      "9 tensor(1.6885)\n",
      "10 tensor(1.6025)\n",
      "11 tensor(1.5917)\n",
      "12 tensor(1.5437)\n",
      "13 tensor(1.5093)\n",
      "14 tensor(1.4577)\n",
      "15 tensor(1.3794)\n",
      "16 tensor(1.3457)\n",
      "17 tensor(1.2853)\n",
      "18 tensor(1.1386)\n",
      "19 tensor(1.0898)\n",
      "20 tensor(1.0590)\n",
      "21 tensor(1.0387)\n",
      "22 tensor(0.9482)\n",
      "23 tensor(0.8871)\n",
      "24 tensor(0.8640)\n",
      "25 tensor(0.8847)\n",
      "26 tensor(0.7707)\n",
      "27 tensor(0.7469)\n",
      "28 tensor(0.7268)\n",
      "29 tensor(0.6874)\n",
      "30 tensor(0.6186)\n",
      "31 tensor(0.6889)\n",
      "32 tensor(0.6337)\n",
      "33 tensor(0.6179)\n",
      "34 tensor(0.6229)\n",
      "35 tensor(0.6171)\n",
      "36 tensor(0.6988)\n",
      "37 tensor(0.5377)\n",
      "38 tensor(0.4776)\n",
      "39 tensor(0.5085)\n",
      "40 tensor(0.5046)\n",
      "41 tensor(0.4095)\n",
      "42 tensor(0.3825)\n",
      "43 tensor(0.4307)\n",
      "44 tensor(0.4305)\n",
      "45 tensor(0.3681)\n",
      "46 tensor(0.3415)\n",
      "47 tensor(0.3196)\n",
      "48 tensor(0.3558)\n",
      "49 tensor(0.4070)\n",
      "50 tensor(0.3418)\n",
      "51 tensor(0.4133)\n",
      "52 tensor(0.4054)\n",
      "53 tensor(0.4708)\n",
      "54 tensor(0.4079)\n",
      "55 tensor(0.5492)\n",
      "56 tensor(0.5459)\n",
      "57 tensor(0.7172)\n",
      "58 tensor(0.5334)\n",
      "59 tensor(0.4048)\n",
      "60 tensor(0.3205)\n",
      "61 tensor(0.3049)\n",
      "62 tensor(0.3694)\n",
      "63 tensor(0.2599)\n",
      "64 tensor(0.2509)\n",
      "65 tensor(0.2684)\n",
      "66 tensor(0.2877)\n",
      "67 tensor(0.3191)\n",
      "68 tensor(0.2583)\n",
      "69 tensor(0.2498)\n",
      "70 tensor(0.2687)\n",
      "71 tensor(0.2065)\n",
      "72 tensor(0.2233)\n",
      "73 tensor(0.2086)\n",
      "74 tensor(0.2078)\n",
      "75 tensor(0.2247)\n",
      "76 tensor(0.2197)\n",
      "77 tensor(0.2015)\n",
      "78 tensor(0.2477)\n",
      "79 tensor(0.2010)\n",
      "80 tensor(0.2139)\n",
      "81 tensor(0.2001)\n",
      "82 tensor(0.2159)\n",
      "83 tensor(0.2152)\n",
      "84 tensor(0.1830)\n",
      "85 tensor(0.1622)\n",
      "86 tensor(0.2174)\n",
      "87 tensor(0.1193)\n",
      "88 tensor(0.2017)\n",
      "89 tensor(0.2116)\n",
      "90 tensor(0.2008)\n",
      "91 tensor(0.1890)\n",
      "92 tensor(0.1882)\n",
      "93 tensor(0.1973)\n",
      "94 tensor(0.1769)\n",
      "95 tensor(0.1922)\n",
      "96 tensor(0.1360)\n",
      "97 tensor(0.1426)\n",
      "98 tensor(0.1842)\n",
      "99 tensor(0.1749)\n",
      "Validation F1: 0.858\n",
      "Average batch time: 0.049343187808990475\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_cora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
