{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0cacdf9",
   "metadata": {},
   "source": [
    "# Structural Deep Network Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3746146",
   "metadata": {},
   "source": [
    "-  参考深度之眼赵老师"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13abd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import utils\n",
    "from data import dataset\n",
    "from models import model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf48344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    # 使用parser加载信息\n",
    "    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter,\n",
    "                            conflict_handler='resolve')\n",
    "    # 输入文件\n",
    "    parser.add_argument('--input', default='./data/cora/cora_edgelist.txt',\n",
    "                        help='Input graph file')\n",
    "    # 训练结果embedding输出文件\n",
    "    parser.add_argument('--output', default='./data/cora/Vec.emb',\n",
    "                        help='Output representation file')\n",
    "    # 并行参数\n",
    "    parser.add_argument('--workers', default=8, type=int,\n",
    "                        help='Number of parallel processes.')\n",
    "    # 处理有/无权图\n",
    "    parser.add_argument('--weighted', action='store_true', default=False,\n",
    "                        help='Treat graph as weighted')\n",
    "    # epoch数量\n",
    "    parser.add_argument('--epochs', default=100, type=int,\n",
    "                        help='The training epochs of SDNE')\n",
    "    # dropout比例值\n",
    "    parser.add_argument('--dropout', default=0.5, type=float,\n",
    "                        help='Dropout rate (1 - keep probability)')\n",
    "    \n",
    "    parser.add_argument('--weight-decay', type=float, default=5e-4,\n",
    "                        help='Weight for L2 loss on embedding matrix')\n",
    "    # 学习率设置\n",
    "    parser.add_argument('--lr', default=0.001, type=float,\n",
    "                        help='learning rate')\n",
    "    # 模型参数，一阶相似度和二阶相似度之间的比重\n",
    "    parser.add_argument('--alpha', default=1e-2, type=float,\n",
    "                        help='alhpa is a hyperparameter in SDNE')\n",
    "    # 图的稀疏性问题，论文中的参数beta, 侧重学习邻接矩阵中为1的值\n",
    "    parser.add_argument('--beta', default=5., type=float,\n",
    "                        help='beta is a hyperparameter in SDNE')\n",
    "    \n",
    "    parser.add_argument('--nu1', default=1e-5, type=float,\n",
    "                        help='nu1 is a hyperparameter in SDNE')\n",
    "    \n",
    "    parser.add_argument('--nu2', default=1e-4, type=float,\n",
    "                        help='nu2 is a hyperparameter in SDNE')\n",
    "    # batch大小\n",
    "    parser.add_argument('--bs', default=100, type=int,\n",
    "                        help='batch size of SDNE')\n",
    "    # 自编码器第一层神经元个数\n",
    "    parser.add_argument('--nhid0', default=1000, type=int,\n",
    "                        help='The first dim')\n",
    "    # 自编码器第二层神经元个数\n",
    "    parser.add_argument('--nhid1', default=128, type=int,\n",
    "                        help='The second dim')\n",
    "    # 学习率步长设置\n",
    "    parser.add_argument('--step_size', default=10, type=int,\n",
    "                        help='The step size for lr')\n",
    "    # 学习率gamma值设置\n",
    "    parser.add_argument('--gamma', default=0.9, type=int,\n",
    "                        help='The gamma for lr')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e68a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b1ea59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, Adj, Node = Read_graph(args.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ba1141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4172d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_graph(file_name):\n",
    "    # 文本文件中的每一行必须含有相同的数据; delimiter分隔符默认是空格; 类型是numpy array\n",
    "    edge = np.loadtxt(file_name).astype(np.int32)\n",
    "    # 得到图中点的最小和最大编号; .min()返回数组中所有元素最小的\n",
    "    min_node, max_node = edge.min(), edge.max()\n",
    "    # Node表示图上一共有多少个顶点，如果标号是从0开始，那么顶点数 = max_node + 1\n",
    "    if min_node == 0:\n",
    "        Node = max_node + 1\n",
    "    else:\n",
    "        Node = max_node\n",
    "    # 这里面使用networkx将图的信息存入\n",
    "    G = nx.Graph()\n",
    "    # Adj就是图的邻接表矩阵，是一个n*n大小的numpy矩阵，这里n是顶点的个数\n",
    "    Adj = np.zeros([Node, Node], dtype=np.int32)\n",
    "    # 遍历边的文件，将每条边存入networkx的图，以及邻接矩阵Adj所对应的位置(i, j)\n",
    "    for i in range(edge.shape[0]):\n",
    "        G.add_edge(edge[i][0], edge[i][1])\n",
    "        if min_node == 0:\n",
    "            Adj[edge[i][0], edge[i][1]] = 1\n",
    "            Adj[edge[i][1], edge[i][0]] = 1\n",
    "        else:\n",
    "            Adj[edge[i][0] - 1, edge[i][1] - 1] = 1\n",
    "            Adj[edge[i][1] - 1, edge[i][0] - 1] = 1\n",
    "    # 转化成tensor\n",
    "    Adj = torch.FloatTensor(Adj)\n",
    "    return G, Adj, Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e299319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继承自pytorch中的data.Dataset类，为了后面batch训练方便\n",
    "class Dataload(data.Dataset):\n",
    "\n",
    "    def __init__(self, Adj, Node):\n",
    "        self.Adj = Adj\n",
    "        self.Node = Node\n",
    "    def __getitem__(self, index):\n",
    "        return index\n",
    "        # adj_batch = self.Adj[index]\n",
    "        # adj_mat = adj_batch[index]\n",
    "        # b_mat = torch.ones_like(adj_batch)\n",
    "        # b_mat[adj_batch != 0] = self.Beta\n",
    "        # return adj_batch, adj_mat, b_mat\n",
    "    def __len__(self):\n",
    "        return self.Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f4de9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ea110fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e3ea516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNN(nn.Module):\n",
    "    def __init__(self, node_size, nhid0, nhid1, droput, alpha):\n",
    "        # 声明神经网络都有哪些层\n",
    "        # 这里的结构是：输入-encode0-encode1-decode0-decode1-输出\n",
    "        \n",
    "        # 于是如果我们输入的向量的长度为node_size，那么每一层神经元的个数分别是\n",
    "        \n",
    "        # 假设我们一次输入的batch_size = K\n",
    "        # 输入 = [K, node_size]\n",
    "        super(MNN, self).__init__()\n",
    "        # encode0: [node_size, nhid0], 这一步输出 = [K, node_size] * [node_size, nhid0] = [K, nhid0]\n",
    "        self.encode0 = nn.Linear(node_size, nhid0)\n",
    "        # encode1: [nhid0, nhid1], 这一步输出 = [K, nhid0] * [nhid0, nhid1] = [K, nhid1]\n",
    "        self.encode1 = nn.Linear(nhid0, nhid1)\n",
    "        # decode0: [nhid1, nhid0], 这一步输出 = [K, nhid1] * [nhid1, nhid0] = [K, nhid0]\n",
    "        self.decode0 = nn.Linear(nhid1, nhid0)\n",
    "        # decode1: [nhid0, node_size], 这一步输出 = [K, nhid0] * [nhid0, node_size] = [K, node_size]\n",
    "        # 自编码器希望最终的输出尽可能与输入相等\n",
    "        self.decode1 = nn.Linear(nhid0, node_size)\n",
    "        self.droput = droput\n",
    "        # 论文公式(5)中的alpha\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, adj_batch, adj_mat, b_mat):\n",
    "        # __init__中声明的神经网络的架构实现\n",
    "        # 激活函数使用的是leaky_relu\n",
    "        # 这里的结构是：输入-encode0-encode1-decode0-decode1-输出\n",
    "        t0 = F.leaky_relu(self.encode0(adj_batch))\n",
    "        t0 = F.leaky_relu(self.encode1(t0))\n",
    "        # 这个中间结果使我们最终要保留的node embeeding\n",
    "        embedding = t0\n",
    "        t0 = F.leaky_relu(self.decode0(t0))\n",
    "        t0 = F.leaky_relu(self.decode1(t0))\n",
    "        embedding_norm = torch.sum(embedding * embedding, dim=1, keepdim=True)\n",
    "        # 一阶相似度的loss值\n",
    "        L_1st = torch.sum(adj_mat * (embedding_norm -\n",
    "                                     2 * torch.mm(embedding, torch.transpose(embedding, dim0=0, dim1=1))\n",
    "                                     + torch.transpose(embedding_norm, dim0=0, dim1=1)))\n",
    "        # 二阶相似度的loss值\n",
    "        # adj_batch是论文中的input X, t0是论文中最后输出的X^\n",
    "        L_2nd = torch.sum(((adj_batch - t0) * b_mat) * ((adj_batch - t0) * b_mat))\n",
    "        return L_1st, self.alpha * L_2nd, L_1st + self.alpha * L_2nd\n",
    "\n",
    "    def savector(self, adj):\n",
    "        t0 = self.encode0(adj)\n",
    "        t0 = self.encode1(t0)\n",
    "        return t0\n",
    "    \n",
    "# broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe909b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./models/model.py中的MNN类，基于pytorch实现的论文中的自编码器\n",
    "model = model.MNN(Node, args.nhid0, args.nhid1, args.dropout, args.alpha)\n",
    "# Adam算法优化模型参数\n",
    "opt = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# 设置模型的学习率的超参数\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=args.step_size, gamma=args.gamma)\n",
    "Data = dataset.Dataload(Adj, Node)\n",
    "# 按batchsize将训练样本分组\n",
    "Data = DataLoader(Data, batch_size=args.bs, shuffle=True, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.0_gpu] *",
   "language": "python",
   "name": "conda-env-pytorch1.0_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
