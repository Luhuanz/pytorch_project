{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINE: Large-scale Information Network Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目录\n",
    "1. 设置模型参数\n",
    "2. 读图，存点和边并做归一化\n",
    "3. 计算点和边的alias table\n",
    "4. Line模型实现\n",
    "5. 模型按边训练以及负采样\n",
    "6. 结果展示和可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from utils.utils import *\n",
    "from utils.line import Line\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 设置模型参数; 读图，存点和边并做归一化\n",
    "\n",
    "1) 设置模型参数\n",
    "设置模型超参数，如1st order, 2nd order，负样本数量(K), embedding维度, batch、epoch、learning rate等\n",
    "\n",
    "2）输入输出\n",
    "\n",
    "输入文件 './data/weighted.karate.edgelist'\n",
    "\n",
    "输出文件 './model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用parser加载信息\n",
    "parser = argparse.ArgumentParser()\n",
    "# 输入文件 \n",
    "# parser.add_argument(\"-g\", \"--graph_path\", type=str, default='./data/erdosrenyi.edgelist')\n",
    "parser.add_argument(\"-g\", \"--graph_path\", type=str, default='./data/weighted.karate.edgelist')\n",
    "# 模型信息输出文件\n",
    "parser.add_argument(\"-save\", \"--save_path\", type=str, default='./model.pt')\n",
    "# 模型损失函数值输出文件\n",
    "parser.add_argument(\"-lossdata\", \"--lossdata_path\", type=str, default='./loss.pkl')\n",
    "\n",
    "# Hyperparams. 超参数\n",
    "# 论文中的1st order, 2nd order\n",
    "parser.add_argument(\"-order\", \"--order\", type=int, default=2)\n",
    "# 负样本数量\n",
    "parser.add_argument(\"-neg\", \"--negsamplesize\", type=int, default=5)\n",
    "# embedding维度\n",
    "parser.add_argument(\"-dim\", \"--dimension\", type=int, default=128)\n",
    "# batch大小\n",
    "parser.add_argument(\"-batchsize\", \"--batchsize\", type=int, default=5)\n",
    "# epoch数量\n",
    "parser.add_argument(\"-epochs\", \"--epochs\", type=int, default=1)\n",
    "# 学习率设置\n",
    "parser.add_argument(\"-lr\", \"--learning_rate\", type=float,\n",
    "                default=0.025)  # As starting value in paper\n",
    "# 负采样指数值设置\n",
    "parser.add_argument(\"-negpow\", \"--negativepower\", type=float, default=0.75)  #3/4 一般都是0.75\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 读图，存点和边并做归一化\n",
    "\n",
    "1）读图\n",
    "自己实现的makeDist函数，在utils.py中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading edgelist file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:00<00:00, 25778.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create dict of distribution when opening file\n",
    "# 读图，函数在utils.py中\n",
    "edgedistdict, nodedistdict, weights, nodedegrees, maxindex = makeDist(\n",
    "args.graph_path, args.negativepower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示模块\n",
    "#normalize的边和点\n",
    "#edgedistdict\n",
    "#nodedistdict\n",
    "\n",
    "# 边的权重和点的出度\n",
    "# weights\n",
    "# nodedegrees\n",
    "\n",
    "# 最大的index\n",
    "#maxindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 计算点和边的alias table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2. Building and sorting scaled probabilities for alias table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:00<00:00, 77171.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2. Building alias table...\n",
      "1/2. Building and sorting scaled probabilities for alias table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2. Building alias table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 构建alias table,达到O(1)的采样效率\n",
    "edgesaliassampler = VoseAlias(edgedistdict)\n",
    "nodesaliassampler = VoseAlias(nodedistdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Line模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "# 按batchsize将训练样本分组\n",
    "batchrange = int(len(edgedistdict) / args.batchsize)\n",
    "print(maxindex)\n",
    "# line.py中的nn.Module类\n",
    "line = Line(maxindex + 1, embed_dim=args.dimension, order=args.order)\n",
    "# SGD算法优化模型\n",
    "opt = optim.SGD(line.parameters(), lr=args.learning_rate,\n",
    "            momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.模型按边训练以及负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 664.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on cpu...\n",
      "\n",
      "Epoch 0\n",
      "tensor([[28, 34, 32,  6,  1, 10,  3],\n",
      "        [31, 33, 27, 30,  5, 27, 23],\n",
      "        [ 1,  6, 19, 29, 20,  7, 21],\n",
      "        [ 6,  7, 19, 25, 27, 32, 20],\n",
      "        [26, 32, 23, 25,  6, 27,  4]])\n",
      "\n",
      "Done training, saving model to ./model.pt\n",
      "Saving loss data at ./loss.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/anaconda2/envs/python37/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Line. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda2/envs/python37/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# 选用gpu或cpu训练\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lossdata = {\"it\": [], \"loss\": []}\n",
    "it = 0\n",
    "helper = 0\n",
    "\n",
    "print(\"\\nTraining on {}...\\n\".format(device))\n",
    "# 共训练epoch次数\n",
    "for epoch in range(args.epochs):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    # 每次训练组数：batchsize\n",
    "    for b in trange(batchrange):\n",
    "        # edgesaliassampler是实现alias building的VoseAlias类，这里采样出batchsize条边\n",
    "        samplededges = edgesaliassampler.sample_n(args.batchsize)\n",
    "        # makeData是utils.py中的函数，为每条边采样出K条负样本边\n",
    "        # 每一条格式是(node i, node j, negative nodes...)\n",
    "        batch = list(makeData(samplededges, args.negsamplesize, weights, nodedegrees,\n",
    "                              nodesaliassampler))\n",
    "        # 转换成tensor格式\n",
    "        batch = torch.LongTensor(batch)\n",
    "        if helper == 0:\n",
    "            print (batch)\n",
    "            helper = 1\n",
    "        # 第0列\n",
    "        v_i = batch[:, 0]\n",
    "        # 第1列\n",
    "        v_j = batch[:, 1]\n",
    "        # 第2列-最后列\n",
    "        negsamples = batch[:, 2:]\n",
    "        # 在做BP之前将gradients置0因为是累加的\n",
    "        line.zero_grad()\n",
    "        # Line模型实现部分\n",
    "        loss = line(v_i, v_j, negsamples, device)\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 根据梯度值更新参数值\n",
    "        opt.step()\n",
    "\n",
    "        lossdata[\"loss\"].append(loss.item())\n",
    "        lossdata[\"it\"].append(it)\n",
    "        it += 1\n",
    "\n",
    "print(\"\\nDone training, saving model to {}\".format(args.save_path))\n",
    "torch.save(line, \"{}\".format(args.save_path))\n",
    "\n",
    "print(\"Saving loss data at {}\".format(args.lossdata_path))\n",
    "with open(args.lossdata_path, \"wb\") as ldata:\n",
    "    pickle.dump(lossdata, ldata)\n",
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.结果展示和可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(35, 128)\n",
      "tensor([[-2.1563e-03, -3.5730e-03,  7.7897e-04, -2.0625e-03,  8.7192e-04,\n",
      "         -1.4623e-03, -9.5805e-04,  2.0489e-03,  2.3217e-03,  1.4508e-03,\n",
      "          1.4889e-03,  2.2237e-03, -1.3446e-03, -2.1977e-03, -1.9399e-03,\n",
      "          4.0687e-04,  1.8620e-03,  2.4981e-03, -3.4427e-03,  8.3303e-04,\n",
      "          2.3330e-03, -3.0304e-03, -2.6531e-03,  3.6443e-03, -7.7189e-04,\n",
      "         -3.7786e-03, -3.4815e-03, -1.4232e-03,  1.3029e-03,  1.4854e-03,\n",
      "         -2.3697e-03,  1.8344e-03,  3.5209e-03, -1.8567e-03,  3.6229e-04,\n",
      "         -3.2596e-03, -2.6341e-03,  4.8524e-04, -6.5288e-04, -5.8510e-04,\n",
      "          3.5332e-03, -3.3168e-03, -3.4773e-03, -9.4892e-05,  3.5563e-03,\n",
      "         -9.3188e-04,  2.3504e-03, -3.0754e-03, -2.9352e-03, -1.8730e-03,\n",
      "          1.0574e-03,  1.4161e-03, -3.4506e-03,  3.8624e-03, -2.6382e-03,\n",
      "          2.3032e-04,  3.0217e-03, -3.3979e-04, -3.2616e-03, -3.0722e-03,\n",
      "         -3.5899e-03,  3.8665e-03, -3.0809e-03,  1.4839e-03, -1.5289e-03,\n",
      "          1.7523e-03,  1.7084e-03,  9.5453e-04,  5.4437e-04,  1.5041e-03,\n",
      "          1.4840e-03,  4.9638e-04,  3.4599e-03, -1.5035e-03, -2.7095e-03,\n",
      "         -2.4233e-03,  2.3277e-03, -4.1098e-04,  1.6435e-03,  3.4060e-03,\n",
      "         -1.8417e-03, -1.1656e-03, -2.6839e-03,  2.9357e-03, -1.6112e-03,\n",
      "          2.9509e-03,  1.6265e-03, -2.1404e-03,  6.5261e-04, -3.3104e-03,\n",
      "         -2.2045e-03,  4.6168e-04,  3.7503e-03,  2.7809e-03, -1.1958e-03,\n",
      "         -1.0737e-03, -1.2114e-04, -3.6455e-03, -1.5532e-03,  3.2746e-03,\n",
      "         -2.8957e-03,  1.5381e-03,  2.9384e-04, -1.8398e-03, -3.0435e-03,\n",
      "         -3.7059e-03, -3.5814e-03,  1.1191e-03, -2.4022e-03,  1.3512e-03,\n",
      "         -1.1326e-03,  9.4719e-04,  6.9987e-04,  1.0697e-03, -6.8784e-04,\n",
      "         -1.0414e-03, -2.6405e-03, -3.6266e-03,  2.4492e-03, -3.8156e-03,\n",
      "          1.7495e-03, -3.8227e-03, -1.3272e-03,  1.6319e-03, -2.0587e-03,\n",
      "         -1.5023e-03,  2.0852e-03,  3.6357e-03]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print (line.nodes_embeddings)\n",
    "input = torch.LongTensor([0])\n",
    "print (line.nodes_embeddings(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 0, 1, 1, 0, 1, 1, 0, 2, 2, 2, 0, 0, 1, 2, 2, 1, 1, 0, 2,\n",
       "       2, 2, 2, 1, 0, 0, 1, 0, 1, 1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k-means聚类\n",
    "from sklearn import  cluster\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "embedding_node=[]\n",
    "for i in range(1,35):\n",
    "    input = torch.LongTensor([i])\n",
    "    t = line.nodes_embeddings(input)\n",
    "    embedding_node.append(t.tolist()[0])\n",
    "embedding_node=np.matrix(embedding_node).reshape((34,-1))\n",
    "y_pred = cluster.KMeans(n_clusters=3, random_state=9).fit_predict(embedding_node) # 调用 test_RandomForestClassifier\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
