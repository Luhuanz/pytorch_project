{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pygcn.utils import load_data, accuracy\n",
    "from pygcn.models import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "# 禁用CUDA训练\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "# 在训练通过期间验证\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "# 随机种子\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "# 要训练的epoch数\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "# 最初的学习率\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "# 权重衰减（参数L2损失）\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "# 隐藏层单元数量\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "# dropout率（1-保持概率)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# 产生随机种子，以使得结果是确定的\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# 加载数据\n",
    "# adj: adj样本关系的对称邻接矩阵的稀疏张量\n",
    "# features: 样本特征张量\n",
    "# labels: 样本标签\n",
    "# idx_train: 训练集索引列表\n",
    "# idx_val: 验证集索引列表\n",
    "# idx_test: 测试集索引列表\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    8,   14,  ..., 1389, 2344, 2707],\n",
       "                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),\n",
       "       values=tensor([0.1667, 0.1667, 0.0500,  ..., 0.2000, 0.5000, 0.2500]),\n",
       "       size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 1,  ..., 6, 5, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213,\n",
       "        214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
       "        228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
       "        242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255,\n",
       "        256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
       "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
       "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
       "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
       "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
       "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
       "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
       "        382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,\n",
       "        396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409,\n",
       "        410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423,\n",
       "        424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437,\n",
       "        438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
       "        452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "        466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479,\n",
       "        480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "        494, 495, 496, 497, 498, 499])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 500,  501,  502,  503,  504,  505,  506,  507,  508,  509,  510,  511,\n",
       "         512,  513,  514,  515,  516,  517,  518,  519,  520,  521,  522,  523,\n",
       "         524,  525,  526,  527,  528,  529,  530,  531,  532,  533,  534,  535,\n",
       "         536,  537,  538,  539,  540,  541,  542,  543,  544,  545,  546,  547,\n",
       "         548,  549,  550,  551,  552,  553,  554,  555,  556,  557,  558,  559,\n",
       "         560,  561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,\n",
       "         572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,  583,\n",
       "         584,  585,  586,  587,  588,  589,  590,  591,  592,  593,  594,  595,\n",
       "         596,  597,  598,  599,  600,  601,  602,  603,  604,  605,  606,  607,\n",
       "         608,  609,  610,  611,  612,  613,  614,  615,  616,  617,  618,  619,\n",
       "         620,  621,  622,  623,  624,  625,  626,  627,  628,  629,  630,  631,\n",
       "         632,  633,  634,  635,  636,  637,  638,  639,  640,  641,  642,  643,\n",
       "         644,  645,  646,  647,  648,  649,  650,  651,  652,  653,  654,  655,\n",
       "         656,  657,  658,  659,  660,  661,  662,  663,  664,  665,  666,  667,\n",
       "         668,  669,  670,  671,  672,  673,  674,  675,  676,  677,  678,  679,\n",
       "         680,  681,  682,  683,  684,  685,  686,  687,  688,  689,  690,  691,\n",
       "         692,  693,  694,  695,  696,  697,  698,  699,  700,  701,  702,  703,\n",
       "         704,  705,  706,  707,  708,  709,  710,  711,  712,  713,  714,  715,\n",
       "         716,  717,  718,  719,  720,  721,  722,  723,  724,  725,  726,  727,\n",
       "         728,  729,  730,  731,  732,  733,  734,  735,  736,  737,  738,  739,\n",
       "         740,  741,  742,  743,  744,  745,  746,  747,  748,  749,  750,  751,\n",
       "         752,  753,  754,  755,  756,  757,  758,  759,  760,  761,  762,  763,\n",
       "         764,  765,  766,  767,  768,  769,  770,  771,  772,  773,  774,  775,\n",
       "         776,  777,  778,  779,  780,  781,  782,  783,  784,  785,  786,  787,\n",
       "         788,  789,  790,  791,  792,  793,  794,  795,  796,  797,  798,  799,\n",
       "         800,  801,  802,  803,  804,  805,  806,  807,  808,  809,  810,  811,\n",
       "         812,  813,  814,  815,  816,  817,  818,  819,  820,  821,  822,  823,\n",
       "         824,  825,  826,  827,  828,  829,  830,  831,  832,  833,  834,  835,\n",
       "         836,  837,  838,  839,  840,  841,  842,  843,  844,  845,  846,  847,\n",
       "         848,  849,  850,  851,  852,  853,  854,  855,  856,  857,  858,  859,\n",
       "         860,  861,  862,  863,  864,  865,  866,  867,  868,  869,  870,  871,\n",
       "         872,  873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
       "         884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,  895,\n",
       "         896,  897,  898,  899,  900,  901,  902,  903,  904,  905,  906,  907,\n",
       "         908,  909,  910,  911,  912,  913,  914,  915,  916,  917,  918,  919,\n",
       "         920,  921,  922,  923,  924,  925,  926,  927,  928,  929,  930,  931,\n",
       "         932,  933,  934,  935,  936,  937,  938,  939,  940,  941,  942,  943,\n",
       "         944,  945,  946,  947,  948,  949,  950,  951,  952,  953,  954,  955,\n",
       "         956,  957,  958,  959,  960,  961,  962,  963,  964,  965,  966,  967,\n",
       "         968,  969,  970,  971,  972,  973,  974,  975,  976,  977,  978,  979,\n",
       "         980,  981,  982,  983,  984,  985,  986,  987,  988,  989,  990,  991,\n",
       "         992,  993,  994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003,\n",
       "        1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
       "        1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027,\n",
       "        1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039,\n",
       "        1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051,\n",
       "        1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063,\n",
       "        1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075,\n",
       "        1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087,\n",
       "        1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099,\n",
       "        1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111,\n",
       "        1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123,\n",
       "        1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135,\n",
       "        1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
       "        1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159,\n",
       "        1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171,\n",
       "        1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183,\n",
       "        1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195,\n",
       "        1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207,\n",
       "        1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219,\n",
       "        1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231,\n",
       "        1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243,\n",
       "        1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255,\n",
       "        1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267,\n",
       "        1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279,\n",
       "        1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291,\n",
       "        1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
       "        1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315,\n",
       "        1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327,\n",
       "        1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339,\n",
       "        1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351,\n",
       "        1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363,\n",
       "        1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375,\n",
       "        1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387,\n",
       "        1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399,\n",
       "        1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411,\n",
       "        1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423,\n",
       "        1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
       "        1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447,\n",
       "        1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459,\n",
       "        1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471,\n",
       "        1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483,\n",
       "        1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495,\n",
       "        1496, 1497, 1498, 1499])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "# 模型和优化器\n",
    "\n",
    "# GCN模型\n",
    "# nfeat输入单元数，shape[1]表示特征矩阵的维度数（列数）\n",
    "# nhid中间层单元数量\n",
    "# nclass输出单元数，即样本标签数=样本标签最大值+1\n",
    "# dropout参数\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "\n",
    "# 构造一个优化器对象Optimizer，用来保存当前的状态，并能够根据计算得到的梯度来更新参数\n",
    "# Adam优化器\n",
    "# lr学习率\n",
    "# weight_decay权重衰减（L2惩罚）\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果使用GPU则执行这里，数据写入cuda，便于后续加速\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(epoch):\n",
    "    # 返回当前时间\n",
    "    t = time.time()\n",
    "    \n",
    "    # train的时候使用dropout, 测试的时候不使用dropout\n",
    "    # pytorch里面eval()固定整个网络参数，没有dropout\n",
    "    \n",
    "    # 固定语句，主要针对启用BatchNormalization和Dropout\n",
    "    model.train()\n",
    "    \n",
    "    # 把梯度置零，也就是把loss关于weight的导数变成0\n",
    "    optimizer.zero_grad()\n",
    "    # 执行GCN中的forward前向传播\n",
    "    output = model(features, adj)\n",
    "    # 最大似然/log似然损失函数，idx_train是140(0~139)\n",
    "    # nll_loss: negative log likelihood loss\n",
    "    # https://www.cnblogs.com/marsggbo/p/10401215.html\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    # 准确率\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    # 反向传播\n",
    "    loss_train.backward()\n",
    "    # 梯度下降，更新值\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate validation set performance separately,\n",
    "    # deactivates dropout during validation run.\n",
    "    # 是否在训练期间进行验证\n",
    "    if not args.fastmode:\n",
    "        # 固定语句，主要针对不启用BatchNormalization和Dropout\n",
    "        model.eval()\n",
    "        # 前向传播\n",
    "        output = model(features, adj)\n",
    "    \n",
    "    # 最大似然/log似然损失函数，idx_val是300(200~499)\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    # 准确率\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    # 正在迭代的epoch数\n",
    "    # 训练集损失函数值\n",
    "    # 训练集准确率\n",
    "    # 验证集损失函数值\n",
    "    # 验证集准确率\n",
    "    # 运行时间\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义测试函数，相当于对已有的模型在测试集上运行对应的loss与accuracy\n",
    "def test():\n",
    "    # 固定语句，主要针对不启用BatchNormalization和Dropout\n",
    "    model.eval()\n",
    "    # 前向传播\n",
    "    output = model(features, adj)\n",
    "    # 最大似然/log似然损失函数，idx_test是1000(500~1499)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    # 准确率\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    \n",
    "    # 测试集损失函数值\n",
    "    # 测试集的准确率\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9390 acc_train: 0.2000 loss_val: 1.9045 acc_val: 0.3500 time: 4.4010s\n",
      "Epoch: 0002 loss_train: 1.9161 acc_train: 0.2929 loss_val: 1.8941 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0003 loss_train: 1.9125 acc_train: 0.2929 loss_val: 1.8844 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0004 loss_train: 1.9027 acc_train: 0.2929 loss_val: 1.8751 acc_val: 0.3500 time: 0.0105s\n",
      "Epoch: 0005 loss_train: 1.8933 acc_train: 0.2929 loss_val: 1.8660 acc_val: 0.3500 time: 0.0076s\n",
      "Epoch: 0006 loss_train: 1.8759 acc_train: 0.2929 loss_val: 1.8574 acc_val: 0.3500 time: 0.0079s\n",
      "Epoch: 0007 loss_train: 1.8626 acc_train: 0.2929 loss_val: 1.8490 acc_val: 0.3500 time: 0.0088s\n",
      "Epoch: 0008 loss_train: 1.8679 acc_train: 0.2929 loss_val: 1.8410 acc_val: 0.3500 time: 0.0082s\n",
      "Epoch: 0009 loss_train: 1.8558 acc_train: 0.2929 loss_val: 1.8335 acc_val: 0.3500 time: 0.0073s\n",
      "Epoch: 0010 loss_train: 1.8452 acc_train: 0.2929 loss_val: 1.8262 acc_val: 0.3500 time: 0.0089s\n",
      "Epoch: 0011 loss_train: 1.8428 acc_train: 0.2929 loss_val: 1.8194 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0012 loss_train: 1.8386 acc_train: 0.2929 loss_val: 1.8128 acc_val: 0.3500 time: 0.0075s\n",
      "Epoch: 0013 loss_train: 1.8225 acc_train: 0.2929 loss_val: 1.8065 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0014 loss_train: 1.8062 acc_train: 0.2929 loss_val: 1.8003 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0015 loss_train: 1.8067 acc_train: 0.2929 loss_val: 1.7941 acc_val: 0.3500 time: 0.0091s\n",
      "Epoch: 0016 loss_train: 1.7937 acc_train: 0.2929 loss_val: 1.7882 acc_val: 0.3500 time: 0.0084s\n",
      "Epoch: 0017 loss_train: 1.7992 acc_train: 0.3000 loss_val: 1.7824 acc_val: 0.3500 time: 0.0075s\n",
      "Epoch: 0018 loss_train: 1.7871 acc_train: 0.2929 loss_val: 1.7767 acc_val: 0.3500 time: 0.0085s\n",
      "Epoch: 0019 loss_train: 1.7758 acc_train: 0.2929 loss_val: 1.7710 acc_val: 0.3500 time: 0.0085s\n",
      "Epoch: 0020 loss_train: 1.7706 acc_train: 0.2929 loss_val: 1.7654 acc_val: 0.3500 time: 0.0095s\n",
      "Epoch: 0021 loss_train: 1.7529 acc_train: 0.3000 loss_val: 1.7599 acc_val: 0.3500 time: 0.0116s\n",
      "Epoch: 0022 loss_train: 1.7504 acc_train: 0.3000 loss_val: 1.7545 acc_val: 0.3500 time: 0.0081s\n",
      "Epoch: 0023 loss_train: 1.7457 acc_train: 0.3071 loss_val: 1.7491 acc_val: 0.3500 time: 0.0126s\n",
      "Epoch: 0024 loss_train: 1.7348 acc_train: 0.2929 loss_val: 1.7438 acc_val: 0.3500 time: 0.0095s\n",
      "Epoch: 0025 loss_train: 1.7333 acc_train: 0.3000 loss_val: 1.7385 acc_val: 0.3500 time: 0.0086s\n",
      "Epoch: 0026 loss_train: 1.7327 acc_train: 0.3000 loss_val: 1.7329 acc_val: 0.3500 time: 0.0085s\n",
      "Epoch: 0027 loss_train: 1.7143 acc_train: 0.3143 loss_val: 1.7270 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0028 loss_train: 1.7193 acc_train: 0.3143 loss_val: 1.7208 acc_val: 0.3500 time: 0.0086s\n",
      "Epoch: 0029 loss_train: 1.6915 acc_train: 0.3071 loss_val: 1.7145 acc_val: 0.3500 time: 0.0075s\n",
      "Epoch: 0030 loss_train: 1.6825 acc_train: 0.3143 loss_val: 1.7082 acc_val: 0.3500 time: 0.0084s\n",
      "Epoch: 0031 loss_train: 1.6866 acc_train: 0.3143 loss_val: 1.7020 acc_val: 0.3500 time: 0.0075s\n",
      "Epoch: 0032 loss_train: 1.6834 acc_train: 0.3286 loss_val: 1.6957 acc_val: 0.3533 time: 0.0101s\n",
      "Epoch: 0033 loss_train: 1.6766 acc_train: 0.3214 loss_val: 1.6891 acc_val: 0.3600 time: 0.0070s\n",
      "Epoch: 0034 loss_train: 1.6866 acc_train: 0.3286 loss_val: 1.6822 acc_val: 0.3633 time: 0.0108s\n",
      "Epoch: 0035 loss_train: 1.6371 acc_train: 0.3571 loss_val: 1.6750 acc_val: 0.3633 time: 0.0105s\n",
      "Epoch: 0036 loss_train: 1.6585 acc_train: 0.3500 loss_val: 1.6675 acc_val: 0.3633 time: 0.0080s\n",
      "Epoch: 0037 loss_train: 1.6349 acc_train: 0.3929 loss_val: 1.6599 acc_val: 0.3600 time: 0.0090s\n",
      "Epoch: 0038 loss_train: 1.6208 acc_train: 0.4000 loss_val: 1.6520 acc_val: 0.3600 time: 0.0080s\n",
      "Epoch: 0039 loss_train: 1.5941 acc_train: 0.3786 loss_val: 1.6435 acc_val: 0.3600 time: 0.0085s\n",
      "Epoch: 0040 loss_train: 1.5862 acc_train: 0.3929 loss_val: 1.6343 acc_val: 0.3600 time: 0.0075s\n",
      "Epoch: 0041 loss_train: 1.5774 acc_train: 0.4214 loss_val: 1.6243 acc_val: 0.3600 time: 0.0085s\n",
      "Epoch: 0042 loss_train: 1.5372 acc_train: 0.4286 loss_val: 1.6137 acc_val: 0.3600 time: 0.0075s\n",
      "Epoch: 0043 loss_train: 1.5557 acc_train: 0.4000 loss_val: 1.6030 acc_val: 0.3700 time: 0.0100s\n",
      "Epoch: 0044 loss_train: 1.5358 acc_train: 0.4429 loss_val: 1.5917 acc_val: 0.3800 time: 0.0081s\n",
      "Epoch: 0045 loss_train: 1.5021 acc_train: 0.4857 loss_val: 1.5802 acc_val: 0.3900 time: 0.0086s\n",
      "Epoch: 0046 loss_train: 1.4938 acc_train: 0.4286 loss_val: 1.5683 acc_val: 0.3900 time: 0.0089s\n",
      "Epoch: 0047 loss_train: 1.4775 acc_train: 0.4857 loss_val: 1.5560 acc_val: 0.3900 time: 0.0081s\n",
      "Epoch: 0048 loss_train: 1.4687 acc_train: 0.4857 loss_val: 1.5438 acc_val: 0.4000 time: 0.0071s\n",
      "Epoch: 0049 loss_train: 1.4575 acc_train: 0.4786 loss_val: 1.5313 acc_val: 0.4200 time: 0.0071s\n",
      "Epoch: 0050 loss_train: 1.4466 acc_train: 0.4929 loss_val: 1.5186 acc_val: 0.4333 time: 0.0091s\n",
      "Epoch: 0051 loss_train: 1.4338 acc_train: 0.4714 loss_val: 1.5060 acc_val: 0.4400 time: 0.0070s\n",
      "Epoch: 0052 loss_train: 1.4318 acc_train: 0.4571 loss_val: 1.4937 acc_val: 0.4600 time: 0.0071s\n",
      "Epoch: 0053 loss_train: 1.3948 acc_train: 0.5000 loss_val: 1.4813 acc_val: 0.4733 time: 0.0080s\n",
      "Epoch: 0054 loss_train: 1.3547 acc_train: 0.5500 loss_val: 1.4691 acc_val: 0.4800 time: 0.0081s\n",
      "Epoch: 0055 loss_train: 1.3548 acc_train: 0.5286 loss_val: 1.4572 acc_val: 0.4933 time: 0.0080s\n",
      "Epoch: 0056 loss_train: 1.3445 acc_train: 0.5500 loss_val: 1.4450 acc_val: 0.5167 time: 0.0073s\n",
      "Epoch: 0057 loss_train: 1.3269 acc_train: 0.6071 loss_val: 1.4326 acc_val: 0.5167 time: 0.0085s\n",
      "Epoch: 0058 loss_train: 1.3007 acc_train: 0.5857 loss_val: 1.4203 acc_val: 0.5400 time: 0.0085s\n",
      "Epoch: 0059 loss_train: 1.2890 acc_train: 0.6214 loss_val: 1.4076 acc_val: 0.5467 time: 0.0086s\n",
      "Epoch: 0060 loss_train: 1.2593 acc_train: 0.6286 loss_val: 1.3951 acc_val: 0.5533 time: 0.0093s\n",
      "Epoch: 0061 loss_train: 1.2838 acc_train: 0.5857 loss_val: 1.3827 acc_val: 0.5633 time: 0.0085s\n",
      "Epoch: 0062 loss_train: 1.2406 acc_train: 0.6571 loss_val: 1.3702 acc_val: 0.5733 time: 0.0085s\n",
      "Epoch: 0063 loss_train: 1.2336 acc_train: 0.6571 loss_val: 1.3573 acc_val: 0.5833 time: 0.0080s\n",
      "Epoch: 0064 loss_train: 1.2474 acc_train: 0.5929 loss_val: 1.3444 acc_val: 0.5900 time: 0.0085s\n",
      "Epoch: 0065 loss_train: 1.2215 acc_train: 0.7071 loss_val: 1.3312 acc_val: 0.5967 time: 0.0074s\n",
      "Epoch: 0066 loss_train: 1.2027 acc_train: 0.6071 loss_val: 1.3182 acc_val: 0.6067 time: 0.0086s\n",
      "Epoch: 0067 loss_train: 1.1572 acc_train: 0.6929 loss_val: 1.3057 acc_val: 0.6133 time: 0.0092s\n",
      "Epoch: 0068 loss_train: 1.1559 acc_train: 0.6643 loss_val: 1.2934 acc_val: 0.6200 time: 0.0086s\n",
      "Epoch: 0069 loss_train: 1.1612 acc_train: 0.6500 loss_val: 1.2814 acc_val: 0.6367 time: 0.0080s\n",
      "Epoch: 0070 loss_train: 1.1386 acc_train: 0.7000 loss_val: 1.2699 acc_val: 0.6567 time: 0.0095s\n",
      "Epoch: 0071 loss_train: 1.1085 acc_train: 0.7000 loss_val: 1.2591 acc_val: 0.6667 time: 0.0075s\n",
      "Epoch: 0072 loss_train: 1.1279 acc_train: 0.7000 loss_val: 1.2491 acc_val: 0.6800 time: 0.0075s\n",
      "Epoch: 0073 loss_train: 1.0841 acc_train: 0.7500 loss_val: 1.2390 acc_val: 0.6933 time: 0.0067s\n",
      "Epoch: 0074 loss_train: 1.0430 acc_train: 0.7500 loss_val: 1.2281 acc_val: 0.6933 time: 0.0080s\n",
      "Epoch: 0075 loss_train: 1.0494 acc_train: 0.7357 loss_val: 1.2173 acc_val: 0.6933 time: 0.0070s\n",
      "Epoch: 0076 loss_train: 1.0421 acc_train: 0.8214 loss_val: 1.2061 acc_val: 0.6967 time: 0.0075s\n",
      "Epoch: 0077 loss_train: 1.0250 acc_train: 0.7786 loss_val: 1.1951 acc_val: 0.7033 time: 0.0073s\n",
      "Epoch: 0078 loss_train: 1.0087 acc_train: 0.7571 loss_val: 1.1847 acc_val: 0.7133 time: 0.0085s\n",
      "Epoch: 0079 loss_train: 1.0200 acc_train: 0.7857 loss_val: 1.1750 acc_val: 0.7200 time: 0.0075s\n",
      "Epoch: 0080 loss_train: 0.9814 acc_train: 0.7643 loss_val: 1.1655 acc_val: 0.7267 time: 0.0075s\n",
      "Epoch: 0081 loss_train: 0.9884 acc_train: 0.7929 loss_val: 1.1559 acc_val: 0.7333 time: 0.0075s\n",
      "Epoch: 0082 loss_train: 0.9724 acc_train: 0.7786 loss_val: 1.1451 acc_val: 0.7500 time: 0.0075s\n",
      "Epoch: 0083 loss_train: 0.9659 acc_train: 0.7643 loss_val: 1.1338 acc_val: 0.7500 time: 0.0070s\n",
      "Epoch: 0084 loss_train: 0.9507 acc_train: 0.8286 loss_val: 1.1230 acc_val: 0.7500 time: 0.0075s\n",
      "Epoch: 0085 loss_train: 0.9390 acc_train: 0.7857 loss_val: 1.1133 acc_val: 0.7500 time: 0.0065s\n",
      "Epoch: 0086 loss_train: 0.9667 acc_train: 0.7571 loss_val: 1.1045 acc_val: 0.7533 time: 0.0075s\n",
      "Epoch: 0087 loss_train: 0.9584 acc_train: 0.7786 loss_val: 1.0964 acc_val: 0.7567 time: 0.0070s\n",
      "Epoch: 0088 loss_train: 0.9491 acc_train: 0.7857 loss_val: 1.0885 acc_val: 0.7567 time: 0.0075s\n",
      "Epoch: 0089 loss_train: 0.8831 acc_train: 0.8000 loss_val: 1.0815 acc_val: 0.7667 time: 0.0082s\n",
      "Epoch: 0090 loss_train: 0.9039 acc_train: 0.8286 loss_val: 1.0739 acc_val: 0.7700 time: 0.0085s\n",
      "Epoch: 0091 loss_train: 0.8625 acc_train: 0.8286 loss_val: 1.0664 acc_val: 0.7700 time: 0.0075s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0092 loss_train: 0.8695 acc_train: 0.8214 loss_val: 1.0585 acc_val: 0.7700 time: 0.0110s\n",
      "Epoch: 0093 loss_train: 0.8396 acc_train: 0.8357 loss_val: 1.0505 acc_val: 0.7800 time: 0.0080s\n",
      "Epoch: 0094 loss_train: 0.8665 acc_train: 0.8429 loss_val: 1.0421 acc_val: 0.7800 time: 0.0075s\n",
      "Epoch: 0095 loss_train: 0.8840 acc_train: 0.8143 loss_val: 1.0345 acc_val: 0.7833 time: 0.0075s\n",
      "Epoch: 0096 loss_train: 0.8822 acc_train: 0.7786 loss_val: 1.0266 acc_val: 0.7900 time: 0.0075s\n",
      "Epoch: 0097 loss_train: 0.8434 acc_train: 0.8286 loss_val: 1.0191 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0098 loss_train: 0.8434 acc_train: 0.8214 loss_val: 1.0118 acc_val: 0.7967 time: 0.0075s\n",
      "Epoch: 0099 loss_train: 0.7982 acc_train: 0.8643 loss_val: 1.0053 acc_val: 0.7967 time: 0.0075s\n",
      "Epoch: 0100 loss_train: 0.8080 acc_train: 0.8286 loss_val: 0.9992 acc_val: 0.8000 time: 0.0075s\n",
      "Epoch: 0101 loss_train: 0.8088 acc_train: 0.8429 loss_val: 0.9932 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0102 loss_train: 0.7629 acc_train: 0.8357 loss_val: 0.9873 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0103 loss_train: 0.7894 acc_train: 0.8143 loss_val: 0.9815 acc_val: 0.8000 time: 0.0075s\n",
      "Epoch: 0104 loss_train: 0.8440 acc_train: 0.8000 loss_val: 0.9757 acc_val: 0.8067 time: 0.0075s\n",
      "Epoch: 0105 loss_train: 0.7656 acc_train: 0.8786 loss_val: 0.9699 acc_val: 0.8067 time: 0.0065s\n",
      "Epoch: 0106 loss_train: 0.7996 acc_train: 0.8429 loss_val: 0.9636 acc_val: 0.8033 time: 0.0085s\n",
      "Epoch: 0107 loss_train: 0.7687 acc_train: 0.8214 loss_val: 0.9585 acc_val: 0.8033 time: 0.0075s\n",
      "Epoch: 0108 loss_train: 0.7387 acc_train: 0.8500 loss_val: 0.9531 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0109 loss_train: 0.7148 acc_train: 0.8714 loss_val: 0.9481 acc_val: 0.8000 time: 0.0075s\n",
      "Epoch: 0110 loss_train: 0.7624 acc_train: 0.8286 loss_val: 0.9432 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0111 loss_train: 0.7229 acc_train: 0.8429 loss_val: 0.9378 acc_val: 0.8033 time: 0.0075s\n",
      "Epoch: 0112 loss_train: 0.7094 acc_train: 0.8714 loss_val: 0.9326 acc_val: 0.8067 time: 0.0081s\n",
      "Epoch: 0113 loss_train: 0.7423 acc_train: 0.8500 loss_val: 0.9272 acc_val: 0.8033 time: 0.0075s\n",
      "Epoch: 0114 loss_train: 0.7526 acc_train: 0.8500 loss_val: 0.9227 acc_val: 0.8033 time: 0.0095s\n",
      "Epoch: 0115 loss_train: 0.7190 acc_train: 0.8143 loss_val: 0.9187 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0116 loss_train: 0.7045 acc_train: 0.8786 loss_val: 0.9153 acc_val: 0.8067 time: 0.0082s\n",
      "Epoch: 0117 loss_train: 0.7388 acc_train: 0.8286 loss_val: 0.9110 acc_val: 0.8067 time: 0.0097s\n",
      "Epoch: 0118 loss_train: 0.6732 acc_train: 0.8714 loss_val: 0.9059 acc_val: 0.8100 time: 0.0085s\n",
      "Epoch: 0119 loss_train: 0.6929 acc_train: 0.8571 loss_val: 0.9006 acc_val: 0.8133 time: 0.0075s\n",
      "Epoch: 0120 loss_train: 0.7430 acc_train: 0.8357 loss_val: 0.8958 acc_val: 0.8100 time: 0.0075s\n",
      "Epoch: 0121 loss_train: 0.7098 acc_train: 0.8143 loss_val: 0.8915 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0122 loss_train: 0.6584 acc_train: 0.8500 loss_val: 0.8877 acc_val: 0.8100 time: 0.0095s\n",
      "Epoch: 0123 loss_train: 0.6651 acc_train: 0.8786 loss_val: 0.8828 acc_val: 0.8133 time: 0.0085s\n",
      "Epoch: 0124 loss_train: 0.6653 acc_train: 0.8929 loss_val: 0.8787 acc_val: 0.8133 time: 0.0065s\n",
      "Epoch: 0125 loss_train: 0.6395 acc_train: 0.8357 loss_val: 0.8750 acc_val: 0.8167 time: 0.0075s\n",
      "Epoch: 0126 loss_train: 0.6719 acc_train: 0.8714 loss_val: 0.8717 acc_val: 0.8133 time: 0.0085s\n",
      "Epoch: 0127 loss_train: 0.6334 acc_train: 0.8857 loss_val: 0.8683 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0128 loss_train: 0.6259 acc_train: 0.9143 loss_val: 0.8649 acc_val: 0.8100 time: 0.0065s\n",
      "Epoch: 0129 loss_train: 0.6369 acc_train: 0.8643 loss_val: 0.8614 acc_val: 0.8067 time: 0.0075s\n",
      "Epoch: 0130 loss_train: 0.6274 acc_train: 0.8929 loss_val: 0.8570 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0131 loss_train: 0.6136 acc_train: 0.8857 loss_val: 0.8528 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0132 loss_train: 0.6366 acc_train: 0.9000 loss_val: 0.8486 acc_val: 0.8133 time: 0.0075s\n",
      "Epoch: 0133 loss_train: 0.6231 acc_train: 0.8500 loss_val: 0.8460 acc_val: 0.8133 time: 0.0075s\n",
      "Epoch: 0134 loss_train: 0.6198 acc_train: 0.8571 loss_val: 0.8441 acc_val: 0.8033 time: 0.0079s\n",
      "Epoch: 0135 loss_train: 0.6233 acc_train: 0.8500 loss_val: 0.8426 acc_val: 0.8100 time: 0.0105s\n",
      "Epoch: 0136 loss_train: 0.6354 acc_train: 0.8643 loss_val: 0.8408 acc_val: 0.8100 time: 0.0075s\n",
      "Epoch: 0137 loss_train: 0.6060 acc_train: 0.9000 loss_val: 0.8385 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0138 loss_train: 0.6092 acc_train: 0.9071 loss_val: 0.8367 acc_val: 0.8133 time: 0.0081s\n",
      "Epoch: 0139 loss_train: 0.6120 acc_train: 0.8857 loss_val: 0.8329 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0140 loss_train: 0.5706 acc_train: 0.9000 loss_val: 0.8282 acc_val: 0.8167 time: 0.0085s\n",
      "Epoch: 0141 loss_train: 0.5891 acc_train: 0.8929 loss_val: 0.8225 acc_val: 0.8167 time: 0.0095s\n",
      "Epoch: 0142 loss_train: 0.5533 acc_train: 0.9214 loss_val: 0.8163 acc_val: 0.8267 time: 0.0081s\n",
      "Epoch: 0143 loss_train: 0.5659 acc_train: 0.9000 loss_val: 0.8113 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0144 loss_train: 0.6414 acc_train: 0.8571 loss_val: 0.8074 acc_val: 0.8200 time: 0.0076s\n",
      "Epoch: 0145 loss_train: 0.6294 acc_train: 0.8571 loss_val: 0.8046 acc_val: 0.8167 time: 0.0074s\n",
      "Epoch: 0146 loss_train: 0.5926 acc_train: 0.8571 loss_val: 0.8026 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0147 loss_train: 0.5488 acc_train: 0.9214 loss_val: 0.8007 acc_val: 0.8133 time: 0.0075s\n",
      "Epoch: 0148 loss_train: 0.5582 acc_train: 0.8857 loss_val: 0.7980 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0149 loss_train: 0.5823 acc_train: 0.8643 loss_val: 0.7956 acc_val: 0.8100 time: 0.0083s\n",
      "Epoch: 0150 loss_train: 0.5280 acc_train: 0.9357 loss_val: 0.7938 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0151 loss_train: 0.5873 acc_train: 0.8786 loss_val: 0.7916 acc_val: 0.8100 time: 0.0091s\n",
      "Epoch: 0152 loss_train: 0.5360 acc_train: 0.9000 loss_val: 0.7885 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0153 loss_train: 0.5633 acc_train: 0.9071 loss_val: 0.7860 acc_val: 0.8133 time: 0.0065s\n",
      "Epoch: 0154 loss_train: 0.5759 acc_train: 0.9000 loss_val: 0.7842 acc_val: 0.8167 time: 0.0121s\n",
      "Epoch: 0155 loss_train: 0.5237 acc_train: 0.9286 loss_val: 0.7825 acc_val: 0.8167 time: 0.0101s\n",
      "Epoch: 0156 loss_train: 0.5288 acc_train: 0.9214 loss_val: 0.7805 acc_val: 0.8167 time: 0.0085s\n",
      "Epoch: 0157 loss_train: 0.5085 acc_train: 0.9071 loss_val: 0.7775 acc_val: 0.8133 time: 0.0075s\n",
      "Epoch: 0158 loss_train: 0.5162 acc_train: 0.9357 loss_val: 0.7743 acc_val: 0.8133 time: 0.0064s\n",
      "Epoch: 0159 loss_train: 0.5646 acc_train: 0.8786 loss_val: 0.7694 acc_val: 0.8100 time: 0.0060s\n",
      "Epoch: 0160 loss_train: 0.5563 acc_train: 0.9071 loss_val: 0.7654 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0161 loss_train: 0.5294 acc_train: 0.9000 loss_val: 0.7623 acc_val: 0.8167 time: 0.0061s\n",
      "Epoch: 0162 loss_train: 0.5350 acc_train: 0.9000 loss_val: 0.7601 acc_val: 0.8100 time: 0.0076s\n",
      "Epoch: 0163 loss_train: 0.4853 acc_train: 0.9214 loss_val: 0.7590 acc_val: 0.8067 time: 0.0105s\n",
      "Epoch: 0164 loss_train: 0.5578 acc_train: 0.9000 loss_val: 0.7581 acc_val: 0.8100 time: 0.0095s\n",
      "Epoch: 0165 loss_train: 0.5202 acc_train: 0.9071 loss_val: 0.7565 acc_val: 0.8133 time: 0.0077s\n",
      "Epoch: 0166 loss_train: 0.5251 acc_train: 0.8857 loss_val: 0.7538 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0167 loss_train: 0.4966 acc_train: 0.9357 loss_val: 0.7507 acc_val: 0.8167 time: 0.0075s\n",
      "Epoch: 0168 loss_train: 0.4610 acc_train: 0.9000 loss_val: 0.7481 acc_val: 0.8200 time: 0.0066s\n",
      "Epoch: 0169 loss_train: 0.5213 acc_train: 0.9071 loss_val: 0.7463 acc_val: 0.8200 time: 0.0076s\n",
      "Epoch: 0170 loss_train: 0.4909 acc_train: 0.9071 loss_val: 0.7450 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0171 loss_train: 0.4767 acc_train: 0.9357 loss_val: 0.7429 acc_val: 0.8133 time: 0.0076s\n",
      "Epoch: 0172 loss_train: 0.4693 acc_train: 0.9429 loss_val: 0.7408 acc_val: 0.8267 time: 0.0085s\n",
      "Epoch: 0173 loss_train: 0.4830 acc_train: 0.9143 loss_val: 0.7388 acc_val: 0.8200 time: 0.0075s\n",
      "Epoch: 0174 loss_train: 0.4730 acc_train: 0.9429 loss_val: 0.7378 acc_val: 0.8267 time: 0.0076s\n",
      "Epoch: 0175 loss_train: 0.4542 acc_train: 0.9429 loss_val: 0.7369 acc_val: 0.8233 time: 0.0089s\n",
      "Epoch: 0176 loss_train: 0.5214 acc_train: 0.8857 loss_val: 0.7363 acc_val: 0.8200 time: 0.0065s\n",
      "Epoch: 0177 loss_train: 0.5061 acc_train: 0.9214 loss_val: 0.7352 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0178 loss_train: 0.4902 acc_train: 0.9357 loss_val: 0.7346 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0179 loss_train: 0.4956 acc_train: 0.9286 loss_val: 0.7339 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0180 loss_train: 0.4933 acc_train: 0.9214 loss_val: 0.7323 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0181 loss_train: 0.4808 acc_train: 0.9000 loss_val: 0.7296 acc_val: 0.8067 time: 0.0075s\n",
      "Epoch: 0182 loss_train: 0.4855 acc_train: 0.9071 loss_val: 0.7267 acc_val: 0.8133 time: 0.0115s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0183 loss_train: 0.4354 acc_train: 0.9357 loss_val: 0.7238 acc_val: 0.8100 time: 0.0075s\n",
      "Epoch: 0184 loss_train: 0.4581 acc_train: 0.9286 loss_val: 0.7218 acc_val: 0.8033 time: 0.0073s\n",
      "Epoch: 0185 loss_train: 0.4628 acc_train: 0.9071 loss_val: 0.7194 acc_val: 0.8167 time: 0.0092s\n",
      "Epoch: 0186 loss_train: 0.4442 acc_train: 0.9214 loss_val: 0.7182 acc_val: 0.8233 time: 0.0085s\n",
      "Epoch: 0187 loss_train: 0.4695 acc_train: 0.9500 loss_val: 0.7173 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0188 loss_train: 0.4952 acc_train: 0.8786 loss_val: 0.7178 acc_val: 0.8267 time: 0.0081s\n",
      "Epoch: 0189 loss_train: 0.4450 acc_train: 0.9429 loss_val: 0.7190 acc_val: 0.8133 time: 0.0075s\n",
      "Epoch: 0190 loss_train: 0.4299 acc_train: 0.9429 loss_val: 0.7199 acc_val: 0.8100 time: 0.0079s\n",
      "Epoch: 0191 loss_train: 0.4375 acc_train: 0.9786 loss_val: 0.7195 acc_val: 0.8133 time: 0.0095s\n",
      "Epoch: 0192 loss_train: 0.4396 acc_train: 0.9143 loss_val: 0.7181 acc_val: 0.8033 time: 0.0093s\n",
      "Epoch: 0193 loss_train: 0.4575 acc_train: 0.9214 loss_val: 0.7177 acc_val: 0.8033 time: 0.0075s\n",
      "Epoch: 0194 loss_train: 0.4022 acc_train: 0.9214 loss_val: 0.7168 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0195 loss_train: 0.4308 acc_train: 0.9357 loss_val: 0.7161 acc_val: 0.8067 time: 0.0065s\n",
      "Epoch: 0196 loss_train: 0.4164 acc_train: 0.9429 loss_val: 0.7130 acc_val: 0.8033 time: 0.0086s\n",
      "Epoch: 0197 loss_train: 0.4197 acc_train: 0.9429 loss_val: 0.7082 acc_val: 0.8000 time: 0.0065s\n",
      "Epoch: 0198 loss_train: 0.4809 acc_train: 0.9143 loss_val: 0.7048 acc_val: 0.8100 time: 0.0105s\n",
      "Epoch: 0199 loss_train: 0.3824 acc_train: 0.9500 loss_val: 0.7027 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0200 loss_train: 0.4398 acc_train: 0.9429 loss_val: 0.7020 acc_val: 0.8167 time: 0.0113s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 6.0693s\n",
      "Test set results: loss= 0.7335 accuracy= 0.8370\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "# epoch数\n",
    "for epoch in range(args.epochs):\n",
    "    # 训练\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "# 已用总时间\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results: loss= 0.7335 accuracy= 0.8370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}