{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d59e2b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHow Powerful are Graph Neural Networks\\nhttps://arxiv.org/abs/1810.00826\\nhttps://openreview.net/forum?id=ryGs6iA5Km\\nAuthor's implementation: https://github.com/weihua916/powerful-gnns\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "How Powerful are Graph Neural Networks\n",
    "https://arxiv.org/abs/1810.00826\n",
    "https://openreview.net/forum?id=ryGs6iA5Km\n",
    "Author's implementation: https://github.com/weihua916/powerful-gnns\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ecd5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch.conv import GINConv\n",
    "from dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4bf00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyNodeFunc(nn.Module):\n",
    "    \"\"\"Update the node feature hv with MLP, BN and ReLU.\"\"\"\n",
    "    def __init__(self, mlp):\n",
    "        super(ApplyNodeFunc, self).__init__()\n",
    "        self.mlp = mlp\n",
    "        self.bn = nn.BatchNorm1d(self.mlp.output_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.mlp(h)\n",
    "        h = self.bn(h)\n",
    "        h = F.relu(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0390b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP with linear output\"\"\"\n",
    "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"MLP layers construction\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        num_layers: int\n",
    "            The number of linear layers\n",
    "        input_dim: int\n",
    "            The dimensionality of input features\n",
    "        hidden_dim: int\n",
    "            The dimensionality of hidden units at ALL layers\n",
    "        output_dim: int\n",
    "            The number of classes for prediction\n",
    "\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_or_not = True  # default is linear model\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if num_layers < 1:\n",
    "            raise ValueError(\"number of layers should be positive!\")\n",
    "        elif num_layers == 1:\n",
    "            # Linear model\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "        else:\n",
    "            # Multi-layer model\n",
    "            self.linear_or_not = False\n",
    "            self.linears = torch.nn.ModuleList()\n",
    "            self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
    "            for layer in range(num_layers - 2):\n",
    "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "            for layer in range(num_layers - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.linear_or_not:\n",
    "            # If linear model\n",
    "            return self.linear(x)\n",
    "        else:\n",
    "            # If MLP\n",
    "            h = x\n",
    "            for i in range(self.num_layers - 1):\n",
    "                h = F.relu(self.batch_norms[i](self.linears[i](h)))\n",
    "            return self.linears[-1](h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757c9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(nn.Module):\n",
    "    \"\"\"GIN model\"\"\"\n",
    "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim,\n",
    "                 output_dim, final_dropout, learn_eps, graph_pooling_type,\n",
    "                 neighbor_pooling_type):\n",
    "        \"\"\"model parameters setting\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        num_layers: int\n",
    "            The number of linear layers in the neural network\n",
    "        num_mlp_layers: int\n",
    "            The number of linear layers in mlps\n",
    "        input_dim: int\n",
    "            The dimensionality of input features\n",
    "        hidden_dim: int\n",
    "            The dimensionality of hidden units at ALL layers\n",
    "        output_dim: int\n",
    "            The number of classes for prediction\n",
    "        final_dropout: float\n",
    "            dropout ratio on the final linear layer\n",
    "        learn_eps: boolean\n",
    "            If True, learn epsilon to distinguish center nodes from neighbors\n",
    "            If False, aggregate neighbors and center nodes altogether.\n",
    "        neighbor_pooling_type: str\n",
    "            how to aggregate neighbors (sum, mean, or max)\n",
    "        graph_pooling_type: str\n",
    "            how to aggregate entire nodes in a graph (sum, mean or max)\n",
    "\n",
    "        \"\"\"\n",
    "        super(GIN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.learn_eps = learn_eps\n",
    "\n",
    "        # List of MLPs\n",
    "        self.ginlayers = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        \n",
    "        # 注意num_mlp_layers参数是MLP的层数，num_layers是GNN的层数\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            if layer == 0:\n",
    "                mlp = MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim)\n",
    "            else:\n",
    "                mlp = MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim)\n",
    "\n",
    "            self.ginlayers.append(\n",
    "                GINConv(ApplyNodeFunc(mlp), neighbor_pooling_type, 0, self.learn_eps))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Linear function for graph poolings of output of each layer\n",
    "        # which maps the output of different layers into a prediction score\n",
    "        self.linears_prediction = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                self.linears_prediction.append(\n",
    "                    nn.Linear(input_dim, output_dim))\n",
    "            else:\n",
    "                self.linears_prediction.append(\n",
    "                    nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.drop = nn.Dropout(final_dropout)\n",
    "\n",
    "        if graph_pooling_type == 'sum':\n",
    "            self.pool = SumPooling()\n",
    "        elif graph_pooling_type == 'mean':\n",
    "            self.pool = AvgPooling()\n",
    "        elif graph_pooling_type == 'max':\n",
    "            self.pool = MaxPooling()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # list of hidden representation at each layer (including input)\n",
    "        hidden_rep = [h]\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            h = self.ginlayers[i](g, h)\n",
    "            h = self.batch_norms[i](h)\n",
    "            h = F.relu(h)\n",
    "            hidden_rep.append(h)\n",
    "\n",
    "        score_over_layer = 0\n",
    "\n",
    "        # perform pooling over all nodes in each graph in every layer\n",
    "        for i, h in enumerate(hidden_rep):\n",
    "            pooled_h = self.pool(g, h)\n",
    "            score_over_layer += self.drop(self.linears_prediction[i](pooled_h))\n",
    "\n",
    "        return score_over_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8296e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1e7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f157de4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
