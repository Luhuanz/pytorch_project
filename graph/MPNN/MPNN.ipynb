{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from MessageFunction import MessageFunction\n",
    "from UpdateFunction import UpdateFunction\n",
    "from ReadoutFunction import ReadoutFunction\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__author__ = \"Pau Riba, Anjan Dutta\"\n",
    "__email__ = \"priba@cvc.uab.cat, adutta@cvc.uab.cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNN(nn.Module):\n",
    "    \"\"\"\n",
    "        MPNN as proposed by Gilmer et al..\n",
    "\n",
    "        This class implements the whole Gilmer et al. model following the functions Message, Update and Readout.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_n : int list\n",
    "            Sizes for the node and edge features.\n",
    "        hidden_state_size : int\n",
    "            Size of the hidden states (the input will be padded with 0's to this size).\n",
    "        message_size : int\n",
    "            Message function output vector size.\n",
    "        n_layers : int\n",
    "            Number of iterations Message+Update (weight tying).\n",
    "        l_target : int\n",
    "            Size of the output.\n",
    "        type : str (Optional)\n",
    "            Classification | [Regression (default)]. If classification, LogSoftmax layer is applied to the output vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_n, hidden_state_size, message_size, n_layers, l_target, type='regression'):\n",
    "        super(MPNN, self).__init__()\n",
    "\n",
    "        # Define message\n",
    "        self.m = nn.ModuleList(\n",
    "            [MessageFunction('mpnn', args={'edge_feat': in_n[1], 'in': hidden_state_size, 'out': message_size})])\n",
    "\n",
    "        # Define Update\n",
    "        self.u = nn.ModuleList([UpdateFunction('mpnn',\n",
    "                                               args={'in_m': message_size,\n",
    "                                                     'out': hidden_state_size})])\n",
    "\n",
    "        # Define Readout\n",
    "        self.r = ReadoutFunction('mpnn',\n",
    "                                 args={'in': hidden_state_size,\n",
    "                                       'target': l_target})\n",
    "        # 回归\n",
    "        self.type = type\n",
    "\n",
    "        self.args = {}\n",
    "        self.args['out'] = hidden_state_size\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    def forward(self, g, h_in, e):\n",
    "\n",
    "        h = []\n",
    "\n",
    "        # Padding to some larger dimension d\n",
    "        # padding补0\n",
    "        h_t = torch.cat([h_in, Variable(\n",
    "            torch.zeros(h_in.size(0), h_in.size(1), self.args['out'] - h_in.size(2)).type_as(h_in.data))], 2)\n",
    "\n",
    "        h.append(h_t.clone())\n",
    "\n",
    "        # Layer\n",
    "        for t in range(0, self.n_layers):\n",
    "            # utils.py中的collate_g函数\n",
    "            # g, h, e, target\n",
    "            # g: [bs, N, N]\n",
    "            # h: [bs, N, d_v]\n",
    "            # e: [bs, N, N, d_e]\n",
    "\n",
    "            # e_aux: [bs * N * N, d_e]\n",
    "            e_aux = e.view(-1, e.size(3))\n",
    "\n",
    "            # h_aux: [bs * N, d_v]\n",
    "            h_aux = h[t].view(-1, h[t].size(2))\n",
    "            \n",
    "            # 1. Message Function\n",
    "            # m: [bs * N * N, d_v]\n",
    "            m = self.m[0].forward(h[t], h_aux, e_aux)\n",
    "            # m: [bs, N, N, d_v]\n",
    "            m = m.view(h[0].size(0), h[0].size(1), -1, m.size(1))\n",
    "\n",
    "            # Nodes without edge set message to 0\n",
    "            # m: [bs, N, N, d_v]\n",
    "            m = torch.unsqueeze(g, 3).expand_as(m) * m\n",
    "            # m: [bs, N, d_v]\n",
    "            m = torch.squeeze(torch.sum(m, 1))\n",
    "            \n",
    "            # 2. Update Function\n",
    "            # h_t: [bs, N, d_v]\n",
    "            h_t = self.u[0].forward(h[t], m)\n",
    "\n",
    "            # Delete virtual nodes\n",
    "            h_t = (torch.sum(h_in, 2).expand_as(h_t) > 0).type_as(h_t) * h_t\n",
    "            h.append(h_t)\n",
    "\n",
    "        # 3. Readout Function\n",
    "        res = self.r.forward(h)\n",
    "\n",
    "        if self.type == 'classification':\n",
    "            res = nn.LogSoftmax()(res)\n",
    "        return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
